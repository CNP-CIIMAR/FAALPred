import logging
import os
import sys
import subprocess
import random
import zipfile
from collections import Counter
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO
import shutil
import time
import argparse 
import numpy as np
import pandas as pd
from Bio import SeqIO, AlignIO
from Bio.Align.Applications import MafftCommandline
import joblib
import plotly.io as pio
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import auc, roc_auc_score, roc_curve, f1_score, average_precision_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from tabulate import tabulate
from sklearn.calibration import CalibratedClassifierCV
from PIL import Image
from matplotlib import ticker
import umap.umap_ as umap  # Import for UMAP
import umap
import base64
from plotly.graph_objs import Figure
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# ============================================
# Definitions of Functions and Classes
# ============================================

# Setting seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,  # Change to DEBUG for more verbosity
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("logs/app.log"),  # Log to file for persistent records
    ],
)

# ============================================
# Streamlit Configuration and Interface
# ============================================

# Ensure st.set_page_config is the first Streamlit command
st.set_page_config(
    page_title="FAAL_Pred",
    #page_icon="Ã°Å¸â€Â¬",  # DNA Symbol
    page_icon="ðŸ§¬", 
    layout="wide",
    initial_sidebar_state="expanded",
)

def are_sequences_aligned(fasta_file: str) -> bool:
    """
    Checks if all sequences in a FASTA file are aligned by verifying they have the same length.
    
    Parameters:
    - fasta_file (str): Path to the FASTA file.
    
    Returns:
    - bool: True if all sequences are aligned (same length), False otherwise.
    """
    lengths = set()
    for record in SeqIO.parse(fasta_file, "fasta"):
        lengths.add(len(record.seq))
    return len(lengths) == 1  # Returns True if all sequences have the same length

def create_unique_model_directory(base_dir: str, aggregation_method: str) -> str:
    """
    Creates a unique directory for models based on the aggregation method.
    
    Parameters:
    - base_dir (str): Base directory for models.
    - aggregation_method (str): Aggregation method used.
    
    Returns:
    - str: Path to the unique model directory.
    """
    model_dir = os.path.join(base_dir, f"models_{aggregation_method}")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    return model_dir

def realign_sequences_with_mafft(input_path: str, output_path: str, threads: int = 8) -> None:
    """
    Realigns sequences using MAFFT.
    
    Parameters:
    - input_path (str): Path to the input file.
    - output_path (str): Path to save the realigned file.
    - threads (int): Number of threads for MAFFT.
    """
    mafft_command = ['mafft', '--thread', str(threads), '--maxiterate', '1000', '--localpair', input_path]
    try:
        with open(output_path, "w") as outfile:
            subprocess.run(mafft_command, stdout=outfile, stderr=subprocess.PIPE, check=True)
        logging.info(f"Realigned sequences saved to {output_path}")
    except subprocess.CalledProcessError as e:
        logging.error(f"Error executing MAFFT: {e.stderr.decode()}")
        sys.exit(1)

from sklearn.cluster import DBSCAN, KMeans

def perform_clustering(data: np.ndarray, method: str = "DBSCAN", eps: float = 0.5, min_samples: int = 5, n_clusters: int = 3) -> np.ndarray:
    """
    Performs clustering on the data using DBSCAN or K-Means.
    
    Parameters:
    - data (np.ndarray): Data for clustering.
    - method (str): Clustering method ("DBSCAN" or "K-Means").
    - eps (float): Epsilon parameter for DBSCAN.
    - min_samples (int): Minimum number of samples for DBSCAN.
    - n_clusters (int): Number of clusters for K-Means.
    
    Returns:
    - np.ndarray: Labels generated by the clustering method.
    """
    if method == "DBSCAN":
        clustering_model = DBSCAN(eps=eps, min_samples=min_samples)
    elif method == "K-Means":
        clustering_model = KMeans(n_clusters=n_clusters, random_state=42)
    else:
        raise ValueError(f"Invalid clustering method: {method}")

    labels = clustering_model.fit_predict(data)
    return labels

def plot_dual_tsne(
    train_embeddings: np.ndarray, 
    train_labels: list,
    train_protein_ids: list,
    predict_embeddings: np.ndarray, 
    predict_labels: list, 
    predict_protein_ids: list, 
    output_dir: str
) -> tuple:
    """
    Plots two 3D t-SNE plots and saves them as HTML files:
    - Plot 1: Training Data.
    - Plot 2: Predictions.
    
    Parameters:
    - train_embeddings (np.ndarray): Training embeddings.
    - train_labels (list): Labels of training data.
    - train_protein_ids (list): Protein IDs of training data.
    - predict_embeddings (np.ndarray): Prediction embeddings.
    - predict_labels (list): Prediction labels.
    - predict_protein_ids (list): Protein IDs of predictions.
    - output_dir (str): Directory to save t-SNE plots.
    
    Returns:
    - tuple: (Training Figure, Prediction Figure)
    """
    from sklearn.manifold import TSNE

    # Dimensionality reduction using t-SNE
    tsne_train = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)
    tsne_train_result = tsne_train.fit_transform(train_embeddings)

    tsne_predict = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)
    tsne_predict_result = tsne_predict.fit_transform(predict_embeddings)

    # Create color maps for training data
    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    # Create color maps for predictions
    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    # Convert labels to colors
    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    # Plot 1: Training Data
    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=tsne_train_result[:, 0],
        y=tsne_train_result[:, 1],
        z=tsne_train_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=train_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text'
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='t-SNE 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Plot 2: Predictions
    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=tsne_predict_result[:, 0],
        y=tsne_predict_result[:, 1],
        z=tsne_predict_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=predict_colors,
            opacity=0.8
        ),
        # Protein IDs added to 'text'
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='t-SNE 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Save the plots as HTML files
    tsne_train_html = os.path.join(output_dir, "tsne_train_3d.html")
    tsne_predict_html = os.path.join(output_dir, "tsne_predict_3d.html")
        
    pio.write_html(fig_train, file=tsne_train_html, auto_open=False)
    pio.write_html(fig_predict, file=tsne_predict_html, auto_open=False)
    
    logging.info(f"t-SNE Training plot saved as {tsne_train_html}")
    logging.info(f"t-SNE Predictions plot saved as {tsne_predict_html}")

    return fig_train, fig_predict
    
def plot_roc_curve_global(y_true: np.ndarray, y_pred_proba: np.ndarray, title: str, save_as: str = None, classes: list = None) -> None:
    """
    Plots the ROC curve for binary or multiclass classifications.
    
    Parameters:
    - y_true (np.ndarray): True labels.
    - y_pred_proba (np.ndarray): Predicted probabilities.
    - title (str): Title of the plot.
    - save_as (str): Path to save the plot.
    - classes (list): List of classes (for multiclass).
    """
    lw = 2  # Line width

    # Check if it's binary or multiclass classification
    unique_classes = np.unique(y_true)
    if len(unique_classes) == 2:  # Binary classification
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC Curve (area = %0.2f)' % roc_auc)
    else:  # Multiclass classification
        y_bin = label_binarize(y_true, classes=unique_classes)
        n_classes = y_bin.shape[1]

        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        plt.figure()

        colors = plt.cm.viridis(np.linspace(0, 1, n_classes))
        for i, color in zip(range(n_classes), colors):
            class_label = classes[i] if classes is not None else unique_classes[i]
            plt.plot(fpr[i], tpr[i], color=color, lw=lw, label=f'ROC Curve for class {class_label} (area = {roc_auc[i]:0.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', color='white')
    plt.ylabel('True Positive Rate', color='white')
    plt.title(title, color='white')
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
    if save_as:
        plt.savefig(save_as, bbox_inches='tight', facecolor='#0B3C5D')  # Match background color
    plt.close()

def get_class_rankings_global(model: RandomForestClassifier, X: np.ndarray) -> list:
    """
    Obtains class rankings based on the predicted probabilities from the model.
    
    Parameters:
    - model (RandomForestClassifier): Trained model.
    - X (np.ndarray): Data to obtain predictions.
    
    Returns:
    - list: List of formatted class rankings for each sample.
    """
    if model is None:
        raise ValueError("Model not trained. Please train the model first.")

    # Obtain probabilities for each class
    y_pred_proba = model.predict_proba(X)

    # Ranking classes based on probabilities
    class_rankings = []
    for probabilities in y_pred_proba:
        ranked_classes = sorted(zip(model.classes_, probabilities), key=lambda x: x[1], reverse=True)
        formatted_rankings = [f"{cls}: {prob*100:.2f}%" for cls, prob in ranked_classes]
        class_rankings.append(formatted_rankings)

    return class_rankings

def calculate_roc_values(model: RandomForestClassifier, X_test: np.ndarray, y_test: np.ndarray) -> pd.DataFrame:
    """
    Calculates ROC AUC scores for each class.
    
    Parameters:
    - model (RandomForestClassifier): Trained model.
    - X_test (np.ndarray): Test data.
    - y_test (np.ndarray): True test labels.
    
    Returns:
    - pd.DataFrame: DataFrame containing ROC AUC scores per class.
    """
    n_classes = len(np.unique(y_test))
    y_pred_proba = model.predict_proba(X_test)

    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)
        roc_auc[i] = auc(fpr[i], tpr[i])

        # Logging ROC values
        logging.info(f"For class {i}:")
        logging.info(f"FPR: {fpr[i]}")
        logging.info(f"TPR: {tpr[i]}")
        logging.info(f"ROC AUC: {roc_auc[i]}")
        logging.info("--------------------------")

    roc_df = pd.DataFrame(list(roc_auc.items()), columns=['Class', 'ROC AUC'])
    return roc_df

def format_and_sum_probabilities(associated_rankings: list) -> tuple:
    """
    Formats and sums probabilities for each category, returning only the main category.
    
    Parameters:
    - associated_rankings (list): List of associated rankings.
    
    Returns:
    - tuple: (main category with confidence, sum of probabilities, top two categories)
    """
    category_sums = {}
    categories = ['C4-C6-C8', 'C6-C8-C10', 'C8-C10-C12', 'C10-C12-C14', 'C12-C14-C16', 'C14-C16-C18']
    pattern_mapping = {
        'C4-C6-C8': ['C4', 'C6', 'C8'],
        'C6-C8-C10': ['C6', 'C8', 'C10'],
        'C8-C10-C12': ['C8', 'C10', 'C12'],
        'C10-C12-C14': ['C10', 'C12', 'C14'],
        'C12-C14-C16': ['C12', 'C14', 'C16'],
        'C14-C16-C18': ['C14', 'C16', 'C18'],
    }

    # Initialize the sum dictionary
    for category in categories:
        category_sums[category] = 0.0

    # Sum probabilities for each category
    for rank in associated_rankings:
        try:
            prob = float(rank.split(": ")[1].replace("%", ""))
        except (IndexError, ValueError):
            logging.error(f"Error processing ranking string: {rank}")
            continue
        for category, patterns in pattern_mapping.items():
            if any(pattern in rank for pattern in patterns):
                category_sums[category] += prob

    if not category_sums:
        return None, None, None  # No valid data

    # Determine the main category based on the sum of probabilities
    top_category, top_sum = max(category_sums.items(), key=lambda x: x[1])

    # Find the top two categories
    sorted_categories = sorted(category_sums.items(), key=lambda x: x[1], reverse=True)
    top_two = sorted_categories[:2] if len(sorted_categories) >=2 else sorted_categories

    # Extract the top two categories and their probabilities
    top_two_categories = [f"{cat} ({prob:.2f}%)" for cat, prob in top_two]

    # Find the main category with confidence
    top_category_with_confidence = f"{top_category} ({top_sum:.2f}%)"

    return top_category_with_confidence, top_sum, top_two_categories


class Support:
    """
    Support class for training and evaluating Random Forest models with oversampling techniques.
    This version integrates an iterative clustering and SMOTE approach guided by both class (y) and var_assoc.
    The idea:
    1. Perform minimum oversampling by class.
    2. Perform oversampling to reach final_target.
    3. Identify duplicates created.
    4. For each class and var_assoc subset:
       - Cluster original samples (class + var_assoc) using K-Means (or another method).
       - Assign duplicates to nearest clusters using cosine similarity or Euclidean distance.
       - Apply SMOTE cluster-by-cluster.
       - Evaluate cluster cohesion using silhouette_score (with cosine metric).
       - If cohesion is below a threshold, iterate again, generating more synthetic samples or adjusting SMOTE parameters until cohesion surpasses the threshold or max iterations reached.
       
    This ensures that the synthetic samples generated via SMOTE integrate more coherently with the original data in the latent space, creating more cohesive clusters guided by both class and var_assoc.
    """

    def __init__(self, cv: int = 5, seed: int = SEED, n_jobs: int = 8):
        self.cv = cv
        self.model = None
        self.seed = seed
        self.n_jobs = n_jobs
        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []
        self.roc_results = []
        self.train_sizes = np.linspace(.1, 1.0, 5)
        self.standard = StandardScaler()
        self.best_params = {}

        self.init_params = {
            "n_estimators": 100,  
            "max_depth": 10,  
            "min_samples_split": 2,  
            "min_samples_leaf": 2,  
            "criterion": "entropy",  
            "max_features": "sqrt",  
            "class_weight": None,  
            "max_leaf_nodes": 10,  
            "min_impurity_decrease": 0.02,  
            "bootstrap": True,  
            "ccp_alpha": 0.01,           
        }

        self.parameters = {
            "n_estimators": [50, 150],
            "max_depth": [5],
            "min_samples_split": [20, 30,40],
            "min_samples_leaf": [4, 5,20],
            "criterion": ["gini", "entropy"],
            "max_features": ["sqrt", "log2"],
            "class_weight": ["balanced", "balanced_subsample", None],
            "max_leaf_nodes": [1, 2, 5, None],
            "min_impurity_decrease": [0.0, 0.01, 0.05],
            "bootstrap": [True, False],
            "ccp_alpha": [0.0, 0.001, 0.01],
        }

    def _oversample_single_sample_classes(
        self, 
        X: np.ndarray, 
        y: np.ndarray, 
        protein_ids: list = None, 
        var_assoc: list = None
    ) -> tuple:
        """
        Oversamples classes with an iterative clustering + SMOTE approach guided by class and var_assoc.

        Steps:
        - Minimum oversampling by class.
        - Oversampling to final target.
        - Identify duplicates.
        - For each class and var_assoc:
          * Cluster originals (K-Means).
          * Assign duplicates to clusters by nearest center (using cosine similarity).
          * Apply SMOTE cluster-by-cluster.
          * Evaluate cohesion (silhouette_score with cosine metric).
          * If cohesion < threshold and iterations < max_iter, try again (e.g. increase SMOTE sampling or neighbors).
        
        The process ends when clusters become sufficiently cohesive or max iterations reached.
        """

        from sklearn.cluster import KMeans
        from imblearn.over_sampling import SMOTE, RandomOverSampler
        from scipy.spatial.distance import cdist
        from sklearn.metrics import silhouette_score
        import numpy as np
        import sys
        import logging
        from collections import Counter

        final_target = 50
        seed = self.seed if hasattr(self, 'seed') else 42

        logging.info("Starting oversampling process with class+var_assoc guided clustering.")

        # Step 1: Minimum oversampling
        original_counts = Counter(y)
        logging.info(f"Initial class distribution: {original_counts}")

        classes_to_oversample = {cls: max(self.cv + 1, count) for cls, count in original_counts.items()}
        logging.info(f"Oversampling strategy for minimum samples: {classes_to_oversample}")

        try:
            ros_min = RandomOverSampler(sampling_strategy=classes_to_oversample, random_state=seed)
            X_ros_min, y_ros_min = ros_min.fit_resample(X, y)
            logging.info(f"Class distribution after minimum oversampling: {Counter(y_ros_min)}")
        except ValueError as e:
            logging.error(f"Error during RandomOverSampler (min step): {e}")
            sys.exit(1)

        if protein_ids is None or len(protein_ids) == 0:
            protein_ids = [f"original_id_{i}" for i in range(len(X))]
        if var_assoc is None or len(var_assoc) == 0:
            var_assoc = [f"original_var_{i}" for i in range(len(X))]

        synthetic_protein_ids_min = []
        synthetic_var_assoc_min = []
        for idx in range(len(X), len(X_ros_min)):
            synthetic_protein_ids_min.append(f"synthetic_ros_min_{idx}")
            synthetic_var_assoc_min.append(f"synthetic_var_min_{idx}")

        protein_ids_min = protein_ids + synthetic_protein_ids_min
        var_assoc_min = var_assoc + synthetic_var_assoc_min

        # Step 2: Oversample to final_target
        dist_after_min = Counter(y_ros_min)
        classes_to_final = {cls: final_target for cls in dist_after_min.keys()}
        logging.info(f"Strategy to reach {final_target} members per class: {classes_to_final}")

        try:
            ros_final = RandomOverSampler(sampling_strategy=classes_to_final, random_state=seed)
            X_final_target, y_final_target = ros_final.fit_resample(X_ros_min, y_ros_min)
            logging.info(f"Class distribution after reaching {final_target} members: {Counter(y_final_target)}")
        except ValueError as e:
            logging.error(f"Error during RandomOverSampler (to {final_target}): {e}")
            sys.exit(1)

        synthetic_protein_ids_final = []
        synthetic_var_assoc_final = []
        for idx in range(len(X_ros_min), len(X_final_target)):
            synthetic_protein_ids_final.append(f"synthetic_ros_{final_target}_{idx}")
            synthetic_var_assoc_final.append(f"synthetic_var_final_{idx}")

        protein_ids_final = protein_ids_min + synthetic_protein_ids_final
        var_assoc_final = var_assoc_min + synthetic_var_assoc_final

        original_counts_after_min = Counter(y_ros_min)
        final_counts = Counter(y_final_target)
        y_final_target_arr = np.array(y_final_target)

        X_original_only = []
        y_original_only = []
        protein_ids_original_only = []
        var_assoc_original_only = []

        X_duplicates_all = []
        y_duplicates_all = []
        protein_ids_duplicates_all = []
        var_assoc_duplicates_all = []

        for cls in final_counts:
            cls_indices_final = np.where(y_final_target_arr == cls)[0]
            n_original_cls = original_counts_after_min[cls]
            n_final_cls = final_target
            n_duplicates = n_final_cls - n_original_cls

            cls_original_indices = cls_indices_final[:n_original_cls]
            cls_duplicate_indices = cls_indices_final[n_original_cls:] if n_duplicates > 0 else []

            X_original_only.append(X_final_target[cls_original_indices])
            y_original_only.append(y_final_target_arr[cls_original_indices])

            for i in cls_original_indices:
                protein_ids_original_only.append(protein_ids_final[i])
                var_assoc_original_only.append(var_assoc_final[i])

            if n_duplicates > 0:
                X_duplicates_all.append(X_final_target[cls_duplicate_indices])
                y_duplicates_all.append(y_final_target_arr[cls_duplicate_indices])
                for di in cls_duplicate_indices:
                    protein_ids_duplicates_all.append(protein_ids_final[di])
                    var_assoc_duplicates_all.append(var_assoc_final[di])

        X_original_only = np.vstack(X_original_only)
        y_original_only = np.hstack(y_original_only)

        if X_duplicates_all:
            X_duplicates_all = np.vstack(X_duplicates_all)
            y_duplicates_all = np.hstack(y_duplicates_all)
        else:
            X_duplicates_all = np.array([])
            y_duplicates_all = np.array([])

        logging.info(f"Number of duplicated samples: {len(X_duplicates_all)}")

        # Função de ajuda: atribuir duplicates ao cluster mais próximo usando cosine similarity
        def assign_duplicates_to_clusters(X_dup, centers):
            # Compute cosine similarity
            # cosine_similarity: we can do 1 - cosine_distance
            from sklearn.metrics.pairwise import cosine_distances
            dist_matrix = cosine_distances(X_dup, centers)
            assigned_clusters = dist_matrix.argmin(axis=1)
            return assigned_clusters

        def compute_cluster_cohesion(X_data, labels, metric='cosine'):
            # Compute silhouette to measure cohesion
            if len(np.unique(labels)) < 2 or X_data.shape[0] < 2:
                return 1.0  # If only one cluster, consider perfect cohesion
            score = silhouette_score(X_data, labels, metric=metric)
            return score

        def generate_synthetic_samples_iterative(
            X_dup, y_dup, prot_dup, var_dup,
            X_orig, y_orig, prot_orig, var_orig,
            cls_label, var_val, seed=42,
            max_iterations=5, silhouette_threshold=0.5
        ):
            """
            Para um dado conjunto (classe + var_val), tenta gerar amostras sintéticas iterativamente
            até que a coesão do cluster (silhouette) supere o threshold ou atinja max_iterations.
            """
            np.random.seed(seed)
            from sklearn.cluster import KMeans
            from imblearn.over_sampling import SMOTE

            # Filtrar originais
            class_mask_orig = (y_orig == cls_label)
            X_class_orig = X_orig[class_mask_orig]
            prot_class_orig = [po for po, mo in zip(prot_orig, class_mask_orig) if mo]
            var_class_orig = [vo for vo, mo in zip(var_orig, class_mask_orig) if mo]

            mask_var_orig = [v == var_val for v in var_class_orig]
            X_sub_orig = X_class_orig[mask_var_orig]
            prot_sub_orig = [p for p, m in zip(prot_class_orig, mask_var_orig) if m]
            var_sub_orig = [vv for vv, m in zip(var_class_orig, mask_var_orig) if m]

            # Filtrar duplicadas
            class_mask_dup = (y_dup == cls_label)
            X_class_dup = X_dup[class_mask_dup]
            y_class_dup = y_dup[class_mask_dup]
            prot_class_dup = [pd for pd, md in zip(prot_dup, class_mask_dup) if md]
            var_class_dup = [vd for vd, md in zip(var_dup, class_mask_dup) if md]

            mask_var_dup = [v == var_val for v in var_class_dup]
            X_sub_dup = X_class_dup[mask_var_dup]
            y_sub_dup = y_class_dup[mask_var_dup]
            prot_sub_dup = [p for p, m in zip(prot_class_dup, mask_var_dup) if m]
            var_sub_dup = [vv for vv, m in zip(var_class_dup, mask_var_dup) if m]

            # Combinar originais + duplicates
            X_comb = np.vstack([X_sub_orig, X_sub_dup]) if X_sub_dup.size > 0 else X_sub_orig
            y_comb = np.hstack([np.repeat(cls_label, len(X_sub_orig)), y_sub_dup]) if X_sub_dup.size > 0 else np.repeat(cls_label, len(X_sub_orig))
            prot_comb = prot_sub_orig + prot_sub_dup
            var_comb = var_sub_orig + var_sub_dup

            if X_comb.shape[0] < 2:
                # Poucas amostras, sem SMOTE
                return X_comb, y_comb, prot_comb, var_comb

            # Iterar até atingir coesão ou max_iterations
            for iteration in range(max_iterations):
                # Clusterizar novamente
                n_clusters = min(2, X_comb.shape[0])  # Pelo menos 2 se possível
                if n_clusters < 2:
                    # Se temos menos de 2 amostras, coesão trivial
                    return X_comb, y_comb, prot_comb, var_comb
                kmeans = KMeans(n_clusters=n_clusters, random_state=seed)
                labels = kmeans.fit_predict(X_comb)

                cohesion = compute_cluster_cohesion(X_comb, labels, metric='cosine')
                logging.info(f"Iteration {iteration+1} class={cls_label}, var_val={var_val}, cohesion={cohesion}")

                if cohesion >= silhouette_threshold:
                    # Suficientemente coeso
                    return X_comb, y_comb, prot_comb, var_comb
                else:
                    # Aplicar SMOTE novamente para tentar melhorar
                    k_smote = min(5, max(2, len(X_comb)-1))
                    sm = SMOTE(random_state=seed, k_neighbors=k_smote)
                    X_sm, y_sm = sm.fit_resample(X_comb, y_comb)

                    # Gerar sintéticos para novas amostras
                    # Contar quantas novas foram criadas
                    new_count = len(X_sm) - len(X_comb)
                    synthetic_prot_sm = []
                    synthetic_var_sm = []
                    for i_sm in range(len(X_sm)):
                        if i_sm < len(X_comb):
                            synthetic_prot_sm.append(prot_comb[i_sm])
                            synthetic_var_sm.append(var_comb[i_sm])
                        else:
                            synthetic_prot_sm.append(f"synthetic_smote_id_{len(prot_comb)+i_sm}")
                            synthetic_var_sm.append(f"synthetic_smote_var_{len(var_comb)+i_sm}")

                    X_comb = X_sm
                    y_comb = y_sm
                    prot_comb = synthetic_prot_sm
                    var_comb = synthetic_var_sm

            # Se chegou aqui, max_iterations atingidas sem atingir threshold
            logging.info(f"Max iterations reached for class={cls_label}, var_val={var_val} without achieving silhouette_threshold.")
            return X_comb, y_comb, prot_comb, var_comb

        def run_iterative_clustering_smote(
            X_dup, y_dup, prot_dup, var_dup,
            X_orig, y_orig, prot_orig, var_orig,
            seed=42, silhouette_threshold=0.5, max_iterations=5
        ):
            # Segmentar por classe e var_assoc
            unique_classes = np.unique(y_dup) if len(y_dup) > 0 else np.unique(y_orig)
            X_synth_list = []
            y_synth_list = []
            prot_synth_list = []
            var_synth_list = []

            for cls_label in unique_classes:
                # Filtrar originais e ver var_assoc
                class_mask_orig = (y_orig == cls_label)
                var_assoc_class = [vo for vo, mo in zip(var_orig, class_mask_orig) if mo]
                if len(var_assoc_class) == 0:
                    # Nenhuma amostra original nesta classe
                    # Apenas combinar duplicates sem SMOTE
                    class_mask_dup = (y_dup == cls_label)
                    X_class_dup = X_dup[class_mask_dup]
                    y_class_dup = y_dup[class_mask_dup]
                    prot_class_dup = [p for p, m in zip(prot_dup, class_mask_dup) if m]
                    var_class_dup = [v for v, m in zip(var_dup, class_mask_dup) if m]
                    if len(X_class_dup) > 0:
                        X_synth_list.append(X_class_dup)
                        y_synth_list.append(y_class_dup)
                        prot_synth_list.extend(prot_class_dup)
                        var_synth_list.extend(var_class_dup)
                    continue

                unique_var_vals = np.unique(var_assoc_class)

                for var_val in unique_var_vals:
                    # Iterative approach for this class+var_val subset
                    X_sub, y_sub, prot_sub, var_sub = generate_synthetic_samples_iterative(
                        X_dup, y_dup, prot_dup, var_dup,
                        X_orig, y_orig, prot_orig, var_orig,
                        cls_label, var_val, seed=seed,
                        max_iterations=max_iterations, silhouette_threshold=silhouette_threshold
                    )

                    if X_sub.size > 0:
                        X_synth_list.append(X_sub)
                        y_synth_list.append(y_sub)
                        prot_synth_list.extend(prot_sub)
                        var_synth_list.extend(var_sub)

            if X_synth_list:
                X_synth_all = np.vstack(X_synth_list)
                y_synth_all = np.hstack(y_synth_list)
            else:
                X_synth_all = np.array([])
                y_synth_all = np.array([])

            return X_synth_all, y_synth_all, prot_synth_list, var_synth_list

        # Agora aplicar a lógica completa
        # Apos identificar originais e duplicadas, rodamos a lógica iterativa

        X_synth, y_synth, prot_synth_all, var_synth_all = run_iterative_clustering_smote(
            X_duplicates_all, y_duplicates_all, protein_ids_duplicates_all, var_assoc_duplicates_all,
            X_original_only, y_original_only, protein_ids_original_only, var_assoc_original_only,
            seed=seed, silhouette_threshold=0.5, max_iterations=5
        )

        logging.info(f"Number of synthetic samples generated: {len(X_synth)}")

        # Combinar tudo para atingir final_target
        final_counts_orig = Counter(y_original_only)
        classes_final = np.unique(y_final_target_arr)
        X_add = []
        y_add = []
        prot_add = []
        var_add = []

        final_target = 50
        for cls in classes_final:
            needed = final_target - sum(y_original_only == cls)
            if needed > 0 and len(X_synth) > 0:
                cls_indices_synth = np.where(y_synth == cls)[0]
                if len(cls_indices_synth) >= needed:
                    chosen = np.random.choice(cls_indices_synth, size=needed, replace=False)
                    X_add.append(X_synth[chosen])
                    y_add.append(y_synth[chosen])
                    for cidx in chosen:
                        if cidx < len(prot_synth_all) and cidx < len(var_synth_all):
                            prot_add.append(prot_synth_all[cidx])
                            var_add.append(var_synth_all[cidx])
                        else:
                            prot_add.append(f"synthetic_final_id_{cidx}")
                            var_add.append(f"synthetic_final_var_{cidx}")
                else:
                    X_add.append(X_synth[cls_indices_synth])
                    y_add.append(y_synth[cls_indices_synth])
                    for cidx in cls_indices_synth:
                        if cidx < len(prot_synth_all) and cidx < len(var_synth_all):
                            prot_add.append(prot_synth_all[cidx])
                            var_add.append(var_synth_all[cidx])
                        else:
                            prot_add.append(f"synthetic_final_id_{cidx}")
                            var_add.append(f"synthetic_final_var_{cidx}")

        X_original_only_arr = X_original_only
        y_original_only_arr = y_original_only

        if X_add:
            X_add = np.vstack(X_add)
            y_add = np.hstack(y_add)
            X_done = np.vstack([X_original_only_arr, X_add])
            y_done = np.hstack([y_original_only_arr, y_add])
            protein_ids_done = protein_ids_original_only + prot_add
            var_assoc_done = var_assoc_original_only + var_add
        else:
            X_done, y_done = X_original_only_arr, y_original_only_arr
            protein_ids_done = protein_ids_original_only
            var_assoc_done = var_assoc_original_only

        dist_final = Counter(y_done)
        logging.info(f"Final class distribution after synthetic substitution: {dist_final}")

        with open("oversampling_counts.txt", "a") as f:
            f.write(f"Class distribution after ensuring min, {final_target}, SMOTE+Clustering (class+var_assoc guided, iterative):\n")
            for cls, count in dist_final.items():
                f.write(f"{cls}: {count}\n")

        synthetic_protein_ids = []
        synthetic_var_assoc = []
        if len(protein_ids_done) == len(var_assoc_done):
            for pid, v in zip(protein_ids_done, var_assoc_done):
                if pid is not None and isinstance(pid, str) and pid.startswith("synthetic_"):
                    synthetic_protein_ids.append(pid)
                    synthetic_var_assoc.append(v)
        else:
            logging.warning("Length mismatch between protein_ids_done and var_assoc_done.")

        X_smote = X_done
        y_smote = y_done

        return X_smote, y_smote, synthetic_protein_ids, synthetic_var_assoc

    def fit(
        self, 
        X: np.ndarray, 
        y: np.ndarray, 
        protein_ids: list = None, 
        var_assoc: list = None, 
        model_name_prefix: str = 'model', 
        model_dir: str = None, 
        min_kmers: int = None
    ) -> RandomForestClassifier:
        logging.info(f"Starting fit method for {model_name_prefix}...")

        X = np.array(X)
        y = np.array(y)

        if min_kmers is not None:
            logging.info(f"Using provided min_kmers: {min_kmers}")
        else:
            min_kmers = len(X)
            logging.info(f"min_kmers not provided. Setting to size of X: {min_kmers}")

        X_smote, y_smote, synthetic_protein_ids, synthetic_var_assoc = self._oversample_single_sample_classes(
            X, y, protein_ids, var_assoc
        )

        num_synthetic = len(synthetic_protein_ids)
        total_samples = len(X_smote)
        original_count = total_samples - num_synthetic

        X_original_final = X_smote[:original_count]
        y_original_final = y_smote[:original_count]
        X_synthetic_final = X_smote[original_count:]
        y_synthetic_final = y_smote[original_count:]

        if protein_ids is None or len(protein_ids) < len(X_original_final):
            protein_ids = [f"orig_id_{i}" for i in range(len(X_original_final))]
        if var_assoc is None or len(var_assoc) < len(X_original_final):
            var_assoc = [f"orig_var_{i}" for i in range(len(X_original_final))]

        if len(synthetic_protein_ids) < len(X_synthetic_final):
            synthetic_protein_ids = [f"synthetic_final_id_{i}" for i in range(len(X_synthetic_final))]
        if len(synthetic_var_assoc) < len(X_synthetic_final):
            synthetic_var_assoc = [f"synthetic_final_var_{i}" for i in range(len(X_synthetic_final))]

        logging.info("Visualizing latent space with similarity measures...")
        fig = visualize_latent_space_with_similarity(
            X_original=X_original_final, 
            X_synthetic=X_synthetic_final,
            y_original=y_original_final, 
            y_synthetic=y_synthetic_final,
            protein_ids_original=protein_ids,
            protein_ids_synthetic=synthetic_protein_ids,
            var_assoc_original=var_assoc,
            var_assoc_synthetic=synthetic_var_assoc,
            output_dir=model_dir
        )
        logging.info("UMAP visualization generated successfully.")

        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []

        class_counts = Counter(y_smote)
        min_class_count = min(class_counts.values())
        adjusted_n_splits = min(self.cv, min_class_count - 1)  
        if adjusted_n_splits < self.cv:
            logging.warning(f"Adjusting n_splits from {self.cv} to {adjusted_n_splits} due to class size constraints.")
            skf = StratifiedKFold(n_splits=adjusted_n_splits, random_state=self.seed, shuffle=True)
        else:
            skf = StratifiedKFold(n_splits=self.cv, random_state=self.seed, shuffle=True)

        for fold_number, (train_index, test_index) in enumerate(skf.split(X_smote, y_smote), start=1):
            X_train, X_test = X_smote[train_index], X_smote[test_index]
            y_train, y_test = y_smote[train_index], y_smote[test_index]

            unique, counts_fold = np.unique(y_test, return_counts=True)
            fold_class_distribution = dict(zip(unique, counts_fold))
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Test set class distribution: {fold_class_distribution}")

            X_train_resampled, y_train_resampled, synthetic_protein_ids_train, synthetic_var_assoc_train = self._oversample_single_sample_classes(
                X_train, 
                y_train, 
                protein_ids=protein_ids if protein_ids else None, 
                var_assoc=var_assoc if var_assoc else None
            )

            train_sample_counts = Counter(y_train_resampled)
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Training set class distribution after oversampling: {train_sample_counts}")

            with open("training_sample_counts_after_oversampling.txt", "a") as f:
                f.write(f"Fold {fold_number} Training sample counts after oversampling for {model_name_prefix}:\n")
                for cls, count in train_sample_counts.items():
                    f.write(f"{cls}: {count}\n")

            self.model = RandomForestClassifier(**self.init_params, n_jobs=self.n_jobs)
            self.model.fit(X_train_resampled, y_train_resampled)

            train_score = self.model.score(X_train_resampled, y_train_resampled)
            test_score = self.model.score(X_test, y_test)
            y_pred = self.model.predict(X_test)

            f1 = f1_score(y_test, y_pred, average='weighted')
            self.f1_scores.append(f1)
            self.train_scores.append(train_score)
            self.test_scores.append(test_score)

            if len(np.unique(y_test)) > 1:
                pr_auc = average_precision_score(y_test, self.model.predict_proba(X_test), average='macro')
            else:
                pr_auc = 0.0
            self.pr_auc_scores.append(pr_auc)

            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Training Score: {train_score}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Test Score: {test_score}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: F1 Score: {f1}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Precision-Recall AUC: {pr_auc}")

            try:
                if len(np.unique(y_test)) == 2:
                    fpr, tpr, thresholds = roc_curve(y_test, self.model.predict_proba(X_test)[:, 1])
                    roc_auc_score_value = auc(fpr, tpr)
                    self.roc_results.append((fpr, tpr, roc_auc_score_value))
                else:
                    y_test_bin = label_binarize(y_test, classes=self.model.classes_)
                    roc_auc_score_value = roc_auc_score(y_test_bin, self.model.predict_proba(X_test), multi_class='ovo', average='macro')
                    self.roc_results.append(roc_auc_score_value)
            except ValueError:
                logging.warning(f"Unable to calculate ROC AUC for fold {fold_number} [{model_name_prefix}] due to insufficient class representation.")

            best_model, best_params = self._perform_grid_search(X_train_resampled, y_train_resampled)
            self.model = best_model
            self.best_params = best_params

            if model_dir:
                best_model_filename = os.path.join(model_dir, f'model_best_{model_name_prefix}.pkl')
                os.makedirs(os.path.dirname(best_model_filename), exist_ok=True)
                joblib.dump(best_model, best_model_filename)
                logging.info(f"Best model saved as {best_model_filename} for {model_name_prefix}")
            else:
                best_model_filename = f'model_best_{model_name_prefix}.pkl'
                joblib.dump(best_model, best_model_filename)
                logging.info(f"Best model saved as {best_model_filename} for {model_name_prefix}")

            if best_params is not None:
                self.best_params = best_params
                logging.info(f"Best parameters for {model_name_prefix}: {self.best_params}")
            else:
                logging.warning(f"No best parameters found in grid search for {model_name_prefix}.")

            calibrator = CalibratedClassifierCV(self.model, method='isotonic', cv=5, n_jobs=self.n_jobs)
            calibrator.fit(X_train_resampled, y_train_resampled)

            self.model = calibrator

            if model_dir:
                calibrated_model_filename = os.path.join(model_dir, f'calibrated_model_{model_name_prefix}.pkl')
            else:
                calibrated_model_filename = f'calibrated_model_{model_name_prefix}.pkl'
            joblib.dump(calibrator, calibrated_model_filename)
            logging.info(f"Calibrated model saved as {calibrated_model_filename} for {model_name_prefix}")

        return self.model

    def _perform_grid_search(self, X_train_resampled: np.ndarray, y_train_resampled: np.ndarray) -> tuple:
        skf = StratifiedKFold(n_splits=self.cv, random_state=self.seed, shuffle=True)
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=self.seed),
            self.parameters,
            cv=skf,
            n_jobs=self.n_jobs,
            scoring='roc_auc_ovo',
            verbose=1
        )

        grid_search.fit(X_train_resampled, y_train_resampled)
        logging.info(f"Best parameters from grid search: {grid_search.best_params_}")
        return grid_search.best_estimator_, grid_search.best_params_

    def get_best_param(self, param_name: str, default = None):
        return self.best_params.get(param_name, default)

    def plot_learning_curve(self, output_path: str) -> None:
        plt.figure()
        plt.plot(self.train_scores, label='Training Score')
        plt.plot(self.test_scores, label='Cross-Validation Score')
        plt.plot(self.f1_scores, label='F1 Score')
        plt.plot(self.pr_auc_scores, label='Precision-Recall AUC')
        plt.title("Learning Curve", color='white')
        plt.xlabel("Fold", fontsize=12, fontweight='bold', color='white')
        plt.ylabel("Score", fontsize=12, fontweight='bold', color='white')
        plt.legend(loc="best")
        plt.grid(color='white', linestyle='--', linewidth=0.5)
        plt.savefig(output_path, facecolor='#0B3C5D')
        plt.close()

    def get_class_rankings(self, X: np.ndarray) -> list:
        if self.model is None:
            raise ValueError("Model not trained. Please train the model first.")

        y_pred_proba = self.model.predict_proba(X)
        class_rankings = []
        for probabilities in y_pred_proba:
            ranked_classes = sorted(zip(self.model.classes_, probabilities), key=lambda x: x[1], reverse=True)
            formatted_rankings = [f"{cls}: {prob*100:.2f}%" for cls, prob in ranked_classes]
            class_rankings.append(formatted_rankings)

        return class_rankings

    def test_best_RF(
        self, 
        X: np.ndarray, 
        y: np.ndarray, 
        scaler_dir: str = '.'
    ) -> tuple:
        scaler_path = os.path.join(scaler_dir, 'scaler_associated.pkl') if scaler_dir else 'scaler_associated.pkl'
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            logging.info(f"Scaler loaded from {scaler_path}")
        else:
            logging.error(f"Scaler not found at {scaler_path}. It should be scaler_associated.pkl.")
            sys.exit(1)

        X_scaled = scaler.transform(X)
        X_resampled, y_resampled, _, _ = self._oversample_single_sample_classes(X_scaled, y)

        X_train, X_test, y_train, y_test = train_test_split(
            X_resampled, y_resampled, test_size=0.4, random_state=self.seed, stratify=y_resampled
        )

        model = RandomForestClassifier(
            n_estimators=self.best_params.get('n_estimators', 100),
            max_depth=self.best_params.get('max_depth', 5),
            min_samples_split=self.best_params.get('min_samples_split', 4),
            min_samples_leaf=self.best_params.get('min_samples_leaf', 2),
            criterion=self.best_params.get('criterion', 'entropy'),
            max_features=self.best_params.get('max_features', 'log2'),
            class_weight=self.best_params.get('class_weight', 'balanced'),
            max_leaf_nodes=self.best_params.get('max_leaf_nodes', 20),
            min_impurity_decrease=self.best_params.get('min_impurity_decrease', 0.01),
            bootstrap=self.best_params.get('bootstrap', True),
            ccp_alpha=self.best_params.get('ccp_alpha', 0.001),
            random_state=self.seed,
            n_jobs=self.n_jobs
        )
        model.fit(X_train, y_train)

        calibrator = CalibratedClassifierCV(model, method='isotonic', cv=5, n_jobs=self.n_jobs)
        calibrator.fit(X_train, y_train)
        calibrated_model = calibrator

        y_pred = calibrated_model.predict_proba(X_test)
        y_pred_adjusted = adjust_predictions_global(y_pred, method='normalize')

        score = self._calculate_score(y_pred_adjusted, y_test)

        y_pred_classes = calibrated_model.predict(X_test)
        f1 = f1_score(y_test, y_pred_classes, average='weighted')
        if len(np.unique(y_test)) > 1:
            pr_auc = average_precision_score(y_test, y_pred_adjusted, average='macro')
        else:
            pr_auc = 0.0

        return score, f1, pr_auc, self.best_params, calibrated_model, X_test, y_test

    def _calculate_score(self, y_pred: np.ndarray, y_test: np.ndarray) -> float:
        n_classes = len(np.unique(y_test))
        if y_pred.ndim == 1 or n_classes == 2:
            return roc_auc_score(y_test, y_pred)
        elif y_pred.ndim == 2 and n_classes > 2:
            y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
            return roc_auc_score(y_test_bin, y_pred, multi_class='ovo', average='macro')
        else:
            logging.warning(f"Unexpected shape or number of classes: y_pred shape: {y_pred.shape}, number of classes: {n_classes}")
            return 0

    def plot_roc_curve(self, y_true: np.ndarray, y_pred_proba: np.ndarray, title: str, save_as: str = None, classes: list = None) -> None:
        plot_roc_curve_global(y_true, y_pred_proba, title, save_as, classes)


def visualize_latent_space_with_similarity(
    X_original: np.ndarray, 
    X_synthetic: np.ndarray, 
    y_original: np.ndarray, 
    y_synthetic: np.ndarray, 
    protein_ids_original: list, 
    protein_ids_synthetic: list, 
    var_assoc_original: list, 
    var_assoc_synthetic: list, 
    output_dir: str = None
):
    import os
    import logging
    import plotly.graph_objects as go
    import pandas as pd
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import umap.umap_ as umap

    X_combined = np.vstack([X_original, X_synthetic])
    y_combined = np.hstack([y_original, y_synthetic])

    umap_reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=40, min_dist=0.8)
    X_transformed = umap_reducer.fit_transform(X_combined)

    X_transformed_original = X_transformed[:len(X_original)]
    X_transformed_synthetic = X_transformed[len(X_original):]

    similarities = cosine_similarity(X_synthetic, X_original)
    max_similarities = similarities.max(axis=1)
    closest_indices = similarities.argmax(axis=1)

    closest_proteins = []
    closest_vars = []
    for idx in closest_indices:
        if idx < len(protein_ids_original) and idx < len(var_assoc_original):
            closest_proteins.append(protein_ids_original[idx])
            closest_vars.append(var_assoc_original[idx])
        else:
            closest_proteins.append("synthetic_closest_id")
            closest_vars.append("synthetic_closest_var")

    df_original = pd.DataFrame({
        'x': X_transformed_original[:, 0],
        'y': X_transformed_original[:, 1],
        'z': X_transformed_original[:, 2],
        'Protein ID': protein_ids_original,
        'Associated Variable': var_assoc_original,
        'Type': 'Original'
    })

    df_synthetic = pd.DataFrame({
        'x': X_transformed_synthetic[:, 0],
        'y': X_transformed_synthetic[:, 1],
        'z': X_transformed_synthetic[:, 2],
        'Protein ID': protein_ids_synthetic,
        'Associated Variable': var_assoc_synthetic,
        'Similarity': max_similarities,
        'Closest Protein': closest_proteins,
        'Closest Variable': closest_vars,
        'Type': 'Synthetic'
    })

    fig = go.Figure()

    fig.add_trace(go.Scatter3d(
        x=df_original['x'], 
        y=df_original['y'],
        z=df_original['z'],
        mode='markers',
        marker=dict(size=8, color='blue', opacity=0.7),
        name='Original',
        text=df_original.apply(lambda row: f"Protein ID: {row['Protein ID']}<br>Associated Variable: {row['Associated Variable']}", axis=1),
        hoverinfo='text'
    ))

    fig.add_trace(go.Scatter3d(
        x=df_synthetic['x'], 
        y=df_synthetic['y'],
        z=df_synthetic['z'],
        mode='markers',
        marker=dict(size=8, color='red', opacity=0.7),
        name='Synthetic',
        text=df_synthetic.apply(lambda row: f"Protein ID: {row['Protein ID']}<br>Associated Variable: {row['Associated Variable']}<br>Similarity: {row['Similarity']:.4f}<br>Closest Protein: {row['Closest Protein']}<br>Closest Variable: {row['Closest Variable']}", axis=1),
        hoverinfo='text'
    ))

    fig.update_layout(
        title="Latent Space Visualization with Similarity (UMAP 3D)",
        scene=dict(
            xaxis_title="UMAP Dimension 1",
            yaxis_title="UMAP Dimension 2",
            zaxis_title="UMAP Dimension 3"
        ),
        legend=dict(orientation="h", y=-0.1),
        template="plotly_dark"
    )

    if output_dir:
        umap_similarity_path = os.path.join(output_dir, "umap_similarity_3D.html")
        fig.write_html(umap_similarity_path)
        logging.info(f"UMAP plot saved at {umap_similarity_path}")

    return fig



class ProteinEmbeddingGenerator:
    """
    Class to generate protein embeddings using Word2Vec.
    """

    def __init__(self, sequences_path: str, table_data: pd.DataFrame = None, aggregation_method: str = 'none'):
        """
        Initializes the embedding generator.
        
        Parameters:
        - sequences_path (str): Path to the sequences file.
        - table_data (pd.DataFrame): Associated table data.
        - aggregation_method (str): Aggregation method ('none' or 'mean').
        """
        aligned_path = sequences_path
        if not are_sequences_aligned(sequences_path):
            realign_sequences_with_mafft(sequences_path, sequences_path.replace(".fasta", "_aligned.fasta"), threads=1)
            aligned_path = sequences_path.replace(".fasta", "_aligned.fasta")
        else:
            logging.info(f"Sequences are already aligned: {sequences_path}")

        self.alignment = AlignIO.read(aligned_path, 'fasta')
        self.table_data = table_data
        self.embeddings = []
        self.models = {}
        self.aggregation_method = aggregation_method  # Aggregation method: 'none' or 'mean'
        self.min_kmers = None  # To store min_kmers

    def generate_embeddings(
        self, 
        k: int = 3, 
        step_size: int = 1, 
        word2vec_model_path: str = "word2vec_model.bin", 
        model_dir: str = None, 
        min_kmers: int = None, 
        save_min_kmers: bool = False
    ) -> None:
        """
        Generates embeddings for protein sequences using Word2Vec, standardizing the number of k-mers.
        
        Parameters:
        - k (int): Size of the k-mer.
        - step_size (int): Step size for generating k-mers.
        - word2vec_model_path (str): Filename for the Word2Vec model.
        - model_dir (str): Directory to save the Word2Vec model.
        - min_kmers (int): Minimum number of k-mers to use.
        - save_min_kmers (bool): If True, saves min_kmers to a file.
        """
        # Define the full path for the Word2Vec model
        if model_dir:
            word2vec_model_full_path = os.path.join(model_dir, word2vec_model_path)
        else:
            word2vec_model_full_path = word2vec_model_path

        # Check if the Word2Vec model already exists
        if os.path.exists(word2vec_model_full_path):
            logging.info(f"Word2Vec model found at {word2vec_model_full_path}. Loading the model.")
            model = Word2Vec.load(word2vec_model_full_path)
            self.models['global'] = model
        else:
            logging.info("Word2Vec model not found. Training a new model.")
            # Initialize variables
            kmer_groups = {}
            all_kmers = []
            kmers_counts = []

            # Generate k-mers
            for record in self.alignment:
                sequence = str(record.seq)
                seq_len = len(sequence)
                protein_accession_alignment = record.id.split()[0]

                # If table data is not provided, skip matching
                if self.table_data is not None:
                    matching_rows = self.table_data['Protein.accession'].str.split().str[0] == protein_accession_alignment
                    matching_info = self.table_data[matching_rows]

                    if matching_info.empty:
                        logging.warning(f"No matching table data for {protein_accession_alignment}")
                        continue  # Skip to next iteration

                    target_variable = matching_info['Target variable'].values[0]  # Can be removed if not necessary
                    associated_variable = matching_info['Associated variable'].values[0]
                else:
                    # If no table, use default values or None
                    target_variable = None  # Can be removed
                    associated_variable = None

                logging.info(f"Processing {protein_accession_alignment} with sequence length {seq_len}")

                if seq_len < k:
                    logging.warning(f"Sequence too short for {protein_accession_alignment}. Length: {seq_len}")
                    continue

                # Generate k-mers, allowing k-mers with fewer than k gaps
                kmers = [sequence[i:i + k] for i in range(0, seq_len - k + 1, step_size)]
                kmers = [kmer for kmer in kmers if kmer.count('-') < k]  # Allow k-mers with fewer than k gaps

                if not kmers:
                    logging.warning(f"No valid k-mers for {protein_accession_alignment}")
                    continue

                all_kmers.append(kmers)  # Add the list of k-mers as a sentence
                kmers_counts.append(len(kmers))  # Store the count of k-mers

                embedding_info = {
                    'protein_accession': protein_accession_alignment,
                    'target_variable': target_variable,  # Can be removed
                    'associated_variable': associated_variable,
                    'kmers': kmers  # Store k-mers for later use
                }
                kmer_groups[protein_accession_alignment] = embedding_info

            # Determine the minimum number of k-mers
            if not kmers_counts:
                logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
                sys.exit(1)

            if min_kmers is not None:
                self.min_kmers = min_kmers
                logging.info(f"Using provided min_kmers: {self.min_kmers}")
            else:
                self.min_kmers = min(kmers_counts)
                logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

            # Save min_kmers if necessary
            if save_min_kmers and model_dir:
                min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
                with open(min_kmers_path, 'w') as f:
                    f.write(str(self.min_kmers))
                logging.info(f"min_kmers saved at {min_kmers_path}")

            # Train the Word2Vec model using all k-mers
            model = Word2Vec(
                sentences=all_kmers,
                vector_size=125,  # Change to 100 if necessary
                window=window,  # Utilize user-specified window size
                min_count=1,
                workers=workers,  # Utilize user-specified number of workers
                sg=1,
                hs=1,  # Hierarchical softmax enabled
                negative=0,  # Negative sampling disabled
                epochs=epochs,  # Use user-specified number of epochs for reproducibility
                seed=SEED  # Fixed seed for reproducibility
            )

            # Create directory for the Word2Vec model if necessary
            if model_dir:
                os.makedirs(os.path.dirname(word2vec_model_full_path), exist_ok=True)

            # Save the Word2Vec model
            model.save(word2vec_model_full_path)
            self.models['global'] = model
            logging.info(f"Word2Vec model saved at {word2vec_model_full_path}")

        # Generate standardized embeddings
        kmer_groups = {}
        kmers_counts = []
        all_kmers = []

        for record in self.alignment:
            sequence_id = record.id.split()[0]  # Use consistent sequence IDs
            sequence = str(record.seq)

            # If table data is not provided, skip matching
            if self.table_data is not None:
                matching_rows = self.table_data['Protein.accession'].str.split().str[0] == sequence_id
                matching_info = self.table_data[matching_rows]

                if matching_info.empty:
                    logging.warning(f"No matching table data for {sequence_id}")
                    continue  # Skip to next iteration

                target_variable = matching_info['Target variable'].values[0]  # Can be removed
                associated_variable = matching_info['Associated variable'].values[0]
            else:
                # If no table, use default values or None
                target_variable = None  # Can be removed
                associated_variable = None

            kmers = [sequence[i:i + k] for i in range(0, len(sequence) - k + 1, step_size)]
            kmers = [kmer for kmer in kmers if kmer.count('-') < k]  # Allow k-mers with fewer than k gaps

            if not kmers:
                logging.warning(f"No valid k-mers for {sequence_id}")
                continue

            all_kmers.append(kmers)
            kmers_counts.append(len(kmers))

            embedding_info = {
                'protein_accession': sequence_id,
                'target_variable': target_variable,  # Can be removed
                'associated_variable': associated_variable,
                'kmers': kmers
            }
            kmer_groups[sequence_id] = embedding_info

        # Determine the minimum number of k-mers
        if not kmers_counts:
            logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
            sys.exit(1)

        if min_kmers is not None:
            self.min_kmers = min_kmers
            logging.info(f"Using provided min_kmers: {self.min_kmers}")
        else:
            self.min_kmers = min(kmers_counts)
            logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

        # Generate standardized embeddings
        for record in self.alignment:
            sequence_id = record.id.split()[0]  # Use consistent sequence IDs
            embedding_info = kmer_groups.get(sequence_id, {})
            kmers_for_protein = embedding_info.get('kmers', [])

            if len(kmers_for_protein) == 0:
                if self.aggregation_method == 'none':
                    embedding_concatenated = np.zeros(self.models['global'].vector_size * self.min_kmers)
                else:
                    embedding_concatenated = np.zeros(self.models['global'].vector_size)
                self.embeddings.append({
                    'protein_accession': sequence_id,
                    'embedding': embedding_concatenated,
                    'target_variable': embedding_info.get('target_variable'),  # Can be removed
                    'associated_variable': embedding_info.get('associated_variable')
                })
                continue

            # Select the first min_kmers k-mers
            selected_kmers = kmers_for_protein[:self.min_kmers]

            # Pad with zeros if necessary
            if len(selected_kmers) < self.min_kmers:
                padding = [np.zeros(self.models['global'].vector_size)] * (self.min_kmers - len(selected_kmers))
                selected_kmers.extend(padding)

            # Obtain embeddings for selected k-mers
            selected_embeddings = [self.models['global'].wv[kmer] if kmer in self.models['global'].wv else np.zeros(self.models['global'].vector_size) for kmer in selected_kmers]

            if self.aggregation_method == 'none':
                # Concatenate embeddings of selected k-mers
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)
            elif self.aggregation_method == 'mean':
                # Aggregate embeddings of selected k-mers by mean
                embedding_concatenated = np.mean(selected_embeddings, axis=0)
            else:
                # If the method is unrecognized, use concatenation as default
                logging.warning(f"Unknown aggregation method '{self.aggregation_method}'. Using concatenation.")
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)

            self.embeddings.append({
                'protein_accession': sequence_id,
                'embedding': embedding_concatenated,
                'target_variable': embedding_info.get('target_variable'),  # Can be removed
                'associated_variable': embedding_info.get('associated_variable')
            })

            logging.debug(f"Protein ID: {sequence_id}, Embedding Shape: {embedding_concatenated.shape}")

        # Fit StandardScaler with embeddings for training/prediction
        embeddings_array_train = np.array([entry['embedding'] for entry in self.embeddings])

        # Check if all embeddings have the same shape
        embedding_shapes = set(embedding.shape for embedding in [entry['embedding'] for entry in self.embeddings])
        if len(embedding_shapes) != 1:
            logging.error(f"Inconsistent embedding shapes detected: {embedding_shapes}")
            raise ValueError("Embeddings have inconsistent shapes.")
        else:
            logging.info(f"All embeddings have the shape: {embedding_shapes.pop()}")

        # Define the full path for the scaler
        scaler_full_path = os.path.join(model_dir, 'scaler_associated.pkl') if model_dir else 'scaler_associated.pkl'

        # Check if the scaler already exists
        if os.path.exists(scaler_full_path):
            logging.info(f"StandardScaler found at {scaler_full_path}. Loading the scaler.")
            scaler = joblib.load(scaler_full_path)
        else:
            logging.info("StandardScaler not found. Training a new scaler.")
            scaler = StandardScaler().fit(embeddings_array_train)
            joblib.dump(scaler, scaler_full_path)
            logging.info(f"StandardScaler saved at {scaler_full_path}")

    def get_embeddings_and_labels(self, label_type: str = 'associated_variable') -> tuple:
        """
        Returns embeddings and associated labels.
        
        Parameters:
        - label_type (str): Type of label ('associated_variable').
        
        Returns:
        - tuple: (Embeddings, Labels)
        """
        embeddings = []
        labels = []

        for embedding_info in self.embeddings:
            embeddings.append(embedding_info['embedding'])
            labels.append(embedding_info[label_type])  # Use the specified label type

        return np.array(embeddings), np.array(labels)

def compute_perplexity(n_samples: int) -> int:
    """
    Calculates perplexity dynamically (not used, as t-SNE was removed).
    
    Parameters:
    - n_samples (int): Number of samples.
    
    Returns:
    - int: Perplexity value.
    """
    return max(5, min(50, n_samples // 100))

def plot_dual_umap(
    train_embeddings: np.ndarray, 
    train_labels: list,
    train_protein_ids: list,
    predict_embeddings: np.ndarray, 
    predict_labels: list, 
    predict_protein_ids: list, 
    output_dir: str
) -> tuple:
    """
    Plots two 3D UMAP plots and saves them as HTML files:
    - Plot 1: Training Data.
    - Plot 2: Predictions.
    
    Parameters:
    - train_embeddings (np.ndarray): Training embeddings.
    - train_labels (list): Labels of training data.
    - train_protein_ids (list): Protein IDs of training data.
    - predict_embeddings (np.ndarray): Prediction embeddings.
    - predict_labels (list): Prediction labels.
    - predict_protein_ids (list): Protein IDs of predictions.
    - output_dir (str): Directory to save UMAP plots.
    
    Returns:
    - tuple: (Training Figure, Prediction Figure)
    """
    # Dimensionality reduction using UMAP
    umap_train = umap.UMAP(n_components=3, random_state=42, n_neighbors=10, min_dist=0.1)
    umap_train_result = umap_train.fit_transform(train_embeddings)

    umap_predict = umap.UMAP(n_components=3, random_state=42, n_neighbors=10, min_dist=0.1)
    umap_predict_result = umap_predict.fit_transform(predict_embeddings)

    # Create color maps for training data
    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    # Create color maps for predictions
    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    # Convert labels to colors
    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    # Plot 1: Training Data
    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=umap_train_result[:, 0],
        y=umap_train_result[:, 1],
        z=umap_train_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=train_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text'
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='UMAP 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Plot 2: Predictions
    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=umap_predict_result[:, 0],
        y=umap_predict_result[:, 1],
        z=umap_predict_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=predict_colors,
            opacity=0.8
        ),
        # Protein IDs added to 'text'
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='UMAP 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Save the plots as HTML files
    umap_train_html = os.path.join(output_dir, "umap_train_3d.html")
    umap_predict_html = os.path.join(output_dir, "umap_predict_3d.html")
    
    pio.write_html(fig_train, file=umap_train_html, auto_open=False)
    pio.write_html(fig_predict, file=umap_predict_html, auto_open=False)
    
    logging.info(f"UMAP Training plot saved as {umap_train_html}")
    logging.info(f"UMAP Predictions plot saved as {umap_predict_html}")

    return fig_train, fig_predict

def plot_predictions_scatterplot_custom(
    results: dict, 
    output_path: str, 
    top_n: int = 1
) -> None:
    """
    Generates a scatter plot showing only the main category with the highest sum of probabilities for each protein.
    
    Y-Axis: Protein accession ID
    X-Axis: Specificities from C4 to C18 (fixed scale)
    Each point represents the corresponding specificity for the protein.
    Only the main category (top 1) is plotted per protein.
    Points are colored in a single uniform color, styled for scientific publication.
    
    Parameters:
    - results (dict): Dictionary containing predictions and rankings for proteins.
    - output_path (str): Path to save the scatter plot.
    - top_n (int): Number of top main categories to plot (default is 1).
    """
    # Prepare data
    protein_specificities = {}
    
    for seq_id, info in results.items():
        associated_rankings = info.get('associated_ranking', [])
        if not associated_rankings:
            logging.warning(f"No ranking data associated with the protein. {seq_id}. Skipping...")
            continue

        # Use the function format_and_sum_probabilities to get the main category
        top_category_with_confidence, top_sum, top_two_categories = format_and_sum_probabilities(associated_rankings)
        if top_category_with_confidence is None:
            logging.warning(f"No valid formatting for protein {seq_id}. Skipping...")
            continue

        # Extract the category without confidence
        category = top_category_with_confidence.split(" (")[0]
        confidence = top_sum  # Sum of probabilities for the main category

        protein_specificities[seq_id] = {
            'top_category': category,
            'confidence': confidence
        }

    if not protein_specificities:
        logging.warning("No data available to plot the scatter plot.")
        return

    # Sort protein IDs for better visualization
    unique_proteins = sorted(protein_specificities.keys())
    protein_order = {protein: idx for idx, protein in enumerate(unique_proteins)}

    # Create the figure
    fig, ax = plt.subplots(figsize=(12, max(6, len(unique_proteins) * 0.5)))  # Adjust height based on number of proteins

    # Fixed scale for X-axis from C4 to C18
    x_values = list(range(4, 19))

    # Plot points for all proteins with their main category
    for protein, data in protein_specificities.items():
        y = protein_order[protein]
        category = data['top_category']
        confidence = data['confidence']

        # Extract specificities from the category string
        specificities = [int(x[1:]) for x in category.split('-') if x.startswith('C')]

        for spec in specificities:
            ax.scatter(
                spec, y,
                color='#1f78b4',  # Uniform color
                edgecolors='black',
                linewidth=0.5,
                s=100,
                label='_nolegend_'  # Avoid duplication in legend
            )

        # Connect points with lines if multiple specificities
        if len(specificities) > 1:
            ax.plot(
                specificities,
                [y] * len(specificities),
                color='#1f78b4',
                linestyle='-',
                linewidth=1.0,
                alpha=0.7
            )

    # Customize the plot for better scientific publication quality
    ax.set_xlabel('Specificity (C4 to C18)', fontsize=14, fontweight='bold', color='white')
    ax.set_ylabel('Proteins', fontsize=14, fontweight='bold', color='white')
    ax.set_title('Scatter Plot of Predictions for New Sequences (SS Prediction)', fontsize=16, fontweight='bold', pad=20, color='white')

    # Define fixed scale and formatting for the X-axis
    ax.set_xticks(x_values)
    ax.set_xticklabels([f'C{spec}' for spec in x_values], fontsize=12, color='white')
    ax.set_yticks(range(len(unique_proteins)))
    ax.set_yticklabels(unique_proteins, fontsize=10, color='white')

    # Define grid and remove unnecessary spines for a clean look
    ax.grid(True, axis='x', linestyle='--', alpha=0.5, color='gray')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color('white')
    ax.spines['bottom'].set_color('white')

    # Minor ticks on the X-axis for better visibility
    ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
    ax.grid(which='minor', axis='x', linestyle=':', linewidth=0.5, alpha=0.6)

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure in high quality for publication
    plt.savefig(output_path, facecolor='#0B3C5D', dpi=600, bbox_inches='tight')  # Match background color
    plt.close()
    logging.info(f"Scatter plot saved at {output_path}")

def adjust_predictions_global(
    predicted_proba: np.ndarray, 
    method: str = 'normalize', 
    alpha: float = 1.0
) -> np.ndarray:
    """
    Adjusts the predicted probabilities from the model.
    
    Parameters:
    - predicted_proba (np.ndarray): Predicted probabilities from the model.
    - method (str): Adjustment method ('normalize', 'smoothing', 'none').
    - alpha (float): Parameter for smoothing (used if method='smoothing').
    
    Returns:
    - np.ndarray: Adjusted probabilities.
    """
    if method == 'normalize':
        # Normalize probabilities so that they sum to 1 for each sample
        logging.info("Normalizing predicted probabilities.")
        adjusted_proba = predicted_proba / predicted_proba.sum(axis=1, keepdims=True)

    elif method == 'smoothing':
        # Apply smoothing to probabilities to avoid extreme values
        logging.info(f"Applying smoothing to predicted probabilities with alpha={alpha}.")
        adjusted_proba = (predicted_proba + alpha) / (predicted_proba.sum(axis=1, keepdims=True) + alpha * predicted_proba.shape[1])

    elif method == 'none':
        # Do not apply any adjustment
        logging.info("No adjustment applied to predicted probabilities.")
        adjusted_proba = predicted_proba.copy()

    else:
        logging.warning(f"Unknown adjustment method '{method}'. No adjustment will be applied.")
        adjusted_proba = predicted_proba.copy()

    return adjusted_proba

def main(args: argparse.Namespace) -> None:
    """
    Main function coordinating the workflow.
    
    Parameters:
    - args (argparse.Namespace): Input arguments.
    """
    model_dir = args.model_dir

    # Initialize progress variables
    total_steps = 5  # Adjusted to include plot_dual_umap
    current_step = 0
    progress_bar = st.progress(0)
    progress_text = st.empty()

    # =============================
    # STEP 1: Training the Model for associated_variable
    # =============================

    # Load training data
    train_alignment_path = args.train_fasta
    train_table_data_path = args.train_table

    # Check if training sequences are aligned
    if not are_sequences_aligned(train_alignment_path):
        logging.info("Training sequences are not aligned. Realigning with MAFFT...")
        aligned_train_path = train_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(train_alignment_path, aligned_train_path, threads=1)  # Threads set to 1
        train_alignment_path = aligned_train_path
    else:
        logging.info(f"Aligned training file found or sequences already aligned: {train_alignment_path}")

    # Load training table data
    train_table_data = pd.read_csv(train_table_data_path, delimiter="\t")
    logging.info("Training table data loaded successfully.")

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Initialize and generate embeddings for training
    protein_embedding_train = ProteinEmbeddingGenerator(
        train_alignment_path, 
        table_data=train_table_data, 
        aggregation_method=args.aggregation_method  # Passing the aggregation method ('none' or 'mean')
    )
    protein_embedding_train.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        save_min_kmers=True  # Save min_kmers after training
    )
    logging.info(f"Number of training embeddings generated: {len(protein_embedding_train.embeddings)}")

    # Save min_kmers to ensure consistency
    min_kmers = protein_embedding_train.min_kmers

    # Obtaining protein IDs and associated variables from the training set
    protein_ids_associated = [entry['protein_accession'] for entry in protein_embedding_train.embeddings]
    var_assoc_associated = [entry['associated_variable'] for entry in protein_embedding_train.embeddings]

    logging.info(f"Protein IDs for associated_variable extracted: {len(protein_ids_associated)}")
    logging.info(f"Associated variables for associated_variable extracted: {len(var_assoc_associated)}")

    # Get embeddings and labels for associated_variable
    X_associated, y_associated = protein_embedding_train.get_embeddings_and_labels(label_type='associated_variable')
    logging.info(f"Shape of X_associated: {X_associated.shape}")

    # Create scaler for X_associated
    scaler_associated = StandardScaler().fit(X_associated)
    scaler_associated_path = os.path.join(model_dir, 'scaler_associated.pkl')
    joblib.dump(scaler_associated, scaler_associated_path)
    logging.info("Scaler for X_associated created and saved.")

    # Scale the X_associated data
    X_associated_scaled = scaler_associated.transform(X_associated)    

    # Full paths for associated_variable models
    rf_model_associated_full_path = os.path.join(model_dir, args.rf_model_associated)
    calibrated_model_associated_full_path = os.path.join(model_dir, 'calibrated_model_associated.pkl')

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Check if the calibrated model for associated_variable already exists
    if os.path.exists(calibrated_model_associated_full_path):
        calibrated_model_associated = joblib.load(calibrated_model_associated_full_path)
        logging.info(f"Random Forest calibrated model for associated_variable loaded from {calibrated_model_associated_full_path}")
    else:
        # Train the model for associated_variable
        support_model_associated = Support()
        calibrated_model_associated = support_model_associated.fit(
            X_associated_scaled, 
            y_associated, 
            protein_ids=protein_ids_associated,  # Pass protein IDs for visualization
            var_assoc=var_assoc_associated, 
            model_name_prefix='associated', 
            model_dir=model_dir, 
            min_kmers=min_kmers
        )

        logging.info("Training and calibration for associated_variable completed.")

        # Plot the learning curve
        logging.info("Plotting Learning Curve for associated_variable")
        learning_curve_associated_path = args.learning_curve_associated
        support_model_associated.plot_learning_curve(learning_curve_associated_path)

        # Save the calibrated model
        joblib.dump(calibrated_model_associated, calibrated_model_associated_full_path)
        logging.info(f"Random Forest calibrated model for associated_variable saved at {calibrated_model_associated_full_path}")

        # Test the model
        best_score_associated, best_f1_associated, best_pr_auc_associated, best_params_associated, best_model_associated, X_test_associated, y_test_associated = support_model_associated.test_best_RF(
            X_associated_scaled, 
            y_associated
        )

        logging.info(f"Best ROC AUC for associated_variable: {best_score_associated}")
        logging.info(f"Best F1 Score for associated_variable: {best_f1_associated}")
        logging.info(f"Best Precision-Recall AUC for associated_variable: {best_pr_auc_associated}")
        logging.info(f"Best Parameters: {best_params_associated}")

        for param, value in best_params_associated.items():
            logging.info(f"{param}: {value}")

        # Get class rankings for associated_variable
        class_rankings_associated = support_model_associated.get_class_rankings(X_test_associated)
        logging.info("Top 3 class rankings for the first 5 samples in the associated_variable data:")
        for i in range(min(5, len(class_rankings_associated))):
            logging.info(f"Sample {i+1}: Rankings of classes - {class_rankings_associated[i][:3]}")  # Shows top 3 rankings

        # Accessing class_weight from best_params_associated
        class_weight = best_params_associated.get('class_weight', None)
        # Printing results
        logging.info(f"Class weight used: {class_weight}")

        # Save the trained model for associated_variable
        joblib.dump(best_model_associated, rf_model_associated_full_path)
        logging.info(f"Random Forest model for associated_variable saved at {rf_model_associated_full_path}")

        # Plot ROC curve for associated_variable
        n_classes_associated = len(np.unique(y_test_associated))
        if n_classes_associated == 2:
            y_pred_proba_associated = best_model_associated.predict_proba(X_test_associated)[:, 1]
            classes_associated = None  # Binary classification
        else:
            y_pred_proba_associated = best_model_associated.predict_proba(X_test_associated)
            classes_associated = np.unique(y_test_associated).astype(str)
        plot_roc_curve_global(
            y_test_associated, 
            y_pred_proba_associated, 
            'ROC Curve for Associated Variable', 
            save_as=args.roc_curve_associated, 
            classes=classes_associated
        )

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # =============================
    # STEP 2: Classifying New Sequences for associated_variable
    # =============================

    # Load min_kmers
    min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
    if os.path.exists(min_kmers_path):
        with open(min_kmers_path, 'r') as f:
            min_kmers_loaded = int(f.read().strip())
        logging.info(f"min_kmers loaded: {min_kmers_loaded}")
    else:
        logging.error(f"min_kmers file not found at {min_kmers_path}. Please make sure the training was completed successfully.")
        sys.exit(1)

    # Load prediction data
    predict_alignment_path = args.predict_fasta

    # Check if prediction sequences are aligned
    if not are_sequences_aligned(predict_alignment_path):
        logging.info("Sequences for prediction are not aligned. Realigning with MAFFT...")
        aligned_predict_path = predict_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(predict_alignment_path, aligned_predict_path, threads=1)  # Threads set to 1
        predict_alignment_path = aligned_predict_path
    else:
        logging.info(f"Aligned file for prediction found or sequences already aligned: {predict_alignment_path}")

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Initialize ProteinEmbedding for prediction, no need for the table
    protein_embedding_predict = ProteinEmbeddingGenerator(
        predict_alignment_path, 
        table_data=None,
        aggregation_method=args.aggregation_method  # Passing the aggregation method ('none' or 'mean')
    )
    protein_embedding_predict.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        min_kmers=min_kmers_loaded  # Use the same min_kmers from training
    )
    logging.info(f"Number of embeddings generated for prediction: {len(protein_embedding_predict.embeddings)}")

    # Get embeddings for prediction
    X_predict = np.array([entry['embedding'] for entry in protein_embedding_predict.embeddings])

    # Load scalers for associated_variable
    scaler_associated_path = os.path.join(model_dir, 'scaler_associated.pkl')

    if os.path.exists(scaler_associated_path):
        scaler_associated = joblib.load(scaler_associated_path)
        logging.info("Scaler for associated_variable loaded successfully.")
    else:
        logging.error("Scalers not found. Please make sure the training was completed successfully.")
        sys.exit(1)

    # Scale prediction embeddings using scaler_associated
    X_predict_scaled_associated = scaler_associated.transform(X_predict)

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Perform prediction for associated_variable
    predictions_associated = calibrated_model_associated.predict(X_predict_scaled_associated)

    # Get class rankings
    rankings_associated = get_class_rankings_global(calibrated_model_associated, X_predict_scaled_associated)

    # Process and save results
    results = {}
    for entry, pred_associated, ranking_associated in zip(
        protein_embedding_predict.embeddings,
        predictions_associated,
        rankings_associated
    ):
        sequence_id = entry['protein_accession']
        results[sequence_id] = {
            "associated_prediction": pred_associated,
            "associated_ranking": ranking_associated
        }

    # Save results to a file
    with open(args.results_file, 'w') as f:
        f.write("Protein_ID\tAssociated_Prediction\tAssociated_Ranking\n")
        for seq_id, result in results.items():
            f.write(f"{seq_id}\t{result['associated_prediction']}\t{'; '.join(result['associated_ranking'])}\n")
            logging.info(f"{seq_id} - Associated Variable: {result['associated_prediction']}, Associated Ranking: {'; '.join(result['associated_ranking'])}")

    # Generate the Scatter Plot of Predictions
    logging.info("Generating scatter plot of predictions for new sequences...")
    plot_predictions_scatterplot_custom(results, args.scatterplot_output)
    logging.info(f"Scatter plot saved at {args.scatterplot_output}")

    # =============================
    # STEP 3: Plot Dual UMAP
    # =============================

    # Plot Dual UMAP for Training and Prediction Data
    logging.info("Generating Dual UMAP plots for training and prediction data...")
    train_labels = y_associated
    predict_labels = predictions_associated
    train_protein_ids = protein_ids_associated
    predict_protein_ids = [entry['protein_accession'] for entry in protein_embedding_predict.embeddings]

    plot_dual_umap(
        train_embeddings=X_associated_scaled, 
        train_labels=train_labels,
        train_protein_ids=train_protein_ids,
        predict_embeddings=X_predict_scaled_associated, 
        predict_labels=predict_labels, 
        predict_protein_ids=predict_protein_ids, 
        output_dir=model_dir
    )
    logging.info("Dual UMAP plots generated successfully.")
    
    # Generate Dual t-SNE
    fig_tsne_train, fig_tsne_predict = plot_dual_tsne(
        train_embeddings=X_associated_scaled, 
        train_labels=train_labels,
        train_protein_ids=train_protein_ids,
        predict_embeddings=X_predict_scaled_associated, 
        predict_labels=predict_labels, 
        predict_protein_ids=predict_protein_ids, 
        output_dir=model_dir
    )
    logging.info("Dual UMAP and Dual t-SNE plots generated successfully.")
    
    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    st.success("Analysis completed successfully!")

    # Display scatter plot
    st.header("Scatter Plot of Predictions")
    scatterplot_path = args.scatterplot_output
    if os.path.exists(scatterplot_path):
        st.image(scatterplot_path, use_column_width=True)
    else:
        st.error(f"Scatter plot not found at {scatterplot_path}")

    # Format the results
    formatted_results = []

    for sequence_id, info in results.items():
        associated_rankings = info.get('associated_ranking', [])
        if not associated_rankings:
            logging.warning(f"No ranking data associated with the protein. {sequence_id}. Skipping...")
            continue

        # Use the function format_and_sum_probabilities to get the main category
        top_specificity, confidence, top_two_specificities = format_and_sum_probabilities(associated_rankings)
        if top_specificity is None:
            logging.warning(f"No valid formatting for protein {sequence_id}. Skipping...")
            continue
        formatted_results.append([
            sequence_id,
            top_specificity,
            f"{confidence:.2f}%",
            "; ".join(top_two_specificities)
        ])

    # Convert to pandas DataFrame
    headers = ["Query Name", "SS Prediction Specificity", "Prediction Confidence", "Top 2     Specificities"]
    df_results = pd.DataFrame(formatted_results, columns=headers)


    # FunÃ§Ã£o para aplicar estilos personalizados
    def highlight_table(df):
        return df.style.set_table_styles([
            {
                'selector': 'th',
                'props': [
                    ('background-color', '#1E3A8A'),  # Dark blue for headers
                    ('color', 'white'),
                    ('border', '1px solid white'),
                    ('font-weight', 'bold'),
                    ('text-align', 'center')
                ]
            },
            {
                'selector': 'td',
                'props': [
                    ('background-color', '#0B3C5D'),  # Navy blue for odd rows
                    ('color', 'white'),
                    ('border', '1px solid white'),
                    ('text-align', 'center'),
                    ('font-family', 'Arial'),
                    ('font-size', '12px'),
                ]
            },
            {
                'selector': 'tr:nth-child(even) td',
                'props': [
                    ('background-color', '#145B9C')  # Slightly lighter blue for even rows
                ]
            },
            {
                'selector': 'tr:hover td',
                'props': [
                    ('background-color', '#0D4F8B')  # Darker blue on hover
                ]
            },
        ])

    # Aplicar estilos ao DataFrame
    styled_df = highlight_table(df_results)

    # Renderizar a tabela estilizada como HTML
    html = styled_df.to_html(index=False, escape=False)

    # Injetar CSS para download buttons e ajustar estilos adicionais
    st.markdown(
        """
        <style>
        /* Estilo para download buttons */
        .stButton > button {
            background-color: #1E3A8A;
            color: white;
            border: none;
            padding: 10px 24px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
    
        /* Hover effect para buttons */
        .stButton > button:hover {
            background-color: #0B3C5D;
        }
    
        /* Estilo para a tabela */
        table {
            border-collapse: collapse;
            width: 100%;
        }
    
        /* Ajustar scroll para a tabela */
        .dataframe-container {
            overflow-x: auto;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    # Exibir a tabela no Streamlit
    st.header("Formatted Results")

    st.markdown(
        f"""
        <div class="dataframe-container">
            {html}
        </div>
        """,
        unsafe_allow_html=True
    )

    # Opcional: Adicionar botÃµes para download da tabela em CSV e Excel
    # EstilizaÃ§Ã£o dos botÃµes jÃ¡ estÃ¡ coberta pelo CSS acima

    # EstilizaÃ§Ã£o personalizada com CSS para os botÃµes
    st.markdown("""
        <style>
        .stDownloadButton > button {
            background-color: #1E3A8A; /* Azul escuro */
            color: white; /* Texto branco */
            border: none;
            padding: 10px 20px;
            text-align: center;
            text-decoration: none;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
        .stDownloadButton > button:hover {
            background-color: #145B9C; /* Azul mais claro no hover */
        }
        </style>
    """, unsafe_allow_html=True)

    # BotÃ£o para download em CSV
    st.download_button(
        label="Download Results as CSV",
        data=df_results.to_csv(index=False).encode('utf-8'),
        file_name='results.csv',
        mime='text/csv',
    )

    # BotÃ£o para download em Excel
    output = BytesIO() 
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_results.to_excel(writer, index=False, sheet_name='Results')
        writer.close()

        processed_data = output.getvalue()

    st.download_button(
        label="Download Results as Excel",
        data=processed_data,
        file_name='results.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    )

    # Prepare results.zip file
    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zip_file:
        for folder_name, subfolders, filenames in os.walk(output_dir):
            for filename in filenames:
                file_path = os.path.join(folder_name, filename)
                zip_file.write(file_path, arcname=os.path.relpath(file_path, output_dir))
    zip_buffer.seek(0)

    # Provide download link
    st.header("Download All Results")
    st.download_button(
        label="Download All Results as results.zip",
        data=zip_buffer,
        file_name="results.zip",
        mime="application/zip"
    )

    # Salvar os resultados em um arquivo Excel
    df = pd.DataFrame(formatted_results, columns=headers)
    df.to_excel(args.excel_output, index=False)
    logging.info(f"Resultados salvos em {args.excel_output}")

    # Salvar a tabela em formato tabulado
    with open(args.formatted_results_table, 'w') as f:
        f.write(tabulate(formatted_results, headers=headers, tablefmt="grid"))
    logging.info(f"Formatted table saved at {args.formatted_results_table}")
    
    
    
    umap_similarity_path = os.path.join(output_dir, "umap_similarity_3D.html")
    # Salvar os Dual UMAP plots na pasta de saÃ­da para inclusÃ£o no ZIP
    dual_umap_train_path = os.path.join(output_dir, "umap_train_3d.html")
    dual_umap_predict_path = os.path.join(output_dir, "umap_predict_3d.html")
    
    # Salvar os Dual tsNE plots na pasta de saÃ­da para inclusÃ£o no ZIP    
    tsne_train_html = os.path.join(output_dir, "tsne_train_3d.html")
    tsne_predict_html = os.path.join(output_dir, "tsne_predict_3d.html")
    
    # CSS personalizado para fundo azul marinho escuro e texto branco
    # CSS personalizado para fundo azul marinho escuro e texto branco
st.markdown(
    """
    <style>
    /* Define o fundo principal do app e a cor do texto */
    .stApp {
        background-color: #0B3C5D;
        color: white;
    }
    /* Define o fundo da barra lateral e a cor do texto */
    [data-testid="stSidebar"] {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    /* Garante que todos os elementos dentro da barra lateral tenham fundo azul marinho escuro e texto branco */
    [data-testid="stSidebar"] * {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    /* Personaliza elementos de entrada dentro da barra lateral */
    [data-testid="stSidebar"] input,
    [data-testid="stSidebar"] select,
    [data-testid="stSidebar"] textarea,
    [data-testid="stSidebar"] button,
    [data-testid="stSidebar"] .stButton,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Personaliza a Ã¡rea de arrastar e soltar do uploader de arquivos */
    [data-testid="stSidebar"] div[data-testid="stFileUploader"] div {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Personaliza as opÃ§Ãµes do dropdown de seleÃ§Ã£o */
    [data-testid="stSidebar"] .stSelectbox [role="listbox"] {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Remove bordas e sombras */
    [data-testid="stSidebar"] .stButton > button,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        border: none !important;
        box-shadow: none !important;
    }
    /* Personaliza checkboxes e radios */
    [data-testid="stSidebar"] .stCheckbox input[type="checkbox"] + div:first-of-type,
    [data-testid="stSidebar"] .stRadio input[type="radio"] + div:first-of-type {
        background-color: #1E3A8A !important;
    }
    /* Personaliza a barra de sliders */
    [data-testid="stSidebar"] .stSlider > div:first-of-type {
        color: white !important;
    }
    [data-testid="stSidebar"] .stSlider .st-bo {
        background-color: #1E3A8A !important;
    }
    /* Garante que os cabeÃ§alhos sejam brancos */
    h1, h2, h3, h4, h5, h6 {
        color: white !important;
    }
    /* Garante que mensagens de alerta (st.info, st.error, etc.) tenham texto branco */
    div[role="alert"] p {
        color: white !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

def get_base64_image(image_path: str) -> str:
    """
    Encode an image file to a base64 string.

    
    Parameters:
    - image_path (str): Path to the image file.

    
    Retorna:
    - str: String base64 da imagem.
    """
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return ""

# Caminho para a imagem

# Caminho para a imagem
image_path = "./images/faal.png"
image_base64 = get_base64_image(image_path)
# Using HTML with st.markdown to align title and text
st.markdown(
    f"""
    <div style="text-align: center; font-family: 'Arial', sans-serif; padding: 30px; background: linear-gradient(to bottom, #f9f9f9, #ffffff); border-radius: 15px; border: 2px solid #dddddd; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); position: relative;">
        <p style="color: black; font-size: 1.5em; font-weight: bold; margin: 0;">
            FAALPred: Predicting Fatty Acyl Chain Specificities in Fatty Acyl-AMP Ligases (FAALs) Using Integrated Approaches of Neural Networks, Bioinformatics, and Machine Learning
        </p>
        <p style="color: #2c3e50; font-size: 1.2em; font-weight: normal; margin-top: 10px;">
            Anne Liong, Leandro de Mattos Pereira, and Pedro LeÃ£o
        </p>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8;">
            <strong>FAALPred</strong> is a comprehensive bioinformatics tool designed to predict the fatty acid chain length specificity of substrates, ranging from C4 to C18.
        </p>
        <h5 style="color: #2c3e50; font-size: 20px; font-weight: bold; margin-top: 25px;">ABSTRACT</h5>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8; text-align: justify;">
            Fatty Acyl-AMP Ligases (FAALs), identified by Zhang et al. (2011), activate fatty acids of varying lengths for the biosynthesis of natural products. 
            These substrates enable the production of compounds such as nocuolin (<em>Nodularia sp.</em>, Martins et al., 2022) 
            and sulfolipid-1 (<em>Mycobacterium tuberculosis</em>, Yan et al., 2023), with applications in cancer and tuberculosis treatment 
            (Kurt et al., 2017; Gilmore et al., 2012). Dr. Pedro LeÃ£o and his team identified several of these natural products in cyanobacteria (<a href="https://leaolab.wixsite.com/leaolab" target="_blank" style="color: #3498db; text-decoration: none;">visit here</a>), 
            and FAALPred classifies FAALs by their substrate specificity.
        </p>
        <div style="text-align: center; margin-top: 20px;">
            <img src="data:image/png;base64,{image_base64}" alt="FAAL Domain" style="width: auto; height: 120px; object-fit: contain;">
            <p style="text-align: center; color: #2c3e50; font-size: 14px; margin-top: 5px;">
                <em>FAAL Domain of Synechococcus sp. PCC7002, link: <a href="https://www.rcsb.org/structure/7R7F" target="_blank" style="color: #3498db; text-decoration: none;">https://www.rcsb.org/structure/7R7F</a></em>
            </p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)


# Sidebar for input parameters
st.sidebar.header("Input Parameters")

# Function to save uploaded files
def save_uploaded_file(uploaded_file, save_path: str) -> str:
    """
    Saves a file uploaded by the user.
    
    Parameters:
    - uploaded_file: File uploaded by the user.
    - save_path (str): Path to save the file.
    
    Returns:
    - str: Path to the saved file.
    """
    with open(save_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())
    return save_path

# Input options
use_default_train = st.sidebar.checkbox("Use Default Training Data", value=True)
if not use_default_train:
    train_fasta_file = st.sidebar.file_uploader("Upload Training FASTA File", type=["fasta", "fa", "fna"])
    train_table_file = st.sidebar.file_uploader("Upload Training Table File (TSV)", type=["tsv"])
else:
    train_fasta_file = None
    train_table_file = None

predict_fasta_file = st.sidebar.file_uploader("Upload Prediction FASTA File", type=["fasta", "fa", "fna"])


kmer_size = st.sidebar.number_input("K-mer Size", min_value=1, max_value=10, value=3, step=1)
step_size = st.sidebar.number_input("Step Size", min_value=1, max_value=10, value=1, step=1)

aggregation_method = st.sidebar.selectbox(
    "Aggregation Method",
    options=['none', 'mean'],  # Only 'none' and 'mean' are options
    index=0
)

# Optional Word2Vec Parameters
st.sidebar.header("Customize Word2Vec Parameters")
custom_word2vec = st.sidebar.checkbox("Customize Word2Vec Parameters", value=False)
if custom_word2vec:
    window = st.sidebar.number_input(
        "Window Size", min_value=5, max_value=20, value=10, step=5
    )
    workers = st.sidebar.number_input(
        "Workers", min_value=1, max_value=112, value=8, step=8
    )
    epochs = st.sidebar.number_input(
        "Epochs", min_value=1, max_value=2500, value=2500, step=100
    )
else:
    window = 10  # Default value
    workers = 8  # Default value
    epochs = 2500  # Default value

# Output directory based on aggregation method
model_dir = create_unique_model_directory("results", aggregation_method)
output_dir = model_dir

# Button to start processing
if st.sidebar.button("Run Analysis"):
    # Paths for internal data
    internal_train_fasta = "data/train.fasta"
    internal_train_table = "data/train_table.tsv"

    # Handling training data
    if use_default_train:
        train_fasta_path = internal_train_fasta
        train_table_path = internal_train_table
        st.markdown("<span style='color:white'>Using default training data.</span>", unsafe_allow_html=True)

    else:
        if train_fasta_file is not None and train_table_file is not None:
            train_fasta_path = os.path.join(output_dir, "uploaded_train.fasta")
            train_table_path = os.path.join(output_dir, "uploaded_train_table.tsv")
            save_uploaded_file(train_fasta_file, train_fasta_path)
            save_uploaded_file(train_table_file, train_table_path)
            st.markdown("<span style='color:white'>Uploaded training data will be used.</span>", unsafe_allow_html=True)

        else:
            st.error("Please upload both the training FASTA file and the training TSV table file.")
            st.stop()

    # Handling prediction data
    if predict_fasta_file is not None:
        predict_fasta_path = os.path.join(output_dir, "uploaded_predict.fasta")
        save_uploaded_file(predict_fasta_file, predict_fasta_path)
    else:
        st.error("Please upload a FASTA file for prediction.")
        st.stop()
        
    # Remaining parameters
    args = argparse.Namespace(
        train_fasta=train_fasta_path,
        train_table=train_table_path,
        predict_fasta=predict_fasta_path,
        kmer_size=kmer_size,
        step_size=step_size,
        aggregation_method=aggregation_method,
        results_file=os.path.join(output_dir, "predictions.tsv"),
        output_dir=output_dir,
        scatterplot_output=os.path.join(output_dir, "scatterplot_predictions.png"),
        excel_output=os.path.join(output_dir, "results.xlsx"),
        formatted_results_table=os.path.join(output_dir, "formatted_results.txt"),
        roc_curve_associated=os.path.join(output_dir, "roc_curve_associated.png"),
        learning_curve_associated=os.path.join(output_dir, "learning_curve_associated.png"),
        roc_values_associated=os.path.join(output_dir, "roc_values_associated.csv"),
        rf_model_associated="rf_model_associated.pkl",
        word2vec_model="word2vec_model.bin",
        scaler="scaler_associated.pkl",  # Corrected scaler name
        model_dir=model_dir,
    )

    # Create model directory if it does not exist
    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)

    # Run the main analysis function
    st.markdown("<span style='color:white'>Processing data and running analysis...</span>", unsafe_allow_html=True)
    try:
        main(args)

    except Exception as e:
        st.error(f"An error occurred during processing: {e}")
        logging.error(f"An error occurred: {e}")

# Function to load and resize images with DPI adjustment
def load_and_resize_image_with_dpi(image_path: str, base_width: int, dpi: int = 300) -> Image.Image:
    """
    Loads and resizes an image with DPI adjustment.
    
    Parameters:
    - image_path (str): Path to the image file.
    - base_width (int): Base width for resizing.
    - dpi (int): DPI for the image.
    
    Returns:
    - Image.Image: Resized image object.
    """
    try:
        # Load the image
        image = Image.open(image_path)
        # Calculate the new height proportionally based on the base width
        w_percent = (base_width / float(image.size[0]))
        h_size = int((float(image.size[1]) * float(w_percent)))
        # Resize the image
        resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)
        return resized_image
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return None

# Function to encode images to base64
def encode_image(image: Image.Image) -> str:
    """
    Encodes an image as a base64 string.
    
    Parameters:
    - image (Image.Image): Image object.
    
    Returns:
    - str: Base64 string of the image.
    """
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

# Definitions of image paths
image_dir = "images"
image_paths = [
    os.path.join(image_dir, "lab_logo.png"),
    os.path.join(image_dir, "ciimar.png"),
    os.path.join(image_dir, "faal_pred_logo.png"), 
    os.path.join(image_dir, "bbf4.png"),
    os.path.join(image_dir, "google.png"),
    os.path.join(image_dir, "uniao.png"),
]

# Load and resize all images
images = [load_and_resize_image_with_dpi(path, base_width=150, dpi=300) for path in image_paths]

# Encode images to base64
encoded_images = [encode_image(img) for img in images if img is not None]

# CSS for footer layout
st.markdown(
    """
    <style>
    .footer-container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 10px;
        margin-bottom: 10px;
        flex-wrap: wrap;
    }
    .footer-text {
        text-align: center;
        color: white;
        font-size: 12px;
        margin-top: 10px;
    }
    .support-text {
        text-align: center;
        color: white;
        font-size: 14px;
        font-weight: bold;
        margin-bottom: 5px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)



def plot_learning_curve(self, output_path: str) -> None:
    """
    Plots the learning curve of the model.
    
    Parameters:
    - output_path (str): Path to save the plot.
    """
    plt.figure()
    plt.plot(self.train_scores, label='Training Score')
    plt.plot(self.test_scores, label='Cross-Validation Score')
    plt.plot(self.f1_scores, label='F1 Score')
    plt.plot(self.pr_auc_scores, label='Precision-Recall AUC')
    plt.title("Learning Curve", color='white')
    plt.xlabel("Fold", fontsize=12, fontweight='bold', color='white')
    plt.ylabel("Score", fontsize=12, fontweight='bold', color='white')
    plt.legend(loc="best")
    plt.grid(color='white', linestyle='--', linewidth=0.5)
    plt.savefig(output_path, facecolor='#0B3C5D')  # Match background color
    plt.close()

# FunÃ§Ã£o para carregar e redimensionar imagens com ajuste de DPI
def load_and_resize_image_with_dpi(image_path: str, base_width: int, dpi: int = 300) -> Image.Image:
    """
    Carrega e redimensiona uma imagem com ajuste de DPI.
    
    ParÃ¢metros:
    - image_path (str): Caminho para o arquivo de imagem.
    - base_width (int): Largura base para redimensionamento.
    - dpi (int): DPI para a imagem.
    
    Retorna:
    - Image.Image: Objeto de imagem redimensionada.
    """
    try:
        # Carrega a imagem
        image = Image.open(image_path)
        # Calcula a nova altura proporcional Ã  largura base
        w_percent = (base_width / float(image.size[0]))
        h_size = int((float(image.size[1]) * float(w_percent)))
        # Redimensiona a imagem
        resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)
        return resized_image
    except FileNotFoundError:
        logging.error(f"Imagem nÃ£o encontrada em {image_path}.")
        return None

# FunÃ§Ã£o para codificar imagens em base64
def encode_image(image: Image.Image) -> str:
    """
    Codifica uma imagem como uma string base64.
    
    ParÃ¢metros:
    - image (Image.Image): Objeto de imagem.
    
    Retorna:
    - str: String base64 da imagem.
    """
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

# DefiniÃ§Ãµes dos caminhos das imagens
image_dir = "images"
image_paths = [
    os.path.join(image_dir, "lab_logo.png"),
    os.path.join(image_dir, "ciimar.png"),
    os.path.join(image_dir, "faal_pred_logo.png"), 
    os.path.join(image_dir, "bbf4.png"),
    os.path.join(image_dir, "google.png"),
    os.path.join(image_dir, "uniao.png"),
]

# Carrega e redimensiona todas as imagens
images = [load_and_resize_image_with_dpi(path, base_width=150, dpi=300) for path in image_paths]

# Codifica as imagens em base64
encoded_images = [encode_image(img) for img in images if img is not None]

# CSS para layout do rodapÃ©
st.markdown(
    """
    <style>
    .footer-container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 10px;
        margin-bottom: 10px;
        flex-wrap: wrap;
    }
    .footer-text {
        text-align: center;
        color: white;
        font-size: 12px;
        margin-top: 10px;
    }
    .support-text {
        text-align: center;
        color: white;
        font-size: 14px;
        font-weight: bold;
        margin-bottom: 5px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# HTML para exibir imagens no rodapÃ©
footer_html = """
<div class="support-text">Supported by:</div>
<div class="footer-container">
    {}
</div>
<div class="footer-text">
    CIIMAR - Pedro LeÃ£o @CNP - 2024 - All rights reserved.
</div>

"""

# Gera tags <img> para cada imagem
img_tags = "".join(
    f'<img src="data:image/png;base64,{img}" style="width: 100px;">' for img in encoded_images
)

# Renderiza o rodapÃ©
st.markdown(footer_html.format(img_tags), unsafe_allow_html=True)

# ============================================
# Fim do Script
# ============================================

