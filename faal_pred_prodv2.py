import logging
import os
import sys
import subprocess
import random
import zipfile
from collections import Counter
from io import BytesIO
import shutil
import time
import argparse 
import numpy as np
import pandas as pd
from Bio import SeqIO, AlignIO
from Bio.Align.Applications import MafftCommandline
import joblib
import plotly.io as pio
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import auc, roc_auc_score, roc_curve, f1_score, average_precision_score
from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.calibration import CalibratedClassifierCV
from sklearn.cluster import DBSCAN, KMeans
from tabulate import tabulate
from PIL import Image
from matplotlib import ticker
import base64
import streamlit as st
import plotly.express as px

# Fixing seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,  # Change to DEBUG for more verbosity
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("logs/app.log"),  # Log to file for persistent records
    ],
)

# ============================================
# Streamlit Configuration and Interface
# ============================================

# Ensure st.set_page_config is the very first Streamlit command
st.set_page_config(
    page_title="FAAL_Pred",
    page_icon="ðŸ’»",  # Computer symbol
    layout="wide",
    initial_sidebar_state="expanded",
)

def are_sequences_aligned(fasta_file):
    """
    Checks if all sequences in a FASTA file have the same length.
    """
    lengths = set()
    for record in SeqIO.parse(fasta_file, "fasta"):
        lengths.add(len(record.seq))
    return len(lengths) == 1  # Returns True if all sequences have the same length

def create_unique_model_directory(base_dir, aggregation_method):
    """
    Creates a unique model directory based on the aggregation method.
    
    Parameters:
    - base_dir (str): The base directory for models.
    - aggregation_method (str): The aggregation method used.

    Returns:
    - model_dir (str): Path to the unique model directory.
    """
    model_dir = os.path.join(base_dir, f"models_{aggregation_method}")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    return model_dir

def is_mafft_installed():
    """
    Checks if MAFFT is installed and accessible in the system's PATH.
    
    Returns:
    - bool: True if MAFFT is installed, False otherwise.
    """
    try:
        subprocess.run(['mafft', '--version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def realign_sequences_with_mafft(input_path, output_path, threads=8):
    """
    Realigns sequences using MAFFT.
    
    Parameters:
    - input_path (str): Path to the input FASTA file.
    - output_path (str): Path where the aligned FASTA file will be saved.
    - threads (int): Number of threads to use for MAFFT.
    """
    if not is_mafft_installed():
        logging.error("MAFFT is not installed or not found in PATH.")
        st.error("MAFFT is required but not installed. Por favor, instale o MAFFT e tente novamente.")
        sys.exit(1)
    
    mafft_command = ['mafft', '--thread', str(threads), '--maxiterate', '1000', '--localpair', input_path]
    try:
        with open(output_path, "w") as outfile:
            subprocess.run(mafft_command, stdout=outfile, stderr=subprocess.PIPE, check=True)
        logging.info(f"Realigned sequences saved to {output_path}")
    except subprocess.CalledProcessError as e:
        logging.error(f"Error executing MAFFT: {e.stderr.decode()}")
        st.error("Ocorreu um erro ao executar o MAFFT. Verifique os logs para mais detalhes.")
        sys.exit(1)

def perform_clustering(data, method="DBSCAN", eps=0.5, min_samples=5, n_clusters=3):
    """
    Performs clustering on data using DBSCAN or K-Means.

    Parameters:
    - data: np.ndarray with the data for clustering.
    - method: "DBSCAN" or "K-Means".
    - eps: Parameter for DBSCAN (epsilon).
    - min_samples: Parameter for DBSCAN.
    - n_clusters: Number of clusters for K-Means.

    Returns:
    - labels: Labels generated by the clustering method.
    """
    if method == "DBSCAN":
        clustering_model = DBSCAN(eps=eps, min_samples=min_samples)
    elif method == "K-Means":
        clustering_model = KMeans(n_clusters=n_clusters, random_state=42)
    else:
        raise ValueError(f"Invalid clustering method: {method}")

    labels = clustering_model.fit_predict(data)
    return labels

def plot_roc_curve_global(y_true, y_pred_proba, title, save_as=None, classes=None):
    """
    Plots the ROC curve for binary or multiclass classifications.
    
    Parameters:
    - y_true (array-like): True labels.
    - y_pred_proba (array-like): Predicted probabilities.
    - title (str): Title of the plot.
    - save_as (str): Path to save the plot image.
    - classes (list): List of class labels.
    """
    lw = 2  # Line width

    unique_classes = np.unique(y_true)
    n_classes = len(unique_classes)

    if n_classes == 2:
        # Binary classification
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=lw, label=f'ROC Curve (area = {roc_auc:.2f})')
    else:
        # Multiclass classification
        y_bin = label_binarize(y_true, classes=unique_classes)
        n_classes = y_bin.shape[1]

        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        plt.figure()

        colors = plt.cm.viridis(np.linspace(0, 1, n_classes))
        for i, color in zip(range(n_classes), colors):
            class_label = classes[i] if classes is not None else unique_classes[i]
            plt.plot(fpr[i], tpr[i], color=color, lw=lw, label=f'ROC Curve for class {class_label} (area = {roc_auc[i]:0.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', color='white')
    plt.ylabel('True Positive Rate', color='white')
    plt.title(title, color='white')
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
    if save_as:
        plt.savefig(save_as, bbox_inches='tight', facecolor='#0B3C5D')  # Matching background color
    plt.close()

def get_class_rankings_global(model, X):
    """
    Obtains class rankings based on the probabilities predicted by the model.
    
    Parameters:
    - model: Trained and calibrated classifier.
    - X (array-like): Feature data.

    Returns:
    - class_rankings (list): List of class rankings for each sample.
    """
    if model is None:
        raise ValueError("Model not trained yet. Por favor, treine o modelo primeiro.")

    # Obtain probabilities for each class
    y_pred_proba = model.predict_proba(X)

    # Class rankings based on probabilities
    class_rankings = []
    for probabilities in y_pred_proba:
        ranked_classes = sorted(zip(model.classes_, probabilities), key=lambda x: x[1], reverse=True)
        formatted_rankings = [f"{cls}: {prob*100:.2f}%" for cls, prob in ranked_classes]
        class_rankings.append(formatted_rankings)

    return class_rankings

def calculate_roc_values(model, X_test, y_test):
    """
    Calculates ROC AUC values for each class.
    
    Parameters:
    - model: Trained and calibrated classifier.
    - X_test (array-like): Test feature data.
    - y_test (array-like): True labels for the test data.

    Returns:
    - roc_df (pd.DataFrame): DataFrame containing ROC AUC for each class.
    """
    unique_classes = np.unique(y_test)
    n_classes = len(unique_classes)
    y_pred_proba = model.predict_proba(X_test)

    if n_classes <= 1:
        logging.error("NÃ£o hÃ¡ classes suficientes para calcular ROC AUC.")
        return pd.DataFrame()

    # Binarize the output
    y_bin = label_binarize(y_test, classes=unique_classes)
    if y_bin.shape[1] == 1:
        # Only one class present
        logging.error("Formato multiclasses nÃ£o suportado. Apenas uma classe encontrada.")
        return pd.DataFrame()

    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

        # Logging ROC values
        logging.info(f"For class {unique_classes[i]}:")
        logging.info(f"False Positive Rate: {fpr[i]}")
        logging.info(f"True Positive Rate: {tpr[i]}")
        logging.info(f"ROC AUC: {roc_auc[i]}")
        logging.info("--------------------------")

    roc_df = pd.DataFrame(list(roc_auc.items()), columns=['Class', 'ROC AUC'])
    return roc_df

def format_and_sum_probabilities(associated_rankings):
    """
    Formats and sums probabilities for each category.
    
    Parameters:
    - associated_rankings (list): List of ranking strings.

    Returns:
    - formatted_results (str): Formatted string of summed probabilities.
    """
    category_sums = {}
    categories = ['C4-C6-C8', 'C6-C8-C10', 'C8-C10-C12', 'C10-C12-C14', 'C12-C14-C16', 'C14-C16-C18']
    pattern_mapping = {
        'C4-C6-C8': ['C4', 'C6', 'C8'],
        'C6-C8-C10': ['C6', 'C8', 'C10'],
        'C8-C10-C12': ['C8', 'C10', 'C12'],
        'C10-C12-C14': ['C10', 'C12', 'C14'],
        'C12-C14-C16': ['C12', 'C14', 'C16'],
        'C14-C16-C18': ['C14', 'C16', 'C18'],
    }

    # Initialize the sums dictionary
    for category in categories:
        category_sums[category] = 0.0

    # Sum probabilities for each category
    for rank in associated_rankings:
        try:
            prob = float(rank.split(": ")[1].replace("%", ""))
        except (IndexError, ValueError):
            logging.error(f"Error processing ranking string: {rank}")
            continue
        for category, patterns in pattern_mapping.items():
            if any(pattern in rank for pattern in patterns):
                category_sums[category] += prob

    # Sort the results and format for output
    sorted_results = sorted(category_sums.items(), key=lambda x: x[1], reverse=True)
    formatted_results = [f"{category} ({sum_prob:.2f}%)" for category, sum_prob in sorted_results if sum_prob > 0]

    return " - ".join(formatted_results)

class Support:
    """
    Support class for training and evaluating Random Forest models with oversampling techniques.
    """

    def __init__(self, cv=5, seed=SEED, n_jobs=8):
        self.cv = cv
        self.model = None
        self.seed = seed
        self.n_jobs = n_jobs
        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []
        self.roc_results = []
        self.train_sizes = np.linspace(.1, 1.0, 5)
        self.standard = StandardScaler()

        self.best_params = {}

        self.init_params = {
            "n_estimators": 100,
            "max_depth": 10,  # Reduced to prevent overfitting
            "min_samples_split": 5,  # Increased to prevent overfitting
            "min_samples_leaf": 2,
            "criterion": "entropy",
            "max_features": "sqrt",  # Changed from 'log2' as per comment
            "class_weight": None,  # No class balancing
            "max_leaf_nodes": 20,  # Adjusted for greater regularization
            "min_impurity_decrease": 0.02,
            "bootstrap": True,
            "ccp_alpha": 0.001,
            "random_state": self.seed  # Added for RandomForest
        }

        self.parameters = {
            "n_estimators": [50, 100, 200],
            "max_depth": [3, 5, 7],
            "min_samples_split": [5, 10],
            "min_samples_leaf": [1, 2, 4],
            "criterion": ["entropy", "gini"],
            "max_features": ["sqrt", "log2", None],
            "class_weight": ["balanced", None],
            "max_leaf_nodes": [5, 10, 20, 30, None],
            "min_impurity_decrease": [0.0],
            "bootstrap": [True, False],
            "ccp_alpha": [0.0, 0.001, 0.01],
        }

    def _oversample_with_smote_on_duplicates(self, X, y):
        """
        Applies RandomOverSampler to balance classes by duplicating samples,
        and then applies SMOTE only on the duplicated samples to generate synthetic samples.
        The original samples are kept intact.

        Parameters:
        - X: Features (numpy array or pandas DataFrame).
        - y: Labels (numpy array or pandas Series).

        Returns:
        - X_final: Features after applying oversampling and SMOTE.
        - y_final: Labels after applying oversampling and SMOTE.
        """
        # Step 1: Original Data
        original_X, original_y = X.copy(), y.copy()
        original_counts = Counter(original_y)
        logging.info(f"Original class distribution: {original_counts}")

        # Step 2: Apply RandomOverSampler to balance classes
        ros = RandomOverSampler(random_state=self.seed)
        X_resampled, y_resampled = ros.fit_resample(original_X, original_y)
        resampled_counts = Counter(y_resampled)
        logging.info(f"Class distribution after RandomOverSampler: {resampled_counts}")

        # Step 3: Determine the number of duplicates per class
        duplicates_X = []
        duplicates_y = []

        for cls in resampled_counts:
            n_resampled = resampled_counts[cls]
            n_original = original_counts.get(cls, 0)
            n_duplicates = n_resampled - n_original

            if n_duplicates > 0:
                # Indices of current class samples
                cls_indices = np.where(original_y == cls)[0]
                # Randomly select samples to duplicate
                duplicated_indices = np.random.choice(cls_indices, size=n_duplicates, replace=True)
                duplicates_X.append(original_X[duplicated_indices])
                duplicates_y.append(original_y[duplicated_indices])

        if duplicates_X:
            duplicates_X = np.vstack(duplicates_X)
            duplicates_y = np.hstack(duplicates_y)
            logging.info(f"Number of duplicated samples: {len(duplicates_X)}")
        else:
            duplicates_X = np.array([])
            duplicates_y = np.array([])
            logging.info("No samples were duplicated.")

        # Step 4: Apply SMOTE on duplicates
        if len(duplicates_X) == 0:
            logging.info("No duplicated samples to apply SMOTE.")
            X_final, y_final = original_X, original_y
        else:
            # Determine k_neighbors dynamically based on the number of samples
            n_samples_duplicates = duplicates_X.shape[0]
            # SMOTE requires n_neighbors <= n_samples - 1
            k_neighbors = min(5, n_samples_duplicates - 1) if n_samples_duplicates > 1 else 1
            smote = SMOTE(random_state=self.seed, k_neighbors=k_neighbors)
            try:
                X_synthetic, y_synthetic = smote.fit_resample(duplicates_X, duplicates_y)
                logging.info(f"Number of synthetic samples generated by SMOTE: {len(X_synthetic)}")
                # Step 5: Combine original samples with synthetic samples
                X_final = np.vstack([original_X, X_synthetic])
                y_final = np.hstack([original_y, y_synthetic])
            except ValueError as e:
                logging.error(f"Error applying SMOTE on duplicates: {e}")
                X_final, y_final = original_X, original_y

        # Step 6: Log final class distribution
        final_counts = Counter(y_final)
        logging.info(f"Final class distribution after RandomOverSampler and SMOTE: {final_counts}")

        # Save class distribution to a log file
        with open("oversampling_counts.txt", "a") as f:
            f.write("Class distribution after RandomOverSampler and SMOTE:\n")
            for cls, count in final_counts.items():
                f.write(f"{cls}: {count}\n")

        return X_final, y_final

    def _oversample_single_sample_classes(self, X, y):
        """
        Replaces the existing function to apply oversampling with SMOTE on duplicates.
        """
        return self._oversample_with_smote_on_duplicates(X, y)

    def fit(self, X, y, model_name_prefix='model', model_dir=None, min_kmers=None):
        """
        Trains the Random Forest model with cross-validation, oversampling, grid search, and calibration.
        
        Parameters:
        - X (array-like): Feature data.
        - y (array-like): Labels.
        - model_name_prefix (str): Prefix for saving models.
        - model_dir (str): Directory to save models.
        - min_kmers (int): Minimum number of k-mers.
        
        Returns:
        - model (CalibratedClassifierCV): Trained and calibrated model.
        """
        logging.info(f"Starting fit method for {model_name_prefix}...")

        X = np.array(X)
        y = np.array(y)

        X_smote, y_smote = self._oversample_single_sample_classes(X, y)

        sample_counts = Counter(y_smote)
        logging.info(f"Sample counts after oversampling for {model_name_prefix}: {sample_counts}")

        with open("sample_counts_after_oversampling.txt", "a") as f:
            f.write(f"Sample counts after Oversampling for {model_name_prefix}:\n")
            for cls, count in sample_counts.items():
                f.write(f"{cls}: {count}\n")

        if any(count < self.cv for count in sample_counts.values()):
            raise ValueError(f"There are classes with fewer members than the number of folds after oversampling for {model_name_prefix}.")

        min_class_count = min(sample_counts.values())
        self.cv = min(self.cv, min_class_count)

        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []

        fold_number = 1

        skf = StratifiedKFold(n_splits=self.cv, random_state=self.seed, shuffle=True)

        for train_index, test_index in skf.split(X_smote, y_smote):
            X_train, X_test = X_smote[train_index], X_smote[test_index]
            y_train, y_test = y_smote[train_index], y_smote[test_index]

            unique, counts_fold = np.unique(y_test, return_counts=True)
            fold_class_distribution = dict(zip(unique, counts_fold))
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Class distribution in test set: {fold_class_distribution}")

            X_train_resampled, y_train_resampled = self._oversample_single_sample_classes(X_train, y_train)

            train_sample_counts = Counter(y_train_resampled)
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Class distribution in training set after oversampling: {train_sample_counts}")

            with open("training_sample_counts_after_oversampling.txt", "a") as f:
                f.write(f"Fold {fold_number} Training sample counts after Oversampling for {model_name_prefix}:\n")
                for cls, count in train_sample_counts.items():
                    f.write(f"{cls}: {count}\n")

            self.model = RandomForestClassifier(**self.init_params, n_jobs=self.n_jobs)
            self.model.fit(X_train_resampled, y_train_resampled)

            train_score = self.model.score(X_train_resampled, y_train_resampled)
            test_score = self.model.score(X_test, y_test)

            self.train_scores.append(train_score)
            self.test_scores.append(test_score)

            # Calculate F1-score and Precision-Recall AUC
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)

            f1 = f1_score(y_test, y_pred, average='weighted')
            self.f1_scores.append(f1)

            if len(np.unique(y_test)) > 1:
                pr_auc = average_precision_score(label_binarize(y_test, classes=self.model.classes_), y_pred_proba, average='macro')
            else:
                pr_auc = 0.0  # Cannot calculate PR AUC for a single class
            self.pr_auc_scores.append(pr_auc)

            logging.info(f"Fold {fold_number} [{model_name_prefix}]: F1 Score: {f1}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Precision-Recall AUC: {pr_auc}")

            # Calculate ROC AUC
            try:
                roc_df = calculate_roc_values(self.model, X_test, y_test)
                self.roc_results.append(roc_df)
            except ValueError as e:
                logging.warning(f"Could not calculate ROC AUC for fold {fold_number} [{model_name_prefix}] due to insufficient class representation: {e}")

            # Perform Grid Search and save the best model
            best_model, best_params = self._perform_grid_search(X_train_resampled, y_train_resampled)
            self.model = best_model
            self.best_params = best_params

            if model_dir:
                best_model_filename = os.path.join(model_dir, f'best_model_{model_name_prefix}.pkl')
                # Ensure the directory exists
                os.makedirs(os.path.dirname(best_model_filename), exist_ok=True)
                joblib.dump(best_model, best_model_filename)
                logging.info(f"Best model saved as {best_model_filename} for {model_name_prefix}")
            else:
                best_model_filename = f'best_model_{model_name_prefix}.pkl'
                joblib.dump(best_model, best_model_filename)
                logging.info(f"Best model saved as {best_model_filename} for {model_name_prefix}")

            if best_params is not None:
                self.best_params = best_params
                logging.info(f"Best parameters for {model_name_prefix}: {self.best_params}")
            else:
                logging.warning(f"No best parameters found from grid search for {model_name_prefix}.")

            # Integrate Probability Calibration
            calibrator = CalibratedClassifierCV(self.model, method='isotonic', cv=5, n_jobs=self.n_jobs)
            calibrator.fit(X_train_resampled, y_train_resampled)

            self.model = calibrator

            if model_dir:
                calibrated_model_filename = os.path.join(model_dir, f'calibrated_model_{model_name_prefix}.pkl')
            else:
                calibrated_model_filename = f'calibrated_model_{model_name_prefix}.pkl'
            joblib.dump(calibrator, calibrated_model_filename)
            logging.info(f"Calibrated model saved as {calibrated_model_filename} for {model_name_prefix}")

            fold_number += 1

            # Allow Streamlit to update the interface
            time.sleep(0.1)

        return self.model

    def _perform_grid_search(self, X_train_resampled, y_train_resampled):
        """
        Performs grid search to find the best hyperparameters for the Random Forest model.
        
        Parameters:
        - X_train_resampled (array-like): Resampled training features.
        - y_train_resampled (array-like): Resampled training labels.

        Returns:
        - best_model (estimator): Best estimator found by grid search.
        - best_params (dict): Best parameters found by grid search.
        """
        skf = StratifiedKFold(n_splits=self.cv, random_state=self.seed, shuffle=True)
        grid_search = GridSearchCV(
            RandomForestClassifier(random_state=self.seed),
            self.parameters,
            cv=skf,
            n_jobs=self.n_jobs,
            scoring='roc_auc_ovo',
            verbose=1
        )

        grid_search.fit(X_train_resampled, y_train_resampled)
        logging.info(f"Best parameters from grid search: {grid_search.best_params_}")
        return grid_search.best_estimator_, grid_search.best_params_

    def get_best_param(self, param_name, default=None):
        """
        Retrieves the best parameter value from grid search.
        
        Parameters:
        - param_name (str): Name of the parameter.
        - default: Default value if parameter not found.

        Returns:
        - value: Best parameter value or default.
        """
        return self.best_params.get(param_name, default)

    def plot_learning_curve(self, output_path):
        """
        Plots the learning curve based on cross-validation scores.
        
        Parameters:
        - output_path (str): Path to save the learning curve plot.
        """
        plt.figure()
        plt.plot(self.train_scores, label='Training Score')
        plt.plot(self.test_scores, label='Cross-Validation Score')
        plt.plot(self.f1_scores, label='F1 Score')
        plt.plot(self.pr_auc_scores, label='Precision-Recall AUC')
        plt.title("Learning Curve", color='white')
        plt.xlabel("Fold", fontsize=12, fontweight='bold', color='white')
        plt.ylabel("Score", fontsize=12, fontweight='bold', color='white')
        plt.legend(loc="best")
        plt.grid(color='white', linestyle='--', linewidth=0.5)
        plt.savefig(output_path, facecolor='#0B3C5D')  # Matching background color
        plt.close()

    def get_class_rankings(self, X):
        """
        Obtains class rankings for the provided data.
        
        Parameters:
        - X (array-like): Feature data.

        Returns:
        - class_rankings (list): List of class rankings for each sample.
        """
        if self.model is None:
            raise ValueError("Model not trained yet. Por favor, treine o modelo primeiro.")

        # Obtain probabilities for each class
        y_pred_proba = self.model.predict_proba(X)

        # Class rankings based on probabilities
        class_rankings = []
        for probabilities in y_pred_proba:
            ranked_classes = sorted(zip(self.model.classes_, probabilities), key=lambda x: x[1], reverse=True)
            formatted_rankings = [f"{cls}: {prob*100:.2f}%" for cls, prob in ranked_classes]
            class_rankings.append(formatted_rankings)

        return class_rankings

    def test_best_RF(self, X, y, scaler_dir='.'):
        """
        Tests the best Random Forest model with the provided data.
        
        Parameters:
        - X (array-like): Feature data.
        - y (array-like): Labels.
        - scaler_dir (str): Directory where the scaler is saved.

        Returns:
        - score (float): ROC AUC score.
        - f1 (float): F1 score.
        - pr_auc (float): Precision-Recall AUC score.
        - best_params (dict): Best hyperparameters.
        - calibrated_model (CalibratedClassifierCV): Trained and calibrated model.
        - X_test (array-like): Test features.
        - y_test (array-like): Test labels.
        """
        # Load the scaler
        scaler_path = os.path.join(scaler_dir, 'scaler.pkl') if scaler_dir else 'scaler.pkl'
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            logging.info(f"Scaler loaded from {scaler_path}")
        else:
            logging.error(f"Scaler not found in {scaler_path}")
            st.error(f"Scaler nÃ£o encontrado em {scaler_path}. Verifique se o treinamento foi concluÃ­do com sucesso.")
            sys.exit(1)

        X_scaled = scaler.transform(X)

        # Apply oversampling on the entire dataset before splitting
        X_resampled, y_resampled = self._oversample_single_sample_classes(X_scaled, y)

        # Split into training and testing
        X_train, X_test, y_train, y_test = train_test_split(
            X_resampled, y_resampled, test_size=0.4, random_state=self.seed, stratify=y_resampled
        )

        # Train the RandomForestClassifier with the best parameters
        model = RandomForestClassifier(
            n_estimators=self.best_params.get('n_estimators', 100),
            max_depth=self.best_params.get('max_depth', 5),
            min_samples_split=self.best_params.get('min_samples_split', 4),
            min_samples_leaf=self.best_params.get('min_samples_leaf', 2),
            criterion=self.best_params.get('criterion', 'entropy'),
            max_features=self.best_params.get('max_features', 'log2'),
            class_weight=self.best_params.get('class_weight', 'balanced'),
            max_leaf_nodes=self.best_params.get('max_leaf_nodes', 20),
            min_impurity_decrease=self.best_params.get('min_impurity_decrease', 0.01),
            bootstrap=self.best_params.get('bootstrap', True),
            ccp_alpha=self.best_params.get('ccp_alpha', 0.001),
            random_state=self.seed,
            n_jobs=self.n_jobs
        )
        model.fit(X_train, y_train)  # Train the model on training data

        # Integrate Probability Calibration
        calibrator = CalibratedClassifierCV(model, method='isotonic', cv=5, n_jobs=self.n_jobs)
        calibrator.fit(X_train, y_train)
        calibrated_model = calibrator

        # Make predictions
        y_pred_proba = calibrated_model.predict_proba(X_test)
        y_pred_adjusted = adjust_predictions_global(y_pred_proba, method='normalize')

        # Calculate the score (e.g., AUC)
        score = self._calculate_score(y_pred_adjusted, y_test)

        # Calculate additional metrics
        y_pred_classes = calibrated_model.predict(X_test)
        f1 = f1_score(y_test, y_pred_classes, average='weighted')
        if len(np.unique(y_test)) > 1:
            pr_auc = average_precision_score(label_binarize(y_test, classes=calibrator.classes_), y_pred_proba, average='macro')
        else:
            pr_auc = 0.0  # Cannot calculate PR AUC for a single class

        # Return the score, best parameters, trained model, and test sets
        return score, f1, pr_auc, self.best_params, calibrated_model, X_test, y_test

    def _calculate_score(self, y_pred, y_test):
        """
        Calculates the ROC AUC score based on predictions and true labels.
        
        Parameters:
        - y_pred (array-like): Predicted probabilities.
        - y_test (array-like): True labels.

        Returns:
        - score (float): ROC AUC score.
        """
        n_classes = len(np.unique(y_test))
        if y_pred.ndim == 1 or n_classes == 2:
            return roc_auc_score(y_test, y_pred)
        elif y_pred.ndim == 2 and n_classes > 2:
            y_test_bin = label_binarize(y_test, classes=np.unique(y_test))
            return roc_auc_score(y_test_bin, y_pred, multi_class='ovo', average='macro')
        else:
            logging.warning(f"Unexpected format or number of classes: y_pred shape: {y_pred.shape}, number of classes: {n_classes}")
            return 0

    def plot_roc_curve(self, y_true, y_pred_proba, title, save_as=None, classes=None):
        """
        Plots the ROC curve using the global function.
        """
        plot_roc_curve_global(y_true, y_pred_proba, title, save_as, classes)

class ProteinEmbeddingGenerator:
    """
    Class for generating protein embeddings using Word2Vec.
    """
    def __init__(self, sequences_path, table_data=None, aggregation_method='none'):
        aligned_path = sequences_path
        if not are_sequences_aligned(sequences_path):
            realign_sequences_with_mafft(sequences_path, sequences_path.replace(".fasta", "_aligned.fasta"), threads=1)
            aligned_path = sequences_path.replace(".fasta", "_aligned.fasta")
        else:
            logging.info(f"Sequences are already aligned: {sequences_path}")

        self.alignment = AlignIO.read(aligned_path, 'fasta')
        self.table_data = table_data
        self.embeddings = []
        self.models = {}
        self.aggregation_method = aggregation_method  # Choice of aggregation method
        self.min_kmers = None  # Stores min_kmers

    def generate_embeddings(self, k=3, step_size=1, word2vec_model_path="word2vec_model.bin", model_dir=None, min_kmers=None, save_min_kmers=False):
        """
        Generates embeddings for protein sequences using Word2Vec, standardizing the number of k-mers.
        
        Parameters:
        - k (int): Size of the k-mer.
        - step_size (int): Step size for k-mer generation.
        - word2vec_model_path (str): Path to save/load the Word2Vec model.
        - model_dir (str): Directory to save the Word2Vec model.
        - min_kmers (int): Minimum number of k-mers.
        - save_min_kmers (bool): Whether to save min_kmers to a file.
        """
        # Define the full path for the Word2Vec model
        if model_dir:
            word2vec_model_full_path = os.path.join(model_dir, word2vec_model_path)
        else:
            word2vec_model_full_path = word2vec_model_path

        # Check if the Word2Vec model already exists
        if os.path.exists(word2vec_model_full_path):
            logging.info(f"Word2Vec model found at {word2vec_model_full_path}. Loading the model.")
            model = Word2Vec.load(word2vec_model_full_path)
            self.models['global'] = model
        else:
            logging.info("Word2Vec model not found. Training a new model.")
            # Initialize Variables
            kmer_groups = {}
            all_kmers = []
            kmers_counts = []

            # Generate k-mers
            for record in self.alignment:
                sequence = str(record.seq)
                seq_len = len(sequence)
                protein_accession_alignment = record.id.split()[0]

                # If table data is not provided, skip matching
                if self.table_data is not None:
                    matching_rows = self.table_data['Protein.accession'].str.split().str[0] == protein_accession_alignment
                    matching_info = self.table_data[matching_rows]

                    if matching_info.empty:
                        logging.warning(f"No matching data in the training table for {protein_accession_alignment}")
                        continue  # Skip to the next iteration

                    target_variable = matching_info['Target variable'].values[0]
                    associated_variable = matching_info['Associated variable'].values[0]

                else:
                    # If no table, use default values or None
                    target_variable = None
                    associated_variable = None

                logging.info(f"Processing {protein_accession_alignment} with sequence length {seq_len}")

                if seq_len < k:
                    logging.warning(f"Sequence too short for {protein_accession_alignment}. Length: {seq_len}")
                    continue

                # Generate k-mers, allowing k-mers with fewer than k gaps
                kmers = [sequence[i:i + k] for i in range(0, seq_len - k + 1, step_size)]
                kmers = [kmer for kmer in kmers if kmer.count('-') < k]  # Allow k-mers with fewer than k gaps

                if not kmers:
                    logging.warning(f"No valid k-mer for {protein_accession_alignment}")
                    continue

                all_kmers.append(kmers)  # Add the list of k-mers as a sentence
                kmers_counts.append(len(kmers))  # Store k-mer count

                embedding_info = {
                    'protein_accession': protein_accession_alignment,
                    'target_variable': target_variable,
                    'associated_variable': associated_variable,
                    'kmers': kmers  # Store k-mers for later use
                }
                kmer_groups[protein_accession_alignment] = embedding_info

            # Determine the minimum number of k-mers
            if not kmers_counts:
                logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
                st.error("Nenhum k-mer foi coletado. Verifique suas sequÃªncias e parÃ¢metros de k-mer.")
                sys.exit(1)

            if min_kmers is not None:
                self.min_kmers = min_kmers
                logging.info(f"Using provided min_kmers: {self.min_kmers}")
            else:
                self.min_kmers = min(kmers_counts)
                logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

            # Save min_kmers if necessary
            if save_min_kmers and model_dir:
                min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
                with open(min_kmers_path, 'w') as f:
                    f.write(str(self.min_kmers))
                logging.info(f"min_kmers saved at {min_kmers_path}")

            # Train the Word2Vec model using all k-mers
            model = Word2Vec(
                sentences=all_kmers,
                vector_size=125,  # Updated parameter name from 'size' to 'vector_size'
                window=10,
                min_count=1,
                workers=8,
                sg=1,
                hs=1,  # Hierarchical softmax enabled
                negative=0,  # Negative sampling disabled
                epochs=2500,  # Updated parameter name from 'iter' to 'epochs'
                seed=SEED  # Fix seed for reproducibility
            )

            # Create directory for the Word2Vec model if necessary
            if model_dir:
                os.makedirs(os.path.dirname(word2vec_model_full_path), exist_ok=True)

            # Save the Word2Vec model
            model.save(word2vec_model_full_path)
            self.models['global'] = model
            logging.info(f"Word2Vec model saved at {word2vec_model_full_path}")

        # Generate standardized embeddings
        kmer_groups = {}
        kmers_counts = []
        all_kmers = []

        for record in self.alignment:
            sequence = str(record.seq)
            protein_accession_alignment = record.id.split()[0]

            # If table data is not provided, skip matching
            if self.table_data is not None:
                matching_rows = self.table_data['Protein.accession'].str.split().str[0] == protein_accession_alignment
                matching_info = self.table_data[matching_rows]

                if matching_info.empty:
                    logging.warning(f"No matching data in the training table for {protein_accession_alignment}")
                    continue  # Skip to the next iteration

                target_variable = matching_info['Target variable'].values[0]
                associated_variable = matching_info['Associated variable'].values[0]

            else:
                # If no table, use default values or None
                target_variable = None
                associated_variable = None

            kmers = [sequence[i:i + k] for i in range(0, len(sequence) - k + 1, step_size)]
            kmers = [kmer for kmer in kmers if kmer.count('-') < k]  # Allow k-mers with fewer than k gaps

            if not kmers:
                logging.warning(f"No valid k-mer for {protein_accession_alignment}")
                continue

            all_kmers.append(kmers)
            kmers_counts.append(len(kmers))

            embedding_info = {
                'protein_accession': protein_accession_alignment,
                'target_variable': target_variable,
                'associated_variable': associated_variable,
                'kmers': kmers
            }
            kmer_groups[protein_accession_alignment] = embedding_info

        # Determine the minimum number of k-mers
        if not kmers_counts:
            logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
            st.error("Nenhum k-mer foi coletado. Verifique suas sequÃªncias e parÃ¢metros de k-mer.")
            sys.exit(1)

        if min_kmers is not None:
            self.min_kmers = min_kmers
            logging.info(f"Using provided min_kmers: {self.min_kmers}")
        else:
            self.min_kmers = min(kmers_counts)
            logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

        # Generate standardized embeddings
        for record in self.alignment:
            sequence_id = record.id.split()[0]  # Use consistent sequence IDs
            embedding_info = kmer_groups.get(sequence_id, {})
            kmers_for_protein = embedding_info.get('kmers', [])

            if len(kmers_for_protein) == 0:
                if self.aggregation_method == 'none':
                    embedding_concatenated = np.zeros(self.models['global'].vector_size * self.min_kmers)
                else:
                    embedding_concatenated = np.zeros(self.models['global'].vector_size)
                self.embeddings.append({
                    'protein_accession': sequence_id,
                    'embedding': embedding_concatenated,
                    'target_variable': embedding_info.get('target_variable'),
                    'associated_variable': embedding_info.get('associated_variable')
                })
                continue

            # Select the first min_kmers k-mers
            selected_kmers = kmers_for_protein[:self.min_kmers]

            # Pad with zeros if necessary
            if len(selected_kmers) < self.min_kmers:
                padding = [np.zeros(self.models['global'].vector_size)] * (self.min_kmers - len(selected_kmers))
                selected_kmers.extend(padding)

            # Obtain embeddings for the selected k-mers
            selected_embeddings = [self.models['global'].wv[kmer] if kmer in self.models['global'].wv else np.zeros(self.models['global'].vector_size) for kmer in selected_kmers]

            if self.aggregation_method == 'none':
                # Concatenate embeddings of the selected k-mers
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)
            elif self.aggregation_method == 'mean':
                # Aggregate embeddings of the selected k-mers by mean
                embedding_concatenated = np.mean(selected_embeddings, axis=0)
            else:
                # If unrecognized method, use concatenation as default
                logging.warning(f"Unknown aggregation method '{self.aggregation_method}'. Using concatenation.")
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)

            self.embeddings.append({
                'protein_accession': sequence_id,
                'embedding': embedding_concatenated,
                'target_variable': embedding_info.get('target_variable'),
                'associated_variable': embedding_info.get('associated_variable')
            })

            logging.debug(f"Protein ID: {sequence_id}, Embedding Shape: {embedding_concatenated.shape}")

        # Fit the StandardScaler with embeddings for training/prediction
        embeddings_array_train = np.array([entry['embedding'] for entry in self.embeddings])

        # Check if all embeddings have the same format
        embedding_shapes = set(embedding.shape for embedding in [entry['embedding'] for entry in self.embeddings])
        if len(embedding_shapes) != 1:
            logging.error(f"Inconsistent embedding formats detected: {embedding_shapes}")
            raise ValueError("Embeddings have inconsistent formats.")
        else:
            logging.info(f"All embeddings have format: {embedding_shapes.pop()}")

        # Define the full path for the scaler
        scaler_full_path = os.path.join(model_dir, 'scaler.pkl') if model_dir else 'scaler.pkl'

        # Check if the scaler already exists
        if os.path.exists(scaler_full_path):
            logging.info(f"StandardScaler found at {scaler_full_path}. Loading the scaler.")
            scaler = joblib.load(scaler_full_path)
        else:
            logging.info("StandardScaler not found. Training a new scaler.")
            scaler = StandardScaler().fit(embeddings_array_train)
            joblib.dump(scaler, scaler_full_path)
            logging.info(f"StandardScaler saved at {scaler_full_path}")

    def get_embeddings_and_labels(self, label_type='target_variable'):
        """
        Returns embeddings and associated labels (target_variable or associated_variable).
        
        Parameters:
        - label_type (str): Type of label to retrieve ('target_variable' or 'associated_variable').

        Returns:
        - embeddings (np.ndarray): Array of embeddings.
        - labels (np.ndarray): Array of labels.
        """
        embeddings = []
        labels = []

        for embedding_info in self.embeddings:
            embeddings.append(embedding_info['embedding'])
            labels.append(embedding_info[label_type])  # Use the specified label type

        return np.array(embeddings), np.array(labels)

# Dynamically adjust perplexity
def compute_perplexity(n_samples):
    """
    Computes a dynamic perplexity value based on the number of samples.
    
    Parameters:
    - n_samples (int): Number of samples.

    Returns:
    - perplexity (int): Calculated perplexity value.
    """
    return max(5, min(50, n_samples // 100))

def plot_dual_tsne_3d(train_embeddings, train_labels, train_protein_ids, 
                      predict_embeddings, predict_labels, predict_protein_ids, output_dir):
    """
    Plots two separate 3D t-SNE plots:
    - Plot 1: Training Data.
    - Plot 2: Predictions.
    
    Parameters:
    - train_embeddings (np.ndarray): Embeddings of training data.
    - train_labels (list or array): Labels associated with training data.
    - train_protein_ids (list): Protein IDs in training data.
    - predict_embeddings (np.ndarray): Embeddings of predictions.
    - predict_labels (list or array): Labels associated with predictions.
    - predict_protein_ids (list): Protein IDs in predictions.
    - output_dir (str): Directory to save the plot images.
    
    Returns:
    - fig_train, fig_predict: Plotly figure objects for training and prediction data.
    """
    # Adjust perplexity dynamically
    n_samples_train = train_embeddings.shape[0]
    dynamic_perplexity_train = compute_perplexity(n_samples_train)

    # Initialize t-SNE with adjusted perplexity for training
    tsne_train = TSNE(n_components=3, random_state=42, perplexity=dynamic_perplexity_train, n_iter=1000)
    tsne_train_result = tsne_train.fit_transform(train_embeddings)

    # Adjust perplexity dynamically for predictions
    n_samples_predict = predict_embeddings.shape[0]
    dynamic_perplexity_predict = compute_perplexity(n_samples_predict)

    # Initialize t-SNE with adjusted perplexity for predictions
    tsne_predict = TSNE(n_components=3, random_state=42, perplexity=dynamic_perplexity_predict, n_iter=1000)
    tsne_predict_result = tsne_predict.fit_transform(predict_embeddings)

    # Create color map for training data
    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    # Create color map for predictions
    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    # Convert labels to colors
    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    # Plot 1: Training Data
    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=tsne_train_result[:, 0],
        y=tsne_train_result[:, 1],
        z=tsne_train_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=train_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text' field
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='t-SNE 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Plot 2: Predictions
    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=tsne_predict_result[:, 0],
        y=tsne_predict_result[:, 1],
        z=tsne_predict_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=predict_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text' field
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='t-SNE 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )
    # Save the plots as HTML
    tsne_train_html = os.path.join(output_dir, "tsne_train_3d.html")
    tsne_predict_html = os.path.join(output_dir, "tsne_predict_3d.html")
    
    pio.write_html(fig_train, file=tsne_train_html, auto_open=False)
    pio.write_html(fig_predict, file=tsne_predict_html, auto_open=False)
    
    logging.info(f"t-SNE Training Data saved as {tsne_train_html}")
    logging.info(f"t-SNE Predictions saved as {tsne_predict_html}")

    return fig_train, fig_predict

def plot_dual_umap(train_embeddings, train_labels, train_protein_ids,
                   predict_embeddings, predict_labels, predict_protein_ids, output_dir):
    """
    Plots two separate 3D UMAP plots:
    - Plot 1: Training Data.
    - Plot 2: Predictions.
    
    Parameters:
    - train_embeddings (np.ndarray): Embeddings of training data.
    - train_labels (list or array): Labels associated with training data.
    - train_protein_ids (list): Protein IDs in training data.
    - predict_embeddings (np.ndarray): Embeddings of predictions.
    - predict_labels (list or array): Labels associated with predictions.
    - predict_protein_ids (list): Protein IDs in predictions.
    - output_dir (str): Directory to save the plot images.
    
    Returns:
    - fig_train, fig_predict: Plotly figure objects for training and prediction data.
    """
    # Dimensionality reduction for training
    umap_train = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)
    umap_train_result = umap_train.fit_transform(train_embeddings)

    # Dimensionality reduction for predictions
    umap_predict = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)
    umap_predict_result = umap_predict.fit_transform(predict_embeddings)

    # Create color map for training data
    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    # Create color map for predictions
    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    # Convert labels to colors
    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    # Plot 1: Training Data
    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=umap_train_result[:, 0],
        y=umap_train_result[:, 1],
        z=umap_train_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=train_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text' field
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='UMAP 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Plot 2: Predictions
    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=umap_predict_result[:, 0],
        y=umap_predict_result[:, 1],
        z=umap_predict_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=predict_colors,
            opacity=0.8
        ),
        # Real protein IDs added to 'text' field
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='UMAP 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Save the plots as HTML
    umap_train_html = os.path.join(output_dir, "umap_train_3d.html")
    umap_predict_html = os.path.join(output_dir, "umap_predict_3d.html")
    
    pio.write_html(fig_train, file=umap_train_html, auto_open=False)
    pio.write_html(fig_predict, file=umap_predict_html, auto_open=False)
    
    logging.info(f"UMAP Training Data saved as {umap_train_html}")
    logging.info(f"UMAP Predictions saved as {umap_predict_html}")

    return fig_train, fig_predict

def plot_predictions_scatterplot_custom(results, output_path, top_n=3):
    """
    Generates a scatter plot of the top N predictions for new sequences.

    Y-axis: Protein accession ID
    X-axis: Specificities from C2 to C18 (fixed scale)
    Each point represents the corresponding specificity probability for the protein.
    Only the top N predictions are plotted.
    Points are colored in a single uniform color, styled for scientific publication.
    
    Parameters:
    - results (dict): Dictionary containing prediction results.
    - output_path (str): Path to save the scatter plot image.
    - top_n (int): Number of top predictions to display.
    """
    # Prepare the data
    protein_specificities = {}
    
    for seq_id, info in results.items():
        associated_rankings = info.get('associated_ranking', [])
        if not associated_rankings:
            logging.warning(f"No associated ranking data for protein {seq_id}. Skipping...")
            continue

        specificity_probs = {}
        for ranking in associated_rankings[:top_n]:
            try:
                # Split and extract the data
                category, prob = ranking.split(": ")
                prob = float(prob.replace("%", ""))

                # Extract the first number from the category
                if category.startswith('C'):
                    # Extract only the first number before any colon or other separator
                    spec = int(category.split(':')[0].strip('C'))
                    specificity_probs[spec] = prob
            except ValueError as e:
                logging.error(f"Error processing ranking: {ranking} for protein {seq_id}. Error: {e}")

        if specificity_probs:
            protein_specificities[seq_id] = specificity_probs

    if not protein_specificities:
        logging.warning("No data available to plot the scatterplot.")
        return

    # Sort protein IDs for better visualization
    unique_proteins = sorted(protein_specificities.keys())
    protein_order = {protein: idx for idx, protein in enumerate(unique_proteins)}

    # Create the figure
    fig, ax = plt.subplots(figsize=(12, max(6, len(unique_proteins) * 0.5)))  # Adjust height based on number of proteins

    # Fixed scale for the x-axis from C2 to C18
    x_values = list(range(2, 19))

    for protein, specs in protein_specificities.items():
        y = protein_order[protein]
        
        # Prepare data for plotting (ensure only the specified top N predictions are plotted)
        x = []
        probs = []
        for spec in x_values:
            if spec in specs:
                x.append(spec)
                probs.append(specs[spec])

        if not x:
            logging.warning(f"No valid data to plot for protein {protein}. Skipping...")
            continue

        # Plot points in a fixed color (e.g., dark blue)
        ax.scatter(x, [y] * len(x), color='#1f78b4', edgecolors='black', linewidth=0.5, s=100, label='_nolegend_')

        # Connect the points with lines
        if len(x) > 1:
            ax.plot(x, [y] * len(x), color='#1f78b4', linestyle='-', linewidth=1.0, alpha=0.7)

    # Customize the plot for better publication quality
    ax.set_xlabel('Specificity (C2 to C18)', fontsize=14, fontweight='bold')
    ax.set_ylabel('Proteins', fontsize=14, fontweight='bold')
    ax.set_title('Scatterplot of New Sequence Predictions (Top Rankings)', fontsize=16, fontweight='bold', pad=20)

    # Define fixed scale for x-axis and formatting
    ax.set_xticks(x_values)
    ax.set_xticklabels([f'C{spec}' for spec in x_values], fontsize=12)
    ax.set_yticks(range(len(unique_proteins)))
    ax.set_yticklabels(unique_proteins, fontsize=10)

    # Define grid and remove unnecessary spines for a clean look
    ax.grid(True, axis='x', linestyle='--', alpha=0.5, color='gray')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    # Minor ticks on the x-axis for better visibility
    ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
    ax.grid(which='minor', axis='x', linestyle=':', linewidth=0.5, alpha=0.6)

    # Adjust layout to prevent label cutoff
    plt.tight_layout()

    # Save the figure in high quality for publication
    plt.savefig(output_path, facecolor='white', dpi=600, bbox_inches='tight')
    plt.close()
    logging.info(f"Scatterplot saved at {output_path}")

def adjust_predictions_global(predicted_proba, method='normalize', alpha=1.0):
    """
    Adjusts the probabilities predicted by the model.
    
    Parameters:
    - predicted_proba (array-like): Predicted probabilities from the model.
    - method (str): Adjustment method ('normalize', 'smoothing', 'none').
    - alpha (float): Smoothing parameter.
    
    Returns:
    - adjusted_proba (array-like): Adjusted probabilities.
    """
    if method == 'normalize':
        # Normalize probabilities to sum to 1 for each sample
        logging.info("Normalizing predicted probabilities.")
        adjusted_proba = predicted_proba / predicted_proba.sum(axis=1, keepdims=True)

    elif method == 'smoothing':
        # Apply smoothing to probabilities to avoid extreme values
        logging.info(f"Applying smoothing to predicted probabilities with alpha={alpha}.")
        adjusted_proba = (predicted_proba + alpha) / (predicted_proba.sum(axis=1, keepdims=True) + alpha * predicted_proba.shape[1])

    elif method == 'none':
        # No adjustment
        logging.info("No adjustment applied to predicted probabilities.")
        adjusted_proba = predicted_proba.copy()

    else:
        logging.warning(f"Unknown adjustment method '{method}'. No adjustment will be applied.")
        adjusted_proba = predicted_proba.copy()

    return adjusted_proba

def main(args):
    """
    Main function that coordinates the workflow.
    
    Parameters:
    - args (argparse.Namespace): Parsed command-line arguments.
    """
    model_dir = args.model_dir  # Should be 'results/models'

    # Initialize progress variables
    total_steps = 8  # Updated after removing dimensionality reduction and visualization steps
    current_step = 0
    progress_bar = st.progress(0)
    progress_text = st.empty()

    # =============================
    # STEP 1: Model Training
    # =============================

    # Load training data
    train_alignment_path = args.train_fasta
    train_table_data_path = args.train_table

    # Check if training sequences are aligned
    if not are_sequences_aligned(train_alignment_path):
        logging.info("Training sequences are not aligned. Realigning with MAFFT...")
        aligned_train_path = train_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(train_alignment_path, aligned_train_path, threads=1)  # Fix threads=1
        train_alignment_path = aligned_train_path
    else:
        logging.info(f"Aligned training file found or sequences are already aligned: {train_alignment_path}")

    # Load training table data
    train_table_data = pd.read_csv(train_table_data_path, delimiter="\t")
    logging.info("Training data table loaded successfully.")

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Initialize and generate embeddings for training
    protein_embedding_train = ProteinEmbeddingGenerator(
        train_alignment_path, 
        train_table_data, 
        aggregation_method=args.aggregation_method  # Pass the aggregation method
    )
    protein_embedding_train.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        save_min_kmers=True  # Save min_kmers after training
    )
    logging.info(f"Number of training embeddings generated: {len(protein_embedding_train.embeddings)}")

    # Save min_kmers to ensure consistency
    min_kmers = protein_embedding_train.min_kmers

    # Get embeddings and labels for target_variable
    X_target, y_target = protein_embedding_train.get_embeddings_and_labels(label_type='target_variable')
    logging.info(f"Shape of X_target: {X_target.shape}")

    # Full paths for target_variable models
    rf_model_target_full_path = os.path.join(model_dir, args.rf_model_target)
    calibrated_model_target_full_path = os.path.join(model_dir, 'calibrated_model_target.pkl')

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Check if calibrated model for target_variable already exists
    if os.path.exists(calibrated_model_target_full_path):
        calibrated_model_target = joblib.load(calibrated_model_target_full_path)
        logging.info(f"Calibrated Random Forest model for target_variable loaded from {calibrated_model_target_full_path}")
    else:
        # Train the model for target_variable
        support_model_target = Support()
        calibrated_model_target = support_model_target.fit(X_target, y_target, model_name_prefix='target', model_dir=model_dir, min_kmers=min_kmers)
        logging.info("Training and calibration for target_variable completed.")

        # Save the calibrated model
        joblib.dump(calibrated_model_target, calibrated_model_target_full_path)
        logging.info(f"Calibrated Random Forest model for target_variable saved at {calibrated_model_target_full_path}")

        # Test the model
        best_score, best_f1, best_pr_auc, best_params, best_model_target, X_test_target, y_test_target = support_model_target.test_best_RF(X_target, y_target, scaler_dir=args.model_dir)

        logging.info(f"Best ROC AUC for target_variable: {best_score}")
        logging.info(f"Best F1 Score for target_variable: {best_f1}")
        logging.info(f"Best Precision-Recall AUC for target_variable: {best_pr_auc}")
        logging.info(f"Best Parameters: {best_params}")

        for param, value in best_params.items():
            logging.info(f"{param}: {value}")

        # Get class rankings
        class_rankings = support_model_target.get_class_rankings(X_test_target)

        # Display rankings for the first 5 samples
        logging.info("Top 3 class rankings for the first 5 samples:")
        for i in range(min(5, len(class_rankings))):
            logging.info(f"Sample {i+1}: Class Rankings - {class_rankings[i][:3]}")  # Shows top 3 rankings

        # Plot ROC curve
        n_classes_target = len(np.unique(y_test_target))
        if n_classes_target == 2:
            y_pred_proba_target = best_model_target.predict_proba(X_test_target)[:, 1]
            unique_classes_target = np.unique(y_test_target)
        else:
            y_pred_proba_target = best_model_target.predict_proba(X_test_target)
            unique_classes_target = np.unique(y_test_target).astype(str)
        plot_roc_curve_global(y_test_target, y_pred_proba_target, 'ROC Curve for Target Variable', save_as=args.roc_curve_target, classes=unique_classes_target)

        # Convert y_test_target to integer labels if necessary
        if isinstance(y_test_target[0], str):
            unique_labels_target = sorted(set(y_test_target))
            label_to_int_target = {label: idx for idx, label in enumerate(unique_labels_target)}
            y_test_target_int = [label_to_int_target[label.strip()] for label in y_test_target]
        else:
            y_test_target_int = y_test_target

        # Calculate and print ROC values for target_variable
        roc_df_target = calculate_roc_values(best_model_target, X_test_target, y_test_target_int)
        logging.info("ROC AUC Scores for target_variable:")
        logging.info(roc_df_target)
        roc_df_target.to_csv(args.roc_values_target, index=False)

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Repeat the process for associated_variable
    X_associated, y_associated = protein_embedding_train.get_embeddings_and_labels(label_type='associated_variable')
    logging.info(f"Shape of X_associated: {X_associated.shape}")

    # Full paths for associated_variable models
    rf_model_associated_full_path = os.path.join(model_dir, args.rf_model_associated)
    calibrated_model_associated_full_path = os.path.join(model_dir, 'calibrated_model_associated.pkl')

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Check if calibrated model for associated_variable already exists
    if os.path.exists(calibrated_model_associated_full_path):
        calibrated_model_associated = joblib.load(calibrated_model_associated_full_path)
        logging.info(f"Calibrated Random Forest model for associated_variable loaded from {calibrated_model_associated_full_path}")
    else:
        # Train the model for associated_variable
        support_model_associated = Support()
        calibrated_model_associated = support_model_associated.fit(X_associated, y_associated, model_name_prefix='associated', model_dir=model_dir, min_kmers=min_kmers)
        logging.info("Training and calibration for associated_variable completed.")
        
        # Plot learning curve
        logging.info("Plotting Learning Curve for Associated Variable")
        support_model_associated.plot_learning_curve(args.learning_curve_associated)

        # Save the calibrated model
        joblib.dump(calibrated_model_associated, calibrated_model_associated_full_path)
        logging.info(f"Calibrated Random Forest model for associated_variable saved at {calibrated_model_associated_full_path}")

        # Test the model
        best_score_associated, best_f1_associated, best_pr_auc_associated, best_params_associated, best_model_associated, X_test_associated, y_test_associated = support_model_associated.test_best_RF(X_associated, y_associated, scaler_dir=args.model_dir)

        logging.info(f"Best ROC AUC for associated_variable in test_best_RF: {best_score_associated}")
        logging.info(f"Best F1 Score for associated_variable in test_best_RF: {best_f1_associated}")
        logging.info(f"Best Precision-Recall AUC for associated_variable in test_best_RF: {best_pr_auc_associated}")
        logging.info(f"Best Parameters found in test_best_RF: {best_params_associated}")
        logging.info(f"Best associated_model in test_best_RF: {best_model_associated}")

        # Get class rankings for associated_variable
        class_rankings_associated = support_model_associated.get_class_rankings(X_test_associated)
        logging.info("Top 3 class rankings for the first 5 samples in associated data:")
        for i in range(min(5, len(class_rankings_associated))):
            logging.info(f"Sample {i+1}: Class Rankings - {class_rankings_associated[i][:3]}")  # Shows top 3 rankings

        # Access class_weight from the best_params_associated dictionary
        class_weight = best_params_associated.get('class_weight', None)
        # Print the results
        logging.info(f"Class weight used: {class_weight}")

        # Save the trained model for associated_variable
        joblib.dump(best_model_associated, rf_model_associated_full_path)
        logging.info(f"Random Forest model for associated_variable saved at {rf_model_associated_full_path}")

        # Plot ROC curve for associated_variable
        n_classes_associated = len(np.unique(y_test_associated))
        if n_classes_associated == 2:
            y_pred_proba_associated = best_model_associated.predict_proba(X_test_associated)[:, 1]
            unique_classes_associated = np.unique(y_test_associated)
        else:
            y_pred_proba_associated = best_model_associated.predict_proba(X_test_associated)
            unique_classes_associated = np.unique(y_test_associated).astype(str)
        plot_roc_curve_global(y_test_associated, y_pred_proba_associated, 'ROC Curve for Associated Variable', save_as=args.roc_curve_associated, classes=unique_classes_associated)

        # Convert y_test_associated to integer labels if necessary
        if isinstance(y_test_associated[0], str):
            unique_labels_associated = sorted(set(y_test_associated))
            label_to_int_associated = {label: idx for idx, label in enumerate(unique_labels_associated)}
            y_test_associated_int = [label_to_int_associated[label.strip()] for label in y_test_associated]
        else:
            y_test_associated_int = y_test_associated

        # Calculate and print ROC values for associated_variable
        roc_df_associated = calculate_roc_values(best_model_associated, X_test_associated, y_test_associated_int)
        logging.info("ROC AUC Scores for associated_variable:")
        logging.info(roc_df_associated)
        roc_df_associated.to_csv(args.roc_values_associated, index=False)

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # =============================
    # STEP 2: Classifying New Sequences
    # =============================

    # Load min_kmers
    min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
    if os.path.exists(min_kmers_path):
        with open(min_kmers_path, 'r') as f:
            min_kmers_loaded = int(f.read().strip())
        logging.info(f"min_kmers loaded: {min_kmers_loaded}")
    else:
        logging.error(f"min_kmers file not found at {min_kmers_path}. Ensure that training was completed successfully.")
        st.error(f"Arquivo min_kmers nÃ£o encontrado em {min_kmers_path}. Certifique-se de que o treinamento foi concluÃ­do com sucesso.")
        sys.exit(1)

    # Load data for prediction
    predict_alignment_path = args.predict_fasta

    # Check if prediction sequences are aligned
    if not are_sequences_aligned(predict_alignment_path):
        logging.info("Prediction sequences are not aligned. Realigning with MAFFT...")
        aligned_predict_path = predict_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(predict_alignment_path, aligned_predict_path, threads=1)  # Fix threads=1
        predict_alignment_path = aligned_predict_path
    else:
        logging.info(f"Aligned prediction file found or sequences are already aligned: {predict_alignment_path}")

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Initialize ProteinEmbedding for prediction, no need for the table
    protein_embedding_predict = ProteinEmbeddingGenerator(
        predict_alignment_path, 
        table_data=None,
        aggregation_method=args.aggregation_method  # Pass the aggregation method
    )
    protein_embedding_predict.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        min_kmers=min_kmers_loaded  # Use the same min_kmers from training
    )
    logging.info(f"Number of prediction embeddings generated: {len(protein_embedding_predict.embeddings)}")

    # Get embeddings for prediction
    X_predict = np.array([entry['embedding'] for entry in protein_embedding_predict.embeddings])

    # Load the scaler
    scaler_full_path = os.path.join(model_dir, args.scaler)
    if os.path.exists(scaler_full_path):
        scaler = joblib.load(scaler_full_path)
        logging.info(f"Scaler loaded from {scaler_full_path}")
    else:
        logging.error(f"Scaler not found at {scaler_full_path}")
        st.error(f"Scaler nÃ£o encontrado em {scaler_full_path}. Verifique se o treinamento foi concluÃ­do com sucesso.")
        sys.exit(1)
    X_predict_scaled = scaler.transform(X_predict)

    # Update progress
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # Make predictions on new sequences

    # Load calibrated models
    calibrated_model_target_path = os.path.join(model_dir, 'calibrated_model_target.pkl')
    calibrated_model_associated_path = os.path.join(model_dir, 'calibrated_model_associated.pkl')

    if os.path.exists(calibrated_model_target_path) and os.path.exists(calibrated_model_associated_path):
        calibrated_model_target = joblib.load(calibrated_model_target_path)
        calibrated_model_associated = joblib.load(calibrated_model_associated_path)
        logging.info("Calibrated models loaded successfully.")
    else:
        logging.error("Calibrated models not found. Verifique se o treinamento foi concluÃ­do com sucesso.")
        st.error("Modelos calibrados nÃ£o encontrados. Verifique se o treinamento foi concluÃ­do com sucesso.")
        sys.exit(1)

    # Check feature size against the original estimator of CalibratedClassifierCV
    if X_predict_scaled.shape[1] > calibrated_model_target.estimator.n_features_in_:
        logging.info(f"Reducing the number of features from {X_predict_scaled.shape[1]} to {calibrated_model_target.estimator.n_features_in_} to match the model's input size.")
        X_predict_scaled = X_predict_scaled[:, :calibrated_model_target.estimator.n_features_in_]

    # Make predictions for target_variable
    predictions_target = calibrated_model_target.predict(X_predict_scaled)

    # Make predictions for associated_variable
    if X_predict_scaled.shape[1] > calibrated_model_associated.estimator.n_features_in_:
        logging.info(f"Reducing the number of features from {X_predict_scaled.shape[1]} to {calibrated_model_associated.estimator.n_features_in_} to match the model's input size for associated_variable.")
        X_predict_scaled = X_predict_scaled[:, :calibrated_model_associated.estimator.n_features_in_]

    predictions_associated = calibrated_model_associated.predict(X_predict_scaled)

    # Get class rankings
    rankings_target = get_class_rankings_global(calibrated_model_target, X_predict_scaled)
    rankings_associated = get_class_rankings_global(calibrated_model_associated, X_predict_scaled)

    # Process and save the results
    results = {}
    for entry, pred_target, pred_associated, ranking_target, ranking_associated in zip(protein_embedding_predict.embeddings, predictions_target, predictions_associated, rankings_target, rankings_associated):
        sequence_id = entry['protein_accession']
        results[sequence_id] = {
            "target_prediction": pred_target,
            "associated_prediction": pred_associated,
            "target_ranking": ranking_target,
            "associated_ranking": ranking_associated
        }

    # Save the results to a file
    with open(args.results_file, 'w') as f:
        f.write("Protein_ID\tTarget_Prediction\tAssociated_Prediction\tTarget_Ranking\tAssociated_Ranking\n")
        for seq_id, result in results.items():
            f.write(f"{seq_id}\t{result['target_prediction']}\t{result['associated_prediction']}\t{'; '.join(result['target_ranking'])}\t{'; '.join(result['associated_ranking'])}\n")
            logging.info(f"{seq_id} - Target Variable: {result['target_prediction']}, Associated Variable: {result['associated_prediction']}, Target Ranking: {'; '.join(result['target_ranking'])}, Associated Ranking: {'; '.join(result['associated_ranking'])}")

    # Format the results for display in Streamlit
    formatted_results = []

    for sequence_id, info in results.items():
        associated_rankings = info['associated_ranking']
        if not associated_rankings:
            logging.warning(f"No associated ranking data for protein {sequence_id}. Skipping...")
            continue

        # Get top 3 rankings
        top3_rankings = associated_rankings[:3]
        predicted_ss = '-'.join([rank.split(": ")[0] for rank in top3_rankings])
        # Sum their probabilities
        prob = sum([float(rank.split(": ")[1].replace("%", "")) for rank in top3_rankings])
        complete_ranking = " - ".join(associated_rankings)
        formatted_results.append([sequence_id, predicted_ss, f"{prob:.2f}", complete_ranking])

    # Log to verify the content of formatted_results
    logging.info("Formatted Results:")
    for result in formatted_results:
        logging.info(result)

    # Create DataFrame for Streamlit
    df_results = pd.DataFrame(formatted_results, columns=["Query Name", "Predicted SS", "SS Prediction Probability (%)", "Complete Ranking"])

    # Display the results in Streamlit
    st.header("Prediction Results")
    st.table(df_results)

    # Save the results to an Excel file
    df_results.to_excel(args.excel_output, index=False)
    logging.info(f"Results saved at {args.excel_output}")

    # Save the table in a tabulated format
    with open(args.formatted_results_table, 'w') as f:
        f.write(tabulate(formatted_results, headers=["Query Name", "Predicted SS", "SS Prediction Probability (%)", "Complete Ranking"], tablefmt="grid"))
    logging.info(f"Formatted table saved at {args.formatted_results_table}")

    # Generate the Scatterplot of Predictions
    logging.info("Generating scatterplot of new sequence predictions...")
    plot_predictions_scatterplot_custom(results, args.scatterplot_output)
    logging.info(f"Scatterplot saved at {args.scatterplot_output}")

    logging.info("Processing completed.")

    # ============================================
    # STEP 3: Dimensionality Reduction and Plotting t-SNE & UMAP
    # ============================================
    # Removed as per user request

    # Update progress to 100%
    progress_bar.progress(1.0)
    progress_text.markdown("<span style='color:white'>Progress: 100%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

# Custom CSS for dark navy blue background and white text
st.markdown(
    """
    <style>
    /* Define the main app background and text color */
    .stApp {
        background-color: #0B3C5D;
        color: white;
    }
    /* Define the sidebar background and text color */
    [data-testid="stSidebar"] {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    /* Ensure all elements within the sidebar have a blue background and white text */
    [data-testid="stSidebar"] * {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    /* Customize input elements within the sidebar */
    [data-testid="stSidebar"] input,
    [data-testid="stSidebar"] select,
    [data-testid="stSidebar"] textarea,
    [data-testid="stSidebar"] button,
    [data-testid="stSidebar"] .stButton,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Customize the drag and drop area of the file uploader */
    [data-testid="stSidebar"] div[data-testid="stFileUploader"] div {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Customize dropdown selection options */
    [data-testid="stSidebar"] .stSelectbox [role="listbox"] {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    /* Remove borders and shadows */
    [data-testid="stSidebar"] .stButton > button,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        border: none !important;
        box-shadow: none !important;
    }
    /* Customize checkboxes and radio buttons */
    [data-testid="stSidebar"] .stCheckbox input[type="checkbox"] + div:first-of-type,
    [data-testid="stSidebar"] .stRadio input[type="radio"] + div:first-of-type {
        background-color: #1E3A8A !important;
    }
    /* Customize slider track and thumb */
    [data-testid="stSidebar"] .stSlider > div:first-of-type {
        color: white !important;
    }
    [data-testid="stSidebar"] .stSlider .st-bo {
        background-color: #1E3A8A !important;
    }
    /* Ensure headers are white */
    h1, h2, h3, h4, h5, h6 {
        color: white !important;
    }
    /* Ensure alert messages (st.info, st.error, etc.) have white text */
    div[role="alert"] p {
        color: white !important;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# Function to convert image to base64
def get_base64_image(image_path):
    """
    Encodes an image file to a base64 string.
    
    Parameters:
    - image_path (str): Path to the image file.
    
    Returns:
    - base64 string of the image.
    """
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return ""

# Image path
image_path = "./images/faal.png"
image_base64 = get_base64_image(image_path)
# Using HTML with st.markdown to align title and text

st.markdown(
    f"""
    <div style="text-align: center; font-family: 'Arial', sans-serif; padding: 30px; background: linear-gradient(to bottom, #f9f9f9, #ffffff); border-radius: 15px; border: 2px solid #dddddd; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); position: relative;">
        <p style="color: black; font-size: 1.5em; font-weight: bold; margin: 0;">
            FAALPred: Predicting Fatty Acid Ligase FAAL (FAALs) Fatty Acid Specificities Using Integrated Neural Networks, Bioinformatics, and Machine Learning Approaches
        </p>
        <p style="color: #2c3e50; font-size: 1.2em; font-weight: normal; margin-top: 10px;">
            Anne Liong, Leandro de Mattos Pereira, and Pedro LeÃ£o
        </p>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8;">
            <strong>FAALPred</strong> is a comprehensive bioinformatics tool designed to predict the substrate chain length specificity of fatty acid ligase FAALs (FAALs), ranging from C4 to C18.
        </p>
        <h5 style="color: #2c3e50; font-size: 20px; font-weight: bold; margin-top: 25px;">ABSTRACT</h5>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8; text-align: justify;">
            Fatty Acid-AMP Ligase (FAAL) enzymes, identified by Zhang et al. (2011), activate fatty acids of different lengths for the biosynthesis of natural products. These substrates allow the production of compounds such as nocuolin (<em>Nodularia sp.</em>, Martins et al., 2022) and sulfolipid-1 (<em>Mycobacterium tuberculosis</em>, Yan et al., 2023), with applications in cancer and tuberculosis treatment (Kurt et al., 2017; Gilmore et al., 2012). Dr. Pedro LeÃ£o and his team have identified several of these natural products in cyanobacteria (<a href="https://leaolab.wixsite.com/leaolab" target="_blank" style="color: #3498db; text-decoration: none;">visit here</a>), and FAALPred classifies FAALs by their substrate specificity.
        </p>
        <div style="text-align: center; margin-top: 20px;">
            <img src="data:image/png;base64,{image_base64}" alt="FAAL Domain" style="width: auto; height: 120px; object-fit: contain;">
            <p style="text-align: center; color: #2c3e50; font-size: 14px; margin-top: 5px;">
                <em>FAAL Domain of Synechococcus sp. PCC7002, link: <a href="https://www.rcsb.org/structure/7R7F" target="_blank" style="color: #3498db; text-decoration: none;">https://www.rcsb.org/structure/7R7F</a></em>
            </p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

# Sidebar for input parameters
st.sidebar.header("Input Parameters")

# Function to save uploaded files
def save_uploaded_file(uploaded_file, save_path):
    """
    Saves the uploaded file to the specified path.
    
    Parameters:
    - uploaded_file (UploadedFile): File uploaded via Streamlit.
    - save_path (str): Path to save the uploaded file.
    
    Returns:
    - save_path (str): Path where the file was saved.
    """
    with open(save_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())
    return save_path

# Input options
use_default_train = st.sidebar.checkbox("Use Default Training Data", value=True)
if not use_default_train:
    train_fasta_file = st.sidebar.file_uploader("Upload Training FASTA File", type=["fasta", "fa", "fna"])
    train_table_file = st.sidebar.file_uploader("Upload Training Table File (TSV)", type=["tsv"])
else:
    train_fasta_file = None
    train_table_file = None

predict_fasta_file = st.sidebar.file_uploader("Upload Prediction FASTA File", type=["fasta", "fa", "fna"])

kmer_size = st.sidebar.number_input("K-mer Size", min_value=1, max_value=10, value=3, step=1)
step_size = st.sidebar.number_input("Step Size", min_value=1, max_value=10, value=1, step=1)
aggregation_method = st.sidebar.selectbox(
    "Aggregation Method",
    options=['none', 'mean'],  # Removed 'median', 'sum', 'max'
    index=0
)

# Optional input for Word2Vec parameters
st.sidebar.header("Optional Word2Vec Parameters")
custom_word2vec = st.sidebar.checkbox("Customize Word2Vec Parameters", value=False)
if custom_word2vec:
    window = st.sidebar.number_input(
        "Window Size", min_value=5, max_value=20, value=10, step=1
    )
    workers = st.sidebar.number_input(
        "Workers", min_value=1, max_value=112, value=8, step=1
    )
    epochs = st.sidebar.number_input(
        "Iterations", min_value=1, max_value=3500, value=2500, step=100
    )
else:
    window = 10  # Default value
    workers = 8  # Default value
    epochs = 2500  # Default value

# Button to start processing
if st.sidebar.button("Run Analysis"):
    # Paths for internal data
    internal_train_fasta = "data/train.fasta"
    internal_train_table = "data/train_table.tsv"
    
    model_dir = create_unique_model_directory("results", aggregation_method)
    output_dir = model_dir
    # Handling training data
    if use_default_train:
        train_fasta_path = internal_train_fasta
        train_table_path = internal_train_table
        st.markdown("<span style='color:white'>Using default training data.</span>", unsafe_allow_html=True)
    else:
        if train_fasta_file is not None and train_table_file is not None:
            train_fasta_path = os.path.join(output_dir, "uploaded_train.fasta")
            train_table_path = os.path.join(output_dir, "uploaded_train_table.tsv")
            save_uploaded_file(train_fasta_file, train_fasta_path)
            save_uploaded_file(train_table_file, train_table_path)
            st.markdown("<span style='color:white'>Uploaded training data will be used.</span>", unsafe_allow_html=True)
        else:
            st.warning("Please upload both the training FASTA file and the TSV table file.")
            st.stop()

    # Handling prediction data
    if predict_fasta_file is not None:
        predict_fasta_path = os.path.join(output_dir, "uploaded_predict.fasta")
        save_uploaded_file(predict_fasta_file, predict_fasta_path)
    else:
        st.error("Please upload a prediction FASTA file.")
        st.stop()
        
    # Remaining parameters
    args = argparse.Namespace(
        train_fasta=train_fasta_path,
        train_table=train_table_path,
        predict_fasta=predict_fasta_path,
        kmer_size=kmer_size,
        step_size=step_size,
        aggregation_method=aggregation_method,
        results_file=os.path.join(output_dir, "predictions.tsv"),
        output_dir=output_dir,
        scatterplot_output=os.path.join(output_dir, "scatterplot_predictions.png"),
        excel_output=os.path.join(output_dir, "results.xlsx"),
        formatted_results_table=os.path.join(output_dir, "formatted_results.txt"),
        roc_curve_target=os.path.join(output_dir, "roc_curve_target.png"),
        roc_curve_associated=os.path.join(output_dir, "roc_curve_associated.png"),
        learning_curve_target=os.path.join(output_dir, "learning_curve_target.png"),
        learning_curve_associated=os.path.join(output_dir, "learning_curve_associated.png"),
        roc_values_target=os.path.join(output_dir, "roc_values_target.csv"),
        roc_values_associated=os.path.join(output_dir, "roc_values_associated.csv"),
        rf_model_target="rf_model_target.pkl",
        rf_model_associated="rf_model_associated.pkl",
        word2vec_model="word2vec_model.bin",
        scaler="scaler.pkl",
        model_dir=model_dir,
    )

    # Create the model directory if it doesn't exist
    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)

    # Execute the main analysis function
    st.markdown("<span style='color:white'>Processing data and running analysis...</span>", unsafe_allow_html=True)
    try:
        main(args)

        st.success("Analysis completed successfully!")

        # Display the scatterplot
        st.header("Prediction Scatterplot")
        scatterplot_path = os.path.join(args.output_dir, "scatterplot_predictions.png")
        if os.path.exists(scatterplot_path):
            st.image(scatterplot_path,  use_column_width=True)
        else:
            st.error(f"Scatterplot not found at {scatterplot_path}.")

        # Display the formatted results table
        formatted_table_path = args.formatted_results_table

        # Check if the file exists and is not empty
        if os.path.exists(formatted_table_path) and os.path.getsize(formatted_table_path) > 0:
            try:
                # Open and read the file content
                with open(formatted_table_path, 'r') as f:
                    formatted_table = f.read()

                # Display the content in Streamlit
                st.text(formatted_table)
            except Exception as e:
                st.error(f"An error occurred while reading the formatted results table: {e}")
        else:
            st.error(f"Formatted results table not found or is empty: {formatted_table_path}")

        # Prepare the results.zip file
        zip_buffer = BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zip_file:
            for folder_name, subfolders, filenames in os.walk(output_dir):
                for filename in filenames:
                    file_path = os.path.join(folder_name, filename)
                    zip_file.write(file_path, arcname=os.path.relpath(file_path, output_dir))
        zip_buffer.seek(0)

        # Provide the download link
        st.header("Download Results")
        st.download_button(
            label="Download All Results as results.zip",
            data=zip_buffer,
            file_name="results.zip",
            mime="application/zip"
        )

        # =============================
        # Additional Information Below the Footer
        # =============================

    except Exception as e:
        st.error(f"An error occurred during processing: {e}")
        logging.error(f"An error occurred: {e}")

# Function to load and resize images with DPI adjustment
def load_and_resize_image_with_dpi(image_path, base_width, dpi=300):
    """
    Loads and resizes an image with the specified DPI.
    
    Parameters:
    - image_path (str): Path to the image file.
    - base_width (int): Desired width of the image.
    - dpi (int): Dots per inch for the image.
    
    Returns:
    - Image object or None if not found.
    """
    try:
        # Load the image
        image = Image.open(image_path)
        # Calculate the new height proportionally
        w_percent = (base_width / float(image.size[0]))
        h_size = int((float(image.size[1]) * float(w_percent)))
        # Resize the image
        resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)
        return resized_image
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return None

# Image paths
image_dir = "images"
image_paths = [
    os.path.join(image_dir, "lab_logo.png"),
    os.path.join(image_dir, "ciimar.png"),
    os.path.join(image_dir, "faal_pred_logo.png"), 
    os.path.join(image_dir, "bbf4.png"),
    os.path.join(image_dir, "google.png"),
    os.path.join(image_dir, "uniao.png"),
]

# Load and resize all images
images = [load_and_resize_image_with_dpi(path, base_width=150, dpi=300) for path in image_paths]

# Encode images in base64
def encode_image(image):
    """
    Encodes a PIL Image to a base64 string.
    
    Parameters:
    - image (Image): PIL Image object.
    
    Returns:
    - base64 string of the image.
    """
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

encoded_images = [encode_image(img) for img in images if img is not None]

# CSS for layout
st.markdown(
    """
    <style>
    .footer-container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 10px;
        margin-bottom: 10px;
        flex-wrap: wrap;
    }
    .footer-text {
        text-align: center;
        color: white;
        font-size: 12px;
        margin-top: 10px;
    }
    .support-text {
        text-align: center;
        color: white;
        font-size: 14px;
        font-weight: bold;
        margin-bottom: 5px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# HTML to display images in the footer
footer_html = """
<div class="support-text">Supported by:</div>
<div class="footer-container">
    {}
</div>
<div class="footer-text" style="font-size: 18px; margin-top: 10px;">
    FAALPred will make predictions for all submitted sequences, even if they are not FAALs. To limit your input only to Fatty Acid Ligase FAALs, first consider extracting from each FAAL the sequences corresponding to the FAAL domain cd05931 (cd05931) using InterProScan.
</div>
<div class="footer-text" style="font-size: 14px; margin-top: 5px;">
    CIIMAR - Pedro LeÃ£o @CNP - 2024 - All rights reserved.
</div>
"""

# Generate <img> tags for each image
img_tags = "".join(
    f'<img src="data:image/png;base64,{img}" style="width: 100px;">' for img in encoded_images
)

# Render the footer
st.markdown(footer_html.format(img_tags), unsafe_allow_html=True)
