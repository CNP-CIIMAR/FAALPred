import logging
import os
import sys
import subprocess
import random
import zipfile
from collections import Counter, defaultdict
from sklearn.metrics.pairwise import cosine_similarity
from io import BytesIO
import shutil
import time
import argparse 
import numpy as np
import pandas as pd
from Bio import SeqIO, AlignIO
from Bio.Align.Applications import MafftCommandline
import joblib
import plotly.io as pio
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
# Precisamos das bibliotecas abaixo para oversampling multilabel
from imblearn.over_sampling import RandomOverSampler
from multi_label_smote import MultiLabelSMOTE   # <-- Biblioteca externa
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (f1_score, average_precision_score, roc_auc_score, 
                             roc_curve, auc)
from sklearn.model_selection import (GridSearchCV, train_test_split,
                                    KFold, StratifiedKFold)
from sklearn.preprocessing import (StandardScaler, MultiLabelBinarizer)
from tabulate import tabulate
from sklearn.calibration import CalibratedClassifierCV
from PIL import Image
from matplotlib import ticker
import umap.umap_ as umap
import umap
import base64
from plotly.graph_objs import Figure
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from sklearn.multiclass import OneVsRestClassifier

import logging
import random
import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.utils import resample
import numpy as np
# ============================================
# Definitions of Functions and Classes
# ============================================

# Setting seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,  # Change to DEBUG for more verbosity
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("logs/app.log"),  # Log to file for persistent records
    ],
)

# ============================================
# Streamlit Configuration and Interface
# ============================================

# Ensure st.set_page_config is the first Streamlit command
st.set_page_config(
    page_title="FAAL_Pred",
    page_icon="üß¨", 
    layout="wide",
    initial_sidebar_state="expanded",
)

def are_sequences_aligned(fasta_file: str) -> bool:
    """
    Checks if all sequences in a FASTA file are aligned by verifying they have the same length.
    
    Parameters:
    - fasta_file (str): Path to the FASTA file.
    
    Returns:
    - bool: True if all sequences are aligned (same length), False otherwise.
    """
    lengths = set()
    for record in SeqIO.parse(fasta_file, "fasta"):
        lengths.add(len(record.seq))
    return len(lengths) == 1  # Returns True if all sequences have the same length

def create_unique_model_directory(base_dir: str, aggregation_method: str) -> str:
    """
    Creates a unique directory for models based on the aggregation method.
    
    Parameters:
    - base_dir (str): Base directory for models.
    - aggregation_method (str): Aggregation method used.
    
    Returns:
    - str: Path to the unique model directory.
    """
    model_dir = os.path.join(base_dir, f"models_{aggregation_method}")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    return model_dir

def realign_sequences_with_mafft(input_path: str, output_path: str, threads: int = 8) -> None:
    """
    Realigns sequences using MAFFT.
    
    Parameters:
    - input_path (str): Path to the input file.
    - output_path (str): Path to save the realigned file.
    - threads (int): Number of threads for MAFFT.
    """
    mafft_command = ['mafft', '--thread', str(threads), '--maxiterate', '1000', '--localpair', input_path]
    try:
        with open(output_path, "w") as outfile:
            subprocess.run(mafft_command, stdout=outfile, stderr=subprocess.PIPE, check=True)
        logging.info(f"Realigned sequences saved to {output_path}")
    except subprocess.CalledProcessError as e:
        logging.error(f"Error executing MAFFT: {e.stderr.decode()}")
        sys.exit(1)

from sklearn.cluster import DBSCAN, KMeans

def perform_clustering(data: np.ndarray, method: str = "DBSCAN", eps: float = 0.5, min_samples: int = 5, n_clusters: int = 3) -> np.ndarray:
    """
    Performs clustering on the data using DBSCAN or K-Means.
    
    Parameters:
    - data (np.ndarray): Data for clustering.
    - method (str): Clustering method ("DBSCAN" or "K-Means").
    - eps (float): Epsilon parameter for DBSCAN.
    - min_samples (int): Minimum number of samples for DBSCAN.
    - n_clusters (int): Number of clusters for K-Means.
    
    Returns:
    - np.ndarray: Labels generated by the clustering method.
    """
    if method == "DBSCAN":
        clustering_model = DBSCAN(eps=eps, min_samples=min_samples)
    elif method == "K-Means":
        clustering_model = KMeans(n_clusters=n_clusters, random_state=42)
    else:
        raise ValueError(f"Invalid clustering method: {method}")

    labels = clustering_model.fit_predict(data)
    return labels

def plot_dual_tsne(
    train_embeddings: np.ndarray, 
    train_labels: list,
    train_protein_ids: list,
    predict_embeddings: np.ndarray, 
    predict_labels: list, 
    predict_protein_ids: list, 
    output_dir: str
) -> tuple:
    """
    Plots two 3D t-SNE plots and saves them as HTML files:
    - Plot 1: Training Data.
    - Plot 2: Predictions.
    
    Parameters:
    - train_embeddings (np.ndarray): Training embeddings.
    - train_labels (list): Labels of training data.
    - train_protein_ids (list): Protein IDs of training data.
    - predict_embeddings (np.ndarray): Prediction embeddings.
    - predict_labels (list): Prediction labels.
    - predict_protein_ids (list): Protein IDs of predictions.
    - output_dir (str): Directory to save t-SNE plots.
    
    Returns:
    - tuple: (Training Figure, Prediction Figure)
    """
    from sklearn.manifold import TSNE

    # Dimensionality reduction using t-SNE
    tsne_train = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)
    tsne_train_result = tsne_train.fit_transform(train_embeddings)

    tsne_predict = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)
    tsne_predict_result = tsne_predict.fit_transform(predict_embeddings)

    # Create color maps for training data
    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    # Create color maps for predictions
    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    # Convert labels to colors
    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    # Plot 1: Training Data
    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=tsne_train_result[:, 0],
        y=tsne_train_result[:, 1],
        z=tsne_train_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=train_colors,
            opacity=0.8
        ),
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='t-SNE 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Plot 2: Predictions
    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=tsne_predict_result[:, 0],
        y=tsne_predict_result[:, 1],
        z=tsne_predict_result[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=predict_colors,
            opacity=0.8
        ),
        text=[f"Protein ID: {protein_id}<br>Label: {label}" for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='t-SNE 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    # Save the plots as HTML files
    tsne_train_html = os.path.join(output_dir, "tsne_train_3d.html")
    tsne_predict_html = os.path.join(output_dir, "tsne_predict_3d.html")
        
    pio.write_html(fig_train, file=tsne_train_html, auto_open=False)
    pio.write_html(fig_predict, file=tsne_predict_html, auto_open=False)
    
    logging.info(f"t-SNE Training plot saved as {tsne_train_html}")
    logging.info(f"t-SNE Predictions plot saved as {tsne_predict_html}")

    return fig_train, fig_predict
    
def plot_roc_curve_global(y_true: np.ndarray, y_pred_proba: np.ndarray, title: str, save_as: str = None, classes: list = None) -> None:
    """
    Plots the ROC curve for binary or multiclass classifications.
    
    Parameters:
    - y_true (np.ndarray): True labels.
    - y_pred_proba (np.ndarray): Predicted probabilities.
    - title (str): Title of the plot.
    - save_as (str): Path to save the plot.
    - classes (list): List of classes (for multiclass).
    """
    lw = 2  # Line width

    # Check if it's binary or multiclass classification
    # (Note: for multilabel, plot_roc_curve_global pode precisar ser adaptado.
    #  Mantemos aqui a l√≥gica original para refer√™ncia.)
    unique_classes = np.unique(y_true)
    if len(unique_classes) == 2:  # Binary classification
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure()
        plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC Curve (area = %0.2f)' % roc_auc)
    else:  # Multiclass classification
        y_bin = label_binarize(y_true, classes=unique_classes)
        n_classes = y_bin.shape[1]

        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        plt.figure()

        colors = plt.cm.viridis(np.linspace(0, 1, n_classes))
        for i, color in zip(range(n_classes), colors):
            class_label = classes[i] if classes is not None else unique_classes[i]
            plt.plot(fpr[i], tpr[i], color=color, lw=lw, 
                     label=f'ROC Curve for class {class_label} (area = {roc_auc[i]:0.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=lw)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', color='white')
    plt.ylabel('True Positive Rate', color='white')
    plt.title(title, color='white')
    plt.legend(loc="center left", bbox_to_anchor=(1, 0.5))
    if save_as:
        plt.savefig(save_as, bbox_inches='tight', facecolor='#0B3C5D')  # Match background color
    plt.close()

def get_class_rankings_global(model, X: np.ndarray) -> list:
    """
    Obtains class rankings based on the predicted probabilities from a OneVsRestClassifier 
    or RandomForestClassifier for multi-label tasks.
    
    Parameters:
    - model: Classifier (could be OneVsRestClassifier wrapping RandomForest).
    - X (np.ndarray): Data to obtain predictions.
    
    Returns:
    - list: List of formatted class rankings for each sample.
    """
    if model is None:
        raise ValueError("Model not trained. Please train the model first.")

    # Se for OneVsRestClassifier, cada "predict_proba" retornar√° 
    # uma matriz shape [n_samples, n_outputs].
    # Abaixo, assumimos que o model.estimators_ possuem predict_proba.
    # No caso do RandomForestClassifier single-label, `model.predict_proba(X)` retorna 
    # shape [n_samples, n_classes].
    # Para o multi-label, √© shape [n_samples, n_labels].
    y_pred_proba = model.predict_proba(X)  # Pode ser multi-label ou single

    # Para multi-label, cada coluna √© a probabilidade de uma classe/label. 
    # Vamos criar um ranking sint√©tico para cada sample:
    # label 0 => prob, label 1 => prob, etc.
    # Exemplo de formata√ß√£o de "ranking":
    class_rankings = []
    # Pegamos o nome das classes do model, se for OneVsRestClassifier, iremos manter 
    # r√≥tulos coerentes
    if hasattr(model, "classes_"):  
        # Em OneVsRestClassifier, model.classes_ √© uma lista de arrays com as classes 
        # de cada estimador, mas para multi-label com binariza√ß√£o, 
        # cada label deve ser [0,1]. Vamos ent√£o criar um label textual "Label_0", "Label_1" etc.
        # Alternativamente, se usarmos MultiLabelBinarizer, podemos armazenar as classes 
        # do mlb (ex.: mlb.classes_).
        class_labels = list(model.classes_)
    else:
        # Caso seja algo diferente, inventariamos classes gen√©ricas
        class_labels = [f"Label_{i}" for i in range(y_pred_proba.shape[1])]

    for sample_proba in y_pred_proba:
        # sample_proba √© shape [n_labels]
        # Vamos criar pares (Label_i, prob) e ordenar decrescente
        pairs = sorted(
            zip(class_labels, sample_proba),
            key=lambda x: x[1],
            reverse=True
        )
        # Formata em "Label_i: XX.XX%" 
        formatted_rankings = [
            f"{p[0]}: {p[1]*100:.2f}%" for p in pairs
        ]
        class_rankings.append(formatted_rankings)

    return class_rankings

def calculate_roc_values(model: RandomForestClassifier, X_test: np.ndarray, y_test: np.ndarray) -> pd.DataFrame:
    """
    Calculates ROC AUC scores for each class.
    
    Parameters:
    - model (RandomForestClassifier): Trained model.
    - X_test (np.ndarray): Test data.
    - y_test (np.ndarray): True test labels.
    
    Returns:
    - pd.DataFrame: DataFrame containing ROC AUC scores per class.
    """
    n_classes = len(np.unique(y_test))
    y_pred_proba = model.predict_proba(X_test)

    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)
        roc_auc[i] = auc(fpr[i], tpr[i])

        # Logging ROC values
        logging.info(f"For class {i}:")
        logging.info(f"FPR: {fpr[i]}")
        logging.info(f"TPR: {tpr[i]}")
        logging.info(f"ROC AUC: {roc_auc[i]}")
        logging.info("--------------------------")

    roc_df = pd.DataFrame(list(roc_auc.items()), columns=['Class', 'ROC AUC'])
    return roc_df

def visualize_latent_space_with_similarity(
    X_original: np.ndarray, 
    X_synthetic: np.ndarray, 
    y_original: np.ndarray, 
    y_synthetic: np.ndarray, 
    protein_ids_original: list, 
    protein_ids_synthetic: list, 
    var_assoc_original: list, 
    var_assoc_synthetic: list, 
    output_dir: str = None
) -> Figure:
    """
    Visualizes the latent space using 3D UMAP with similarity measures between 
    original and synthetic samples.
    
    Parameters:
    - X_original (np.ndarray): Original embeddings.
    - X_synthetic (np.ndarray): Synthetic embeddings.
    - y_original (np.ndarray): Original labels.
    - y_synthetic (np.ndarray): Synthetic labels.
    - protein_ids_original (list): IDs of original proteins.
    - protein_ids_synthetic (list): IDs of synthetic proteins.
    - var_assoc_original (list): Associated variables of original data.
    - var_assoc_synthetic (list): Associated variables of synthetic data.
    - output_dir (str): Directory to save the plot.
    
    Returns:
    - Figure: Plotly figure object.
    """
    # Combine data for UMAP
    X_combined = np.vstack([X_original, X_synthetic])
    y_combined = np.hstack([y_original, y_synthetic])  # Usado se for necess√°rio

    # Apply UMAP for dimensionality reduction
    umap_reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)
    X_transformed = umap_reducer.fit_transform(X_combined)

    # Split transformed data
    X_transformed_original = X_transformed[:len(X_original)]
    X_transformed_synthetic = X_transformed[len(X_original):]

    # Calculate similarities between synthetic and original samples
    similarities = cosine_similarity(X_synthetic, X_original)
    max_similarities = similarities.max(axis=1)  # Maximum similarity for each synthetic sample
    closest_indices = similarities.argmax(axis=1)  # Indices of closest original samples

    df_original = pd.DataFrame({
        'x': X_transformed_original[:, 0],
        'y': X_transformed_original[:, 1],
        'z': X_transformed_original[:, 2],
        'Protein ID': protein_ids_original,
        'Associated Variable': var_assoc_original,
        'Type': 'Original'
    })

    df_synthetic = pd.DataFrame({
        'x': X_transformed_synthetic[:, 0],
        'y': X_transformed_synthetic[:, 1],
        'z': X_transformed_synthetic[:, 2],
        'Protein ID': protein_ids_synthetic,
        'Associated Variable': var_assoc_synthetic,
        'Similarity': max_similarities,
        'Closest Protein': [protein_ids_original[idx] for idx in closest_indices],
        'Closest Variable': [var_assoc_original[idx] for idx in closest_indices],
        'Type': 'Synthetic'
    })

    fig = go.Figure()

    fig.add_trace(go.Scatter3d(
        x=df_original['x'], 
        y=df_original['y'],
        z=df_original['z'],
        mode='markers',
        marker=dict(size=8, color='blue', opacity=0.7),
        name='Original',
        text=df_original.apply(lambda row: f"Protein ID: {row['Protein ID']}<br>"
                                           f"Associated Variable: {row['Associated Variable']}", axis=1),
        hoverinfo='text'
    ))

    fig.add_trace(go.Scatter3d(
        x=df_synthetic['x'], 
        y=df_synthetic['y'],
        z=df_synthetic['z'],
        mode='markers',
        marker=dict(size=8, color='red', opacity=0.7),
        name='Synthetic',
        text=df_synthetic.apply(lambda row: f"Protein ID: {row['Protein ID']}<br>"
                                           f"Associated Variable: {row['Associated Variable']}<br>"
                                           f"Similarity: {row['Similarity']:.4f}<br>"
                                           f"Closest Protein: {row['Closest Protein']}<br>"
                                           f"Closest Variable: {row['Closest Variable']}", axis=1),
        hoverinfo='text'
    ))

    fig.update_layout(
        title="Latent Space Visualization with Similarity (UMAP 3D)",
        scene=dict(
            xaxis_title="UMAP Dimension 1",
            yaxis_title="UMAP Dimension 2",
            zaxis_title="UMAP Dimension 3"
        ),
        legend=dict(orientation="h", y=-0.1),
        template="plotly_dark"
    )

    if output_dir:
        umap_similarity_path = os.path.join(output_dir, "umap_similarity_3D.html")
        fig.write_html(umap_similarity_path)
        logging.info(f"UMAP plot saved at {umap_similarity_path}")

    return fig

def format_and_sum_probabilities(associated_rankings: list) -> tuple:
    """
    Formats and sums probabilities for each category, returning only the main category.
    (Mantido aqui como exemplo de como somar probabilidades 
     para cada subcategoria, caso necess√°rio.)
    
    Parameters:
    - associated_rankings (list): List of associated rankings.
    
    Returns:
    - tuple: (main category with confidence, sum of probabilities, top two categories)
    """
    category_sums = {}
    categories = ['C4-C6-C8', 'C6-C8-C10', 'C8-C10-C12', 'C10-C12-C14', 'C12-C14-C16', 'C14-C16-C18']
    pattern_mapping = {
        'C4-C6-C8': ['C4', 'C6', 'C8'],
        'C6-C8-C10': ['C6', 'C8', 'C10'],
        'C8-C10-C12': ['C8', 'C10', 'C12'],
        'C10-C12-C14': ['C10', 'C12', 'C14'],
        'C12-C14-C16': ['C12', 'C14', 'C16'],
        'C14-C16-C18': ['C14', 'C16', 'C18'],
    }

    for category in categories:
        category_sums[category] = 0.0

    for rank in associated_rankings:
        try:
            prob = float(rank.split(": ")[1].replace("%", ""))
        except (IndexError, ValueError):
            logging.error(f"Error processing ranking string: {rank}")
            continue
        for cat, patterns in pattern_mapping.items():
            if any(pattern in rank for pattern in patterns):
                category_sums[cat] += prob

    if not category_sums:
        return None, None, None

    top_category, top_sum = max(category_sums.items(), key=lambda x: x[1])
    sorted_categories = sorted(category_sums.items(), key=lambda x: x[1], reverse=True)
    top_two = sorted_categories[:2] if len(sorted_categories) >= 2 else sorted_categories

    top_two_categories = [f"{cat} ({prob:.2f}%)" for cat, prob in top_two]
    top_category_with_confidence = f"{top_category} ({top_sum:.2f}%)"

    return top_category_with_confidence, top_sum, top_two_categories

# Para suporte a Multi-Label, vamos usar OneVsRestClassifier

# ============================================
# Classe MLSMOTE (Custom Multi-Label SMOTE)
# ============================================
class MLSMOTE:
    """
    Classe personalizada para implementar a t√©cnica MLSMOTE 
    (Multi-Label Synthetic Minority Over-sampling Technique).
    """

    def __init__(self, n_neighbors=5, random_state=None, min_samples=5):
        self.n_neighbors = n_neighbors
        self.random_state = random_state
        self.min_samples = min_samples
        if self.random_state:
            np.random.seed(self.random_state)
            random.seed(self.random_state)

    def fit_resample(self, X, y, n_samples):
        """
        Realiza o oversampling no conjunto de dados.

        Args:
            X (pd.DataFrame ou np.ndarray): Features.
            y (pd.DataFrame ou np.ndarray): Labels bin√°rias (multilabel).
            n_samples (int): N√∫mero de novas amostras a serem geradas.
        Returns:
            X_res, y_res (pd.DataFrame): Conjuntos aumentados.
        """
        if isinstance(X, pd.DataFrame):
            X = X.reset_index(drop=True)
        else:
            X = pd.DataFrame(X)

        if isinstance(y, pd.DataFrame):
            y = y.reset_index(drop=True)
        else:
            y = pd.DataFrame(y)

        X_sub, y_sub = self.get_minority_instance(X, y)
        logging.info(f"N√∫mero de amostras minorit√°rias: {X_sub.shape[0]}")

        if X_sub.shape[0] < self.min_samples:
            oversampled_X, oversampled_y = self.random_oversample(X_sub, y_sub, self.min_samples)
            X_sub = pd.concat([X_sub, oversampled_X], axis=0).reset_index(drop=True)
            y_sub = pd.concat([y_sub, oversampled_y], axis=0).reset_index(drop=True)
            logging.info(f"Amostras minorit√°rias ap√≥s Random OverSampling: {X_sub.shape[0]}")

        neighbors = self.find_nearest_neighbors(X_sub)
        logging.info(f"Vizinhos mais pr√≥ximos encontrados para as amostras minorit√°rias.")

        new_X, new_y = self.generate_synthetic_samples(X_sub, y_sub, neighbors, n_samples)
        logging.info(f"Novas amostras sint√©ticas geradas: {new_X.shape[0]}")

        X_res = pd.concat([X, new_X], axis=0).reset_index(drop=True)
        y_res = pd.concat([y, new_y], axis=0).reset_index(drop=True)

        # Verifica se h√° classes com <2 amostras
        y_res_np = y_res.values
        class_counts = y_res_np.sum(axis=0)
        classes_to_oversample = np.where(class_counts < 2)[0]

        if len(classes_to_oversample) > 0:
            logging.info(f"Classes com menos de 2 amostras: {classes_to_oversample.tolist()}")
            for class_idx in classes_to_oversample:
                mask = y_res_np[:, class_idx] == 1
                X_minority = X_res[mask]
                y_minority = y_res[mask]
                if X_minority.shape[0] < self.min_samples:
                    overX, overY = self.random_oversample(X_minority, y_minority, self.min_samples)
                    X_res = pd.concat([X_res, overX], axis=0).reset_index(drop=True)
                    y_res = pd.concat([y_res, overY], axis=0).reset_index(drop=True)
                    logging.info(f"Random OverSampling aplicado para a classe {class_idx}. Novas amostras: {overX.shape[0]}")

        return X_res, y_res

    def get_tail_label(self, y):
        columns = y.columns
        n = len(columns)
        counts = np.zeros(n)
        for column in range(n):
            counts[column] = y[columns[column]].sum()
        ratios = counts.max() / counts
        average_ratio = ratios.mean()
        tail_labels = [columns[i] for i in range(n) if ratios[i] > average_ratio]
        logging.info(f"Classes minorit√°rias identificadas: {tail_labels}")
        return tail_labels

    def get_index(self, y):
        tail_labels = self.get_tail_label(y)
        index = set()
        for tail_label in tail_labels:
            sub_index = set(y[y[tail_label] == 1].index)
            index = index.union(sub_index)
        logging.info(f"N√∫mero total de amostras minorit√°rias: {len(index)}")
        return list(index)

    def get_minority_instance(self, X, y):
        index = self.get_index(y)
        X_sub = X.loc[index].reset_index(drop=True)
        y_sub = y.loc[index].reset_index(drop=True)
        return X_sub, y_sub

    def random_oversample(self, X_sub, y_sub, min_samples):
        n_samples_current = X_sub.shape[0]
        n_samples_needed = max(min_samples - n_samples_current, 0)
        if n_samples_needed == 0:
            return pd.DataFrame(), pd.DataFrame()
        overX, overY = resample(
            X_sub,
            y_sub,
            replace=True,
            n_samples=n_samples_needed,
            random_state=self.random_state
        )
        logging.info(f"Random OverSampling: Geradas {n_samples_needed} novas amostras.")
        return overX, overY

    def find_nearest_neighbors(self, X_sub):
        from sklearn.neighbors import NearestNeighbors
        current_n_neighbors = min(self.n_neighbors, X_sub.shape[0])
        if current_n_neighbors < 2:
            raise ValueError(f"N√£o h√° amostras suficientes para NearestNeighbors. n_neighbors={self.n_neighbors}, n_samples_fit={X_sub.shape[0]}")
        nbrs = NearestNeighbors(n_neighbors=current_n_neighbors, metric='euclidean', algorithm='kd_tree').fit(X_sub)
        distances, indices = nbrs.kneighbors(X_sub)
        return indices

    def generate_synthetic_samples(self, X_sub, y_sub, neighbors, n_samples):
        new_X = []
        new_y = []
        n_minority = X_sub.shape[0]
        for _ in range(n_samples):
            reference_idx = random.randint(0, n_minority - 1)
            neighbor_pool = neighbors[reference_idx].tolist()
            if len(neighbor_pool) > 1 and reference_idx in neighbor_pool:
                neighbor_pool.remove(reference_idx)
            if not neighbor_pool:
                continue
            neighbor_idx = random.choice(neighbor_pool)

            reference_point = X_sub.iloc[reference_idx].values
            neighbor_point = X_sub.iloc[neighbor_idx].values

            ratio = random.random()
            synthetic_point = reference_point + ratio * (neighbor_point - reference_point)
            new_X.append(synthetic_point)

            synthetic_label = y_sub.iloc[neighbor_idx].values
            new_y.append(synthetic_label)

        if new_X and new_y:
            new_X = pd.DataFrame(new_X, columns=X_sub.columns)
            new_y = pd.DataFrame(new_y, columns=y_sub.columns)
        else:
            new_X = pd.DataFrame(columns=X_sub.columns)
            new_y = pd.DataFrame(columns=y_sub.columns)

        return new_X, new_y

# ============================================
# Classe de Suporte
# ============================================
class Support:
    """
    Classe para treinar e avaliar RandomForest multi-label,
    usando Random OverSampling (combinando classes compostas) e
    a classe MLSMOTE para gerar amostras sint√©ticas em problemas multilabel.
    """

    def __init__(self, cv: int = 5, seed: int = SEED, n_jobs: int = 8):
        self.cv = cv
        self.seed = seed
        self.n_jobs = n_jobs
        self.model = None
        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []
        self.roc_results = []
        self.train_sizes = np.linspace(.1, 1.0, 5)
        self.standard = StandardScaler()
        self.best_params = {}

        self.init_params = {
            "n_estimators": 100,
            "max_depth": 2,
            "min_samples_split": 2,
            "min_samples_leaf": 2,
            "criterion": "entropy",
            "max_features": "sqrt",
            "class_weight": "balanced_subsample",
            "max_leaf_nodes": 5,
            "min_impurity_decrease": 0.01,
            "bootstrap": True,
            "ccp_alpha": 0.005,            
       }

        self.parameters = {
            "estimator__n_estimators": [250,300],
            "estimator__max_depth": [10, 20],
            "estimator__min_samples_split": [4,6],
            "estimator__min_samples_leaf": [4, 6],
            "estimator__criterion": ["gini", "entropy"],
            "estimator__max_features": ["log2"],
            "estimator__class_weight": ["balanced",  None],
            "estimator__max_leaf_nodes": [10, 20, None],
            "estimator__min_impurity_decrease": [0.0],
            "estimator__bootstrap": [True, False],
            "estimator__ccp_alpha": [0.0],
        }

    def _oversample_multilabel(self, X: np.ndarray, y: np.ndarray) -> tuple:
        """
        1) Usa RandomOverSampler para aumentar as 'classes compostas' (linhas de y).
        2) Usa MLSMOTE para gerar amostras sint√©ticas no contexto multilabel.
        """
        # Transforma y em DataFrame para facilitar
        if not isinstance(X, pd.DataFrame):
            X_df = pd.DataFrame(X)
        else:
            X_df = X.copy()
        if not isinstance(y, pd.DataFrame):
            y_df = pd.DataFrame(y)
        else:
            y_df = y.copy()

        # 1) Agrupando combina√ß√µes
        composed_classes = [tuple(row) for row in y_df.values.astype(int)]
        ros = RandomOverSampler(random_state=self.seed)
        X_ros, composed_ros = ros.fit_resample(X_df, composed_classes)
        y_ros = pd.DataFrame([list(c) for c in composed_ros], columns=y_df.columns)

        # 2) MLSMOTE custom
        mlsmote = MLSMOTE(n_neighbors=5, random_state=self.seed, min_samples=5)
        # definimos "n_samples" como, por exemplo, len(X_ros)
        # ou alguma heur√≠stica (ex.: 20% do total). Para fins de exemplo:
        n_extra = int(len(X_ros) * 0.3)  # 30% a mais, adapt√°vel
        if n_extra <= 0:
            n_extra = 1
        X_synth, y_synth = mlsmote.fit_resample(X_ros, y_ros, n_extra)

        # Converte de volta para numpy
        X_final = X_synth.values
        y_final = y_synth.values
        return X_final, y_final

    def fit(self, 
            X: np.ndarray, 
            y: np.ndarray, 
            protein_ids: list = None, 
            var_assoc: list = None, 
            model_name_prefix: str = 'model', 
            model_dir: str = None, 
            min_kmers: int = None):
        logging.info(f"Starting fit method for {model_name_prefix}...")

        if min_kmers is not None:
            logging.info(f"Using provided min_kmers: {min_kmers}")
        else:
            min_kmers = len(X)

        # Aplica oversampling multilabel custom
        X_smote, y_smote = self._oversample_multilabel(X, y)

        if protein_ids is not None and var_assoc is not None:
            # Visualiza√ß√£o do espa√ßo latente
            visualize_latent_space_with_similarity(
                X_original=X, 
                X_synthetic=X_smote[len(y):],
                y_original=y, 
                y_synthetic=y_smote[len(y):],
                protein_ids_original=protein_ids,
                protein_ids_synthetic=[f"synthetic_{i}" for i in range(len(X), len(X_smote))],
                var_assoc_original=var_assoc,
                var_assoc_synthetic=[f"synthetic_var" for _ in range(len(X), len(X_smote))],
                output_dir=model_dir
            )

        kf = KFold(n_splits=self.cv, shuffle=True, random_state=self.seed)
        self.train_scores = []
        self.test_scores = []
        self.f1_scores = []
        self.pr_auc_scores = []

        fold_number = 0
        for train_idx, test_idx in kf.split(X_smote, y_smote):
            fold_number += 1
            X_train, X_test = X_smote[train_idx], X_smote[test_idx]
            y_train, y_test = y_smote[train_idx], y_smote[test_idx]

            base_rf = RandomForestClassifier(**self.init_params, n_jobs=self.n_jobs, random_state=self.seed)
            self.model = OneVsRestClassifier(base_rf)
            self.model.fit(X_train, y_train)

            train_score = self.model.score(X_train, y_train)
            test_score = self.model.score(X_test, y_test)
            y_pred = self.model.predict(X_test)

            f1 = f1_score(y_test, y_pred, average='micro')
            self.f1_scores.append(f1)
            self.train_scores.append(train_score)
            self.test_scores.append(test_score)

            y_pred_proba = self.model.predict_proba(X_test)
            pr_auc = average_precision_score(y_test, y_pred_proba, average='micro')
            self.pr_auc_scores.append(pr_auc)

            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Train Score: {train_score}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: Test Score: {test_score}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: F1 micro: {f1}")
            logging.info(f"Fold {fold_number} [{model_name_prefix}]: PR AUC micro: {pr_auc}")

            try:
                roc_auc_score_value = roc_auc_score(y_test, y_pred_proba, average='micro')
                self.roc_results.append(roc_auc_score_value)
            except ValueError:
                logging.warning(f"Unable to calculate ROC AUC for fold {fold_number}.")

            best_model, best_params = self._perform_grid_search(X_train, y_train)
            self.model = best_model
            self.best_params = best_params

            if model_dir:
                best_model_filename = os.path.join(model_dir, f'model_best_{model_name_prefix}.pkl')
                os.makedirs(os.path.dirname(best_model_filename), exist_ok=True)
                joblib.dump(best_model, best_model_filename)
                logging.info(f"Best model saved as {best_model_filename} for {model_name_prefix}")
            else:
                best_model_filename = f'model_best_{model_name_prefix}.pkl'
                joblib.dump(best_model, best_model_filename)

            if best_params is not None:
                self.best_params = best_params
                logging.info(f"Best parameters for {model_name_prefix}: {self.best_params}")
            else:
                logging.warning(f"No best parameters found in grid search for {model_name_prefix}.")

            # Calibra√ß√£o
            calibrator = CalibratedClassifierCV(self.model, method='isotonic', cv=3, n_jobs=self.n_jobs)
            calibrator.fit(X_train, y_train)
            self.model = calibrator

            if model_dir:
                calibrated_model_filename = os.path.join(model_dir, f'calibrated_model_{model_name_prefix}.pkl')
            else:
                calibrated_model_filename = f'calibrated_model_{model_name_prefix}.pkl'
            joblib.dump(calibrator, calibrated_model_filename)
            logging.info(f"Calibrated model saved as {calibrated_model_filename} for {model_name_prefix}")

        return self.model

    def _perform_grid_search(self, X_train, y_train) -> tuple:
        base_rf = RandomForestClassifier(random_state=self.seed)
        estimator = OneVsRestClassifier(base_rf)

        kf = KFold(n_splits=self.cv, shuffle=True, random_state=self.seed)
        grid_search = GridSearchCV(
            estimator,
            self.parameters,
            cv=kf,
            n_jobs=self.n_jobs,
            scoring='f1_micro',
            verbose=1
        )
        grid_search.fit(X_train, y_train)
        logging.info(f"Best parameters from grid search: {grid_search.best_params_}")
        return grid_search.best_estimator_, grid_search.best_params_

    def get_class_rankings(self, X: np.ndarray) -> list:
        if self.model is None:
            raise ValueError("Model not trained.")
        y_pred_proba = self.model.predict_proba(X)
        if hasattr(self.model, "classes_"):
            class_labels = self.model.classes_
        else:
            class_labels = [f"Label_{i}" for i in range(y_pred_proba.shape[1])]
        rankings = []
        for sample_proba in y_pred_proba:
            pairs = sorted(zip(class_labels, sample_proba), key=lambda x: x[1], reverse=True)
            formatted_rankings = [f"{cls}: {prob*100:.2f}%" for cls, prob in pairs]
            rankings.append(formatted_rankings)
        return rankings

    def plot_learning_curve(self, output_path: str) -> None:
        plt.figure()
        plt.plot(self.train_scores, label='Train Score')
        plt.plot(self.test_scores, label='Test Score')
        plt.plot(self.f1_scores, label='F1 micro')
        plt.plot(self.pr_auc_scores, label='PR AUC micro')
        plt.title("Learning Curve (Multilabel + MLSMOTE)", color='white')
        plt.xlabel("Fold", fontsize=12, fontweight='bold', color='white')
        plt.ylabel("Score", fontsize=12, fontweight='bold', color='white')
        plt.legend(loc="best")
        plt.grid(color='white', linestyle='--', linewidth=0.5)
        plt.savefig(output_path, facecolor='#0B3C5D')
        plt.close()

    def test_best_RF(self, X: np.ndarray, y: np.ndarray, scaler_dir: str = '.'):
        scaler_path = os.path.join(model_dir, 'scaler_associated.pkl') if model_dir else 'scaler_associated.pkl'
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            logging.info(f"Scaler loaded from {scaler_path}")
        else:
            logging.error(f"Scaler not found at {scaler_path}.")
            sys.exit(1)

        X_scaled = scaler.transform(X)
        X_res, y_res = self._oversample_multilabel(X_scaled, y)

        X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.4, random_state=self.seed)

        base_rf = RandomForestClassifier(
            n_estimators=self.best_params.get('estimator__n_estimators', 100),
            max_depth=self.best_params.get('estimator__max_depth', 10),
            min_samples_split=self.best_params.get('estimator__min_samples_split', 2),
            min_samples_leaf=self.best_params.get('estimator__min_samples_leaf', 4),
            criterion=self.best_params.get('estimator__criterion', 'gini'),
            max_features=self.best_params.get('estimator__max_features', 'log2'),
            class_weight=self.best_params.get('estimator__class_weight', 'balanced'),
            max_leaf_nodes=self.best_params.get('estimator__max_leaf_nodes', 20),
            min_impurity_decrease=self.best_params.get('estimator__min_impurity_decrease', 0.0),
            bootstrap=self.best_params.get('estimator__bootstrap', True),
            ccp_alpha=self.best_params.get('estimator__ccp_alpha', 0.0),
            random_state=self.seed,
            n_jobs=self.n_jobs
        )
        model_ovr = OneVsRestClassifier(base_rf)
        model_ovr.fit(X_train, y_train)

        calibrator = CalibratedClassifierCV(model_ovr, method='isotonic', cv=3, n_jobs=self.n_jobs)
        calibrator.fit(X_train, y_train)
        calibrated_model = calibrator

        y_pred_proba = calibrated_model.predict_proba(X_test)
        y_pred = calibrated_model.predict(X_test)

        f1_val = f1_score(y_test, y_pred, average='micro')
        pr_auc = average_precision_score(y_test, y_pred_proba, average='micro')
        try:
            roc_val = roc_auc_score(y_test, y_pred_proba, average='micro')
        except ValueError:
            roc_val = 0.0

        return roc_val, f1_val, pr_auc, self.best_params, calibrated_model, X_test, y_test

    
class ProteinEmbeddingGenerator:
    """
    Class to generate protein embeddings using Word2Vec.
    """

    def __init__(self, sequences_path: str, table_data: pd.DataFrame = None, aggregation_method: str = 'none'):
        aligned_path = sequences_path
        if not are_sequences_aligned(sequences_path):
            realign_sequences_with_mafft(sequences_path, sequences_path.replace(".fasta", "_aligned.fasta"), threads=1)
            aligned_path = sequences_path.replace(".fasta", "_aligned.fasta")
        else:
            logging.info(f"Sequences are already aligned: {sequences_path}")

        self.alignment = AlignIO.read(aligned_path, 'fasta')
        self.table_data = table_data
        self.embeddings = []
        self.models = {}
        self.aggregation_method = aggregation_method
        self.min_kmers = None

    def generate_embeddings(
        self, 
        k: int = 3, 
        step_size: int = 1, 
        word2vec_model_path: str = "word2vec_model.bin", 
        model_dir: str = None, 
        min_kmers: int = None, 
        save_min_kmers: bool = False
    ) -> None:
        if model_dir:
            word2vec_model_full_path = os.path.join(model_dir, word2vec_model_path)
        else:
            word2vec_model_full_path = word2vec_model_path

        if os.path.exists(word2vec_model_full_path):
            logging.info(f"Word2Vec model found at {word2vec_model_full_path}. Loading the model.")
            model = Word2Vec.load(word2vec_model_full_path)
            self.models['global'] = model
        else:
            logging.info("Word2Vec model not found. Training a new model.")
            kmer_groups = {}
            all_kmers = []
            kmers_counts = []

            for record in self.alignment:
                sequence = str(record.seq)
                seq_len = len(sequence)
                protein_accession_alignment = record.id.split()[0]

                if self.table_data is not None:
                    matching_rows = self.table_data['Protein.accession'].str.split().str[0] == protein_accession_alignment
                    matching_info = self.table_data[matching_rows]
                    if matching_info.empty:
                        logging.warning(f"No matching table data for {protein_accession_alignment}")
                        continue

                    target_variable = matching_info['Target variable'].values[0]
                    associated_variable = matching_info['Associated variable'].values[0]
                else:
                    target_variable = None
                    associated_variable = None

                logging.info(f"Processing {protein_accession_alignment} with sequence length {seq_len}")

                if seq_len < k:
                    logging.warning(f"Sequence too short for {protein_accession_alignment}. Length: {seq_len}")
                    continue

                kmers = [sequence[i:i + k] for i in range(0, seq_len - k + 1, step_size)]
                kmers = [kmer for kmer in kmers if kmer.count('-') < k]

                if not kmers:
                    logging.warning(f"No valid k-mers for {protein_accession_alignment}")
                    continue

                all_kmers.append(kmers)
                kmers_counts.append(len(kmers))

                embedding_info = {
                    'protein_accession': protein_accession_alignment,
                    'target_variable': target_variable,
                    'associated_variable': associated_variable,
                    'kmers': kmers
                }
                kmer_groups[protein_accession_alignment] = embedding_info

            if not kmers_counts:
                logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
                sys.exit(1)

            if min_kmers is not None:
                self.min_kmers = min_kmers
                logging.info(f"Using provided min_kmers: {self.min_kmers}")
            else:
                self.min_kmers = min(kmers_counts)
                logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

            if save_min_kmers and model_dir:
                min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
                with open(min_kmers_path, 'w') as f:
                    f.write(str(self.min_kmers))
                logging.info(f"min_kmers saved at {min_kmers_path}")

            model = Word2Vec(
                sentences=all_kmers,
                vector_size=390,
                window=window,
                min_count=1,
                workers=workers,
                sg=1,
                hs=1,
                negative=0,
                epochs=epochs,
                seed=SEED
            )

            if model_dir:
                os.makedirs(os.path.dirname(word2vec_model_full_path), exist_ok=True)
            model.save(word2vec_model_full_path)
            self.models['global'] = model
            logging.info(f"Word2Vec model saved at {word2vec_model_full_path}")

        # Gera√ß√£o de embeddings padronizada
        kmer_groups = {}
        kmers_counts = []
        all_kmers = []

        for record in self.alignment:
            sequence_id = record.id.split()[0]
            sequence = str(record.seq)

            if self.table_data is not None:
                matching_rows = self.table_data['Protein.accession'].str.split().str[0] == sequence_id
                matching_info = self.table_data[matching_rows]
                if matching_info.empty:
                    logging.warning(f"No matching table data for {sequence_id}")
                    continue
                target_variable = matching_info['Target variable'].values[0]
                associated_variable = matching_info['Associated variable'].values[0]
            else:
                target_variable = None
                associated_variable = None

            kmers = [sequence[i:i + k] for i in range(0, len(sequence) - k + 1, step_size)]
            kmers = [kmer for kmer in kmers if kmer.count('-') < k]

            if not kmers:
                logging.warning(f"No valid k-mers for {sequence_id}")
                continue

            all_kmers.append(kmers)
            kmers_counts.append(len(kmers))

            embedding_info = {
                'protein_accession': sequence_id,
                'target_variable': target_variable,
                'associated_variable': associated_variable,
                'kmers': kmers
            }
            kmer_groups[sequence_id] = embedding_info

        if not kmers_counts:
            logging.error("No k-mers were collected. Please check your sequences and k-mer parameters.")
            sys.exit(1)

        if min_kmers is not None:
            self.min_kmers = min_kmers
            logging.info(f"Using provided min_kmers: {self.min_kmers}")
        else:
            self.min_kmers = min(kmers_counts)
            logging.info(f"Minimum number of k-mers in any sequence: {self.min_kmers}")

        for record in self.alignment:
            sequence_id = record.id.split()[0]
            embedding_info = kmer_groups.get(sequence_id, {})
            kmers_for_protein = embedding_info.get('kmers', [])

            if len(kmers_for_protein) == 0:
                if self.aggregation_method == 'none':
                    embedding_concatenated = np.zeros(self.models['global'].vector_size * self.min_kmers)
                else:
                    embedding_concatenated = np.zeros(self.models['global'].vector_size)
                self.embeddings.append({
                    'protein_accession': sequence_id,
                    'embedding': embedding_concatenated,
                    'target_variable': embedding_info.get('target_variable'),
                    'associated_variable': embedding_info.get('associated_variable')
                })
                continue

            selected_kmers = kmers_for_protein[:self.min_kmers]
            if len(selected_kmers) < self.min_kmers:
                padding = [np.zeros(self.models['global'].vector_size)] * (self.min_kmers - len(selected_kmers))
                selected_kmers.extend(padding)

            selected_embeddings = [
                self.models['global'].wv[kmer] 
                if kmer in self.models['global'].wv 
                else np.zeros(self.models['global'].vector_size) 
                for kmer in selected_kmers
            ]

            if self.aggregation_method == 'none':
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)
            elif self.aggregation_method == 'mean':
                embedding_concatenated = np.mean(selected_embeddings, axis=0)
            else:
                logging.warning(f"Unknown aggregation method '{self.aggregation_method}'. Using concatenation.")
                embedding_concatenated = np.concatenate(selected_embeddings, axis=0)

            self.embeddings.append({
                'protein_accession': sequence_id,
                'embedding': embedding_concatenated,
                'target_variable': embedding_info.get('target_variable'),
                'associated_variable': embedding_info.get('associated_variable')
            })

            logging.debug(f"Protein ID: {sequence_id}, Embedding Shape: {embedding_concatenated.shape}")

        embeddings_array_train = np.array([entry['embedding'] for entry in self.embeddings])
        embedding_shapes = set(embedding.shape for embedding in [entry['embedding'] for entry in self.embeddings])
        if len(embedding_shapes) != 1:
            logging.error(f"Inconsistent embedding shapes detected: {embedding_shapes}")
            raise ValueError("Embeddings have inconsistent shapes.")
        else:
            logging.info(f"All embeddings have the shape: {embedding_shapes.pop()}")

        scaler_full_path = os.path.join(model_dir, 'scaler_associated.pkl') if model_dir else 'scaler_associated.pkl'
        if os.path.exists(scaler_full_path):
            logging.info(f"StandardScaler found at {scaler_full_path}. Loading the scaler.")
            scaler = joblib.load(scaler_full_path)
        else:
            logging.info("StandardScaler not found. Training a new scaler.")
            scaler = StandardScaler().fit(embeddings_array_train)
            joblib.dump(scaler, scaler_full_path)
            logging.info(f"StandardScaler saved at {scaler_full_path}")

    def get_embeddings_and_labels(self, label_type: str = 'associated_variable') -> tuple:
        """
        Returns embeddings and associated labels in a possible multilabel form.
        
        Se o campo associated_variable for do tipo string contendo v√°rias classes 
        separadas por v√≠rgula, iremos transform√°-lo em lista e depois binarizar.
        """
        embeddings = []
        labels_raw = []

        for embedding_info in self.embeddings:
            embeddings.append(embedding_info['embedding'])
            label_str = embedding_info[label_type]
            if isinstance(label_str, str):
                # Se contiver v√≠rgulas, faz split para multilabel
                split_labels = [lbl.strip() for lbl in label_str.split(',')]
                labels_raw.append(split_labels)
            else:
                # se for string √∫nica ou None, tratamos como 1 label
                # ou se for float nan, etc.
                if not label_str:
                    # Pode ser None -> "NoLabel"
                    labels_raw.append(["NoLabel"])
                else:
                    labels_raw.append([str(label_str)])

        embeddings_array = np.array(embeddings)

        # Binariza em multilabel se houver mais de um label
        mlb = MultiLabelBinarizer()
        Y = mlb.fit_transform(labels_raw)
        self.mlb_classes_ = mlb.classes_  # Podemos armazenar se quisermos

        return embeddings_array, Y
        
def compute_perplexity(n_samples: int) -> int:
    return max(5, min(50, n_samples // 100))

def plot_dual_umap(
    train_embeddings: np.ndarray, 
    train_labels: list,
    train_protein_ids: list,
    predict_embeddings: np.ndarray, 
    predict_labels: list, 
    predict_protein_ids: list, 
    output_dir: str
) -> tuple:
    umap_train = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)
    umap_train_result = umap_train.fit_transform(train_embeddings)

    umap_predict = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)
    umap_predict_result = umap_predict.fit_transform(predict_embeddings)

    unique_train_labels = sorted(list(set(train_labels)))
    color_map_train = px.colors.qualitative.Dark24
    color_dict_train = {label: color_map_train[i % len(color_map_train)] for i, label in enumerate(unique_train_labels)}

    unique_predict_labels = sorted(list(set(predict_labels)))
    color_map_predict = px.colors.qualitative.Light24
    color_dict_predict = {label: color_map_predict[i % len(color_map_predict)] for i, label in enumerate(unique_predict_labels)}

    train_colors = [color_dict_train.get(label, 'gray') for label in train_labels]
    predict_colors = [color_dict_predict.get(label, 'gray') for label in predict_labels]

    fig_train = go.Figure()
    fig_train.add_trace(go.Scatter3d(
        x=umap_train_result[:, 0],
        y=umap_train_result[:, 1],
        z=umap_train_result[:, 2],
        mode='markers',
        marker=dict(size=5, color=train_colors, opacity=0.8),
        text=[f"Protein ID: {protein_id}<br>Label: {label}" 
              for protein_id, label in zip(train_protein_ids, train_labels)],
        hoverinfo='text',
        name='Training Data'
    ))
    fig_train.update_layout(
        title='UMAP 3D: Training Data',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    fig_predict = go.Figure()
    fig_predict.add_trace(go.Scatter3d(
        x=umap_predict_result[:, 0],
        y=umap_predict_result[:, 1],
        z=umap_predict_result[:, 2],
        mode='markers',
        marker=dict(size=5, color=predict_colors, opacity=0.8),
        text=[f"Protein ID: {protein_id}<br>Label: {label}" 
              for protein_id, label in zip(predict_protein_ids, predict_labels)],
        hoverinfo='text',
        name='Predictions'
    ))
    fig_predict.update_layout(
        title='UMAP 3D: Predictions',
        scene=dict(
            xaxis=dict(title='Component 1'),
            yaxis=dict(title='Component 2'),
            zaxis=dict(title='Component 3')
        )
    )

    umap_train_html = os.path.join(output_dir, "umap_train_3d.html")
    umap_predict_html = os.path.join(output_dir, "umap_predict_3d.html")
    
    pio.write_html(fig_train, file=umap_train_html, auto_open=False)
    pio.write_html(fig_predict, file=umap_predict_html, auto_open=False)
    
    logging.info(f"UMAP Training plot saved as {umap_train_html}")
    logging.info(f"UMAP Predictions plot saved as {umap_predict_html}")

    return fig_train, fig_predict

def plot_predictions_scatterplot_custom(
    results: dict, 
    output_path: str, 
    top_n: int = 1
) -> None:
    protein_specificities = {}
    
    for seq_id, info in results.items():
        associated_rankings = info.get('associated_ranking', [])
        if not associated_rankings:
            logging.warning(f"No ranking data associated with the protein. {seq_id}. Skipping...")
            continue

        top_category_with_confidence, top_sum, top_two_categories = format_and_sum_probabilities(associated_rankings)
        if top_category_with_confidence is None:
            logging.warning(f"No valid formatting for protein {seq_id}. Skipping...")
            continue

        category = top_category_with_confidence.split(" (")[0]
        confidence = top_sum

        protein_specificities[seq_id] = {
            'top_category': category,
            'confidence': confidence
        }

    if not protein_specificities:
        logging.warning("No data available to plot the scatter plot.")
        return

    unique_proteins = sorted(protein_specificities.keys())
    protein_order = {protein: idx for idx, protein in enumerate(unique_proteins)}

    fig, ax = plt.subplots(figsize=(12, max(6, len(unique_proteins) * 0.5)))
    x_values = list(range(4, 19))

    for protein, data in protein_specificities.items():
        y = protein_order[protein]
        category = data['top_category']
        confidence = data['confidence']
        specificities = [int(x[1:]) for x in category.split('-') if x.startswith('C')]

        for spec in specificities:
            ax.scatter(
                spec, y,
                color='#1f78b4',
                edgecolors='black',
                linewidth=0.5,
                s=100,
                label='_nolegend_'
            )

        if len(specificities) > 1:
            ax.plot(
                specificities,
                [y] * len(specificities),
                color='#1f78b4',
                linestyle='-',
                linewidth=1.0,
                alpha=0.7
            )

    ax.set_xlabel('Specificity (C4 to C18)', fontsize=14, fontweight='bold', color='white')
    ax.set_ylabel('Proteins', fontsize=14, fontweight='bold', color='white')
    ax.set_title('Scatter Plot of Predictions for New Sequences (SS Prediction)', fontsize=16, fontweight='bold', pad=20, color='white')

    ax.set_xticks(x_values)
    ax.set_xticklabels([f'C{spec}' for spec in x_values], fontsize=12, color='white')
    ax.set_yticks(range(len(unique_proteins)))
    ax.set_yticklabels(unique_proteins, fontsize=10, color='white')

    ax.grid(True, axis='x', linestyle='--', alpha=0.5, color='gray')
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color('white')
    ax.spines['bottom'].set_color('white')

    ax.xaxis.set_minor_locator(ticker.AutoMinorLocator())
    ax.grid(which='minor', axis='x', linestyle=':', linewidth=0.5, alpha=0.6)

    plt.tight_layout()
    plt.savefig(output_path, facecolor='#0B3C5D', dpi=600, bbox_inches='tight')
    plt.close()
    logging.info(f"Scatter plot saved at {output_path}")


def adjust_predictions_global(
    predicted_proba: np.ndarray, 
    method: str = 'normalize', 
    alpha: float = 1.0
) -> np.ndarray:
    if method == 'normalize':
        logging.info("Normalizing predicted probabilities.")
        sums = predicted_proba.sum(axis=1, keepdims=True)
        sums[sums == 0] = 1e-9
        adjusted_proba = predicted_proba / sums
    elif method == 'smoothing':
        logging.info(f"Applying smoothing to predicted probabilities with alpha={alpha}.")
        sums = predicted_proba.sum(axis=1, keepdims=True) + alpha * predicted_proba.shape[1]
        adjusted_proba = (predicted_proba + alpha) / sums
    elif method == 'none':
        logging.info("No adjustment applied to predicted probabilities.")
        adjusted_proba = predicted_proba.copy()
    else:
        logging.warning(f"Unknown adjustment method '{method}'. No adjustment will be applied.")
        adjusted_proba = predicted_proba.copy()
    return adjusted_proba

def main(args: argparse.Namespace) -> None:
    model_dir = args.model_dir
    total_steps = 5
    current_step = 0
    progress_bar = st.progress(0)
    progress_text = st.empty()

    # =============================
    # STEP 1: Training the Model
    # =============================
    train_alignment_path = args.train_fasta
    train_table_data_path = args.train_table

    if not are_sequences_aligned(train_alignment_path):
        logging.info("Training sequences not aligned. Realigning with MAFFT...")
        aligned_train_path = train_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(train_alignment_path, aligned_train_path, threads=1)
        train_alignment_path = aligned_train_path
    else:
        logging.info(f"Aligned training file found or sequences already aligned: {train_alignment_path}")

    train_table_data = pd.read_csv(train_table_data_path, delimiter="\t")
    logging.info("Training table data loaded successfully.")
    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    protein_embedding_train = ProteinEmbeddingGenerator(
        train_alignment_path, 
        table_data=train_table_data, 
        aggregation_method=args.aggregation_method
    )
    protein_embedding_train.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        save_min_kmers=True
    )
    logging.info(f"Train embeddings generated: {len(protein_embedding_train.embeddings)}")

    min_kmers = protein_embedding_train.min_kmers
    protein_ids_associated = [e['protein_accession'] for e in protein_embedding_train.embeddings]
    var_assoc_associated = [e['associated_variable'] for e in protein_embedding_train.embeddings]
    X_associated, y_associated = protein_embedding_train.get_embeddings_and_labels(label_type='associated_variable')

    logging.info(f"X_associated shape: {X_associated.shape}, y_associated shape: {y_associated.shape}")

    scaler_associated = StandardScaler().fit(X_associated)
    scaler_associated_path = os.path.join(model_dir, 'scaler_associated.pkl')
    joblib.dump(scaler_associated, scaler_associated_path)
    logging.info("Scaler for X_associated saved.")

    X_associated_scaled = scaler_associated.transform(X_associated)
    rf_model_associated_full_path = os.path.join(model_dir, args.rf_model_associated)
    calibrated_model_associated_full_path = os.path.join(model_dir, 'calibrated_model_associated.pkl')

    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    if os.path.exists(calibrated_model_associated_full_path):
        calibrated_model_associated = joblib.load(calibrated_model_associated_full_path)
        logging.info(f"Loaded existing calibrated model from {calibrated_model_associated_full_path}")
    else:
        support_model_associated = Support()
        calibrated_model_associated = support_model_associated.fit(
            X_associated_scaled,
            y_associated,
            protein_ids=protein_ids_associated,
            var_assoc=var_assoc_associated,
            model_name_prefix='associated',
            model_dir=model_dir,
            min_kmers=min_kmers
        )
        support_model_associated.plot_learning_curve(args.learning_curve_associated)
        joblib.dump(calibrated_model_associated, calibrated_model_associated_full_path)
        logging.info(f"Calibrated model saved at {calibrated_model_associated_full_path}")

        best_roc, best_f1, best_pr_auc, best_params, best_calibrated_model, X_test_, y_test_ = support_model_associated.test_best_RF(
            X_associated_scaled, 
            y_associated
        )
        logging.info(f"Best ROC (micro): {best_roc}")
        logging.info(f"Best F1 (micro): {best_f1}")
        logging.info(f"Best Precision-Recall AUC (micro): {best_pr_auc}")
        logging.info(f"Best Parameters: {best_params}")

        joblib.dump(best_calibrated_model, rf_model_associated_full_path)
        logging.info(f"Random Forest model (multilabel) saved at {rf_model_associated_full_path}")

        # Plot ROC multilabel
        y_pred_proba_test = best_calibrated_model.predict_proba(X_test_)
        # Precisamos das classes do mlb se quisermos nomes
        if hasattr(protein_embedding_train, 'mlb_classes_'):
            mlb_classes = protein_embedding_train.mlb_classes_
        else:
            mlb_classes = [f"Label_{i}" for i in range(y_pred_proba_test.shape[1])]

        plot_roc_curve_multilabel(y_test_, y_pred_proba_test, mlb_classes, 
                                  title="ROC Curve Multilabel (Associated)", 
                                  save_as=args.roc_curve_associated)

    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    # =============================
    # STEP 2: Classifying New Sequences
    # =============================
    min_kmers_path = os.path.join(model_dir, 'min_kmers.txt')
    if os.path.exists(min_kmers_path):
        with open(min_kmers_path, 'r') as f:
            min_kmers_loaded = int(f.read().strip())
        logging.info(f"min_kmers loaded: {min_kmers_loaded}")
    else:
        logging.error("min_kmers file not found. Ensure training was completed.")
        sys.exit(1)

    predict_alignment_path = args.predict_fasta
    if not are_sequences_aligned(predict_alignment_path):
        logging.info("Prediction sequences not aligned. Realigning with MAFFT...")
        aligned_predict_path = predict_alignment_path.replace(".fasta", "_aligned.fasta")
        realign_sequences_with_mafft(predict_alignment_path, aligned_predict_path, threads=1)
        predict_alignment_path = aligned_predict_path
    else:
        logging.info(f"Aligned predict file found: {predict_alignment_path}")

    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    protein_embedding_predict = ProteinEmbeddingGenerator(
        predict_alignment_path, 
        table_data=None,
        aggregation_method=args.aggregation_method
    )
    protein_embedding_predict.generate_embeddings(
        k=args.kmer_size,
        step_size=args.step_size,
        word2vec_model_path=args.word2vec_model,
        model_dir=model_dir,
        min_kmers=min_kmers_loaded
    )
    logging.info(f"Prediction embeddings: {len(protein_embedding_predict.embeddings)}")

    X_predict = np.array([ent['embedding'] for ent in protein_embedding_predict.embeddings])
    scaler_path_ = os.path.join(model_dir, 'scaler_associated.pkl')
    if os.path.exists(scaler_path_):
        scaler_associated = joblib.load(scaler_path_)
        logging.info("Scaler loaded for prediction.")
    else:
        logging.error("Scaler not found, training incomplete.")
        sys.exit(1)

    X_predict_scaled = scaler_associated.transform(X_predict)
    predictions_associated = calibrated_model_associated.predict(X_predict_scaled)
    rankings_associated = get_class_rankings_global(calibrated_model_associated, X_predict_scaled)

    results = {}
    for ent, pred, ranking in zip(protein_embedding_predict.embeddings,
                                  predictions_associated, rankings_associated):
        seq_id = ent['protein_accession']
        results[seq_id] = {
            "associated_prediction": pred,  # array 0/1
            "associated_ranking": ranking   # list com "Label: XX%"
        }

    with open(args.results_file, 'w') as f:
        f.write("Protein_ID\tAssociated_Prediction\tAssociated_Ranking\n")
        for seq_id, info in results.items():
            f.write(f"{seq_id}\t{info['associated_prediction']}\t{'; '.join(info['associated_ranking'])}\n")
            logging.info(f"{seq_id} => {info['associated_prediction']}, {info['associated_ranking']}")

    logging.info("Generating scatter plot of predictions for new sequences...")
    plot_predictions_scatterplot_custom(results, args.scatterplot_output)
    logging.info(f"Scatter plot saved at {args.scatterplot_output}")

    logging.info("Generating Dual UMAP plots for training/prediction data...")

    # Convert one-hot em string para exibir nos gr√°ficos
    if hasattr(protein_embedding_train, 'mlb_classes_'):
        mlb_classes = protein_embedding_train.mlb_classes_
    else:
        mlb_classes = [f"Label_{i}" for i in range(y_associated.shape[1])]

    # Gera label textual para UMAP/TSNE
    train_labels_str = []
    for row in y_associated:
        # row = ex.: [0,1,1,0,...]
        active_labels = [mlb_classes[i] for i, val in enumerate(row) if val == 1]
        if not active_labels:
            train_labels_str.append("NoLabel")
        else:
            train_labels_str.append(",".join(active_labels))

    # idem para as predi√ß√µes
    predict_labels_str = []
    for row in predictions_associated:
        active_labels = [mlb_classes[i] for i, val in enumerate(row) if val == 1]
        if not active_labels:
            predict_labels_str.append("NoLabel")
        else:
            predict_labels_str.append(",".join(active_labels))

    train_protein_ids = protein_ids_associated
    predict_protein_ids = [ent['protein_accession'] for ent in protein_embedding_predict.embeddings]

    plot_dual_umap(
        train_embeddings=X_associated_scaled,
        train_labels=train_labels_str,
        train_protein_ids=train_protein_ids,
        predict_embeddings=X_predict_scaled,
        predict_labels=predict_labels_str,
        predict_protein_ids=predict_protein_ids,
        output_dir=model_dir
    )
    logging.info("Dual UMAP done.")

    fig_tsne_train, fig_tsne_predict = plot_dual_tsne(
        train_embeddings=X_associated_scaled,
        train_labels=train_labels_str,
        train_protein_ids=train_protein_ids,
        predict_embeddings=X_predict_scaled,
        predict_labels=predict_labels_str,
        predict_protein_ids=predict_protein_ids,
        output_dir=model_dir
    )
    logging.info("Dual TSNE done.")

    current_step += 1
    progress = min(current_step / total_steps, 1.0)
    progress_bar.progress(progress)
    progress_text.markdown(f"<span style='color:white'>Progress: {int(progress * 100)}%</span>", unsafe_allow_html=True)
    time.sleep(0.1)

    st.success("Analysis completed successfully!")
    st.header("Scatter Plot of Predictions")
    scatterplot_path = args.scatterplot_output
    if os.path.exists(scatterplot_path):
        st.image(scatterplot_path, use_column_width=True)
    else:
        st.error(f"Scatter plot not found at {scatterplot_path}")

    # Exibir uma tabela formatada resumindo as predi√ß√µes:
    formatted_results = []
    for seq_id, info in results.items():
        associated_rankings = info.get('associated_ranking', [])
        if not associated_rankings:
            continue
        top_specificity, confidence, top_two_specificities = format_and_sum_probabilities(associated_rankings)
        if top_specificity is None:
            continue
        formatted_results.append([seq_id, top_specificity, f"{confidence:.2f}%", "; ".join(top_two_specificities)])

    headers = ["Query Name", "SS Prediction Specificity", "Prediction Confidence", "Top 2 Specificities"]
    df_results = pd.DataFrame(formatted_results, columns=headers)

    def highlight_table(df):
        return df.style.set_table_styles([
            {
                'selector': 'th',
                'props': [
                    ('background-color', '#1E3A8A'),
                    ('color', 'white'),
                    ('border', '1px solid white'),
                    ('font-weight', 'bold'),
                    ('text-align', 'center')
                ]
            },
            {
                'selector': 'td',
                'props': [
                    ('background-color', '#0B3C5D'),
                    ('color', 'white'),
                    ('border', '1px solid white'),
                    ('text-align', 'center'),
                    ('font-family', 'Arial'),
                    ('font-size', '12px'),
                ]
            },
            {
                'selector': 'tr:nth-child(even) td',
                'props': [
                    ('background-color', '#145B9C')
                ]
            },
            {
                'selector': 'tr:hover td',
                'props': [
                    ('background-color', '#0D4F8B')
                ]
            },
        ])

    styled_df = highlight_table(df_results)
    html = styled_df.to_html(index=False, escape=False)

    st.markdown(
        """
        <style>
        .stButton > button {
            background-color: #1E3A8A;
            color: white;
            border: none;
            padding: 10px 24px;
            text-align: center;
            text-decoration: none;
            display: inline-block;
            font-size: 14px;
            margin: 4px 2px;
            cursor: pointer;
            border-radius: 4px;
        }
    
        .stButton > button:hover {
            background-color: #0B3C5D;
        }
        table {
            border-collapse: collapse;
            width: 100%;
        }
        .dataframe-container {
            overflow-x: auto;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    st.header("Formatted Results")
    st.markdown(
        f"""
        <div class="dataframe-container">
            {html}
        </div>
        """,
        unsafe_allow_html=True
    )

    st.download_button(
        label="Download Results as CSV",
        data=df_results.to_csv(index=False).encode('utf-8'),
        file_name='results.csv',
        mime='text/csv',
    )

    output = BytesIO() 
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df_results.to_excel(writer, index=False, sheet_name='Results')
        writer.close()
        processed_data = output.getvalue()

    st.download_button(
        label="Download Results as Excel",
        data=processed_data,
        file_name='results.xlsx',
        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    )

    zip_buffer = BytesIO()
    with zipfile.ZipFile(zip_buffer, "w") as zip_file:
        for folder_name, subfolders, filenames in os.walk(output_dir):
            for filename in filenames:
                file_path = os.path.join(folder_name, filename)
                zip_file.write(file_path, arcname=os.path.relpath(file_path, output_dir))
    zip_buffer.seek(0)

    st.header("Download All Results")
    st.download_button(
        label="Download All Results as results.zip",
        data=zip_buffer,
        file_name="results.zip",
        mime="application/zip"
    )

    df = pd.DataFrame(formatted_results, columns=headers)
    df.to_excel(args.excel_output, index=False)
    logging.info(f"Resultados salvos em {args.excel_output}")

    with open(args.formatted_results_table, 'w') as f:
        f.write(tabulate(formatted_results, headers=headers, tablefmt="grid"))
    logging.info(f"Formatted table saved at {args.formatted_results_table}")
    
    umap_similarity_path = os.path.join(output_dir, "umap_similarity_3D.html")
    dual_umap_train_path = os.path.join(output_dir, "umap_train_3d.html")
    dual_umap_predict_path = os.path.join(output_dir, "umap_predict_3d.html")
    tsne_train_html = os.path.join(output_dir, "tsne_train_3d.html")
    tsne_predict_html = os.path.join(output_dir, "tsne_predict_3d.html")

st.markdown(
    """
    <style>
    .stApp {
        background-color: #0B3C5D;
        color: white;
    }
    [data-testid="stSidebar"] {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    [data-testid="stSidebar"] * {
        background-color: #0B3C5D !important;
        color: white !important;
    }
    [data-testid="stSidebar"] input,
    [data-testid="stSidebar"] select,
    [data-testid="stSidebar"] textarea,
    [data-testid="stSidebar"] button,
    [data-testid="stSidebar"] .stButton,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    [data-testid="stSidebar"] div[data-testid="stFileUploader"] div {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    [data-testid="stSidebar"] .stSelectbox [role="listbox"] {
        background-color: #1E3A8A !important;
        color: white !important;
    }
    [data-testid="stSidebar"] .stButton > button,
    [data-testid="stSidebar"] .stFileUploader,
    [data-testid="stSidebar"] .stSelectbox,
    [data-testid="stSidebar"] .stNumberInput,
    [data-testid="stSidebar"] .stTextInput,
    [data-testid="stSidebar"] .stCheckbox,
    [data-testid="stSidebar"] .stRadio,
    [data-testid="stSidebar"] .stSlider {
        border: none !important;
        box-shadow: none !important;
    }
    [data-testid="stSidebar"] .stCheckbox input[type="checkbox"] + div:first-of-type,
    [data-testid="stSidebar"] .stRadio input[type="radio"] + div:first-of-type {
        background-color: #1E3A8A !important;
    }
    [data-testid="stSidebar"] .stSlider > div:first-of-type {
        color: white !important;
    }
    [data-testid="stSidebar"] .stSlider .st-bo {
        background-color: #1E3A8A !important;
    }
    h1, h2, h3, h4, h5, h6 {
        color: white !important;
    }
    div[role="alert"] p {
        color: white !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

def get_base64_image(image_path: str) -> str:
    """
    Encode an image file to a base64 string.
    
    Parameters:
    - image_path (str): Path to the image file.
    
    Returns:
    - str: Base64 string of the image.
    """
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return ""

image_path = "./images/faal.png"
image_base64 = get_base64_image(image_path)
st.markdown(
    f"""
    <div style="text-align: center; font-family: 'Arial', sans-serif; padding: 30px; 
         background: linear-gradient(to bottom, #f9f9f9, #ffffff); border-radius: 15px; 
         border: 2px solid #dddddd; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); 
         position: relative;">
        <p style="color: black; font-size: 1.5em; font-weight: bold; margin: 0;">
            FAALPred: Predicting Fatty Acyl Chain Specificities in Fatty Acyl-AMP Ligases (FAALs) 
            Using Integrated Approaches of Neural Networks, Bioinformatics, and Machine Learning
        </p>
        <p style="color: #2c3e50; font-size: 1.2em; font-weight: normal; margin-top: 10px;">
            Anne Liong, Leandro de Mattos Pereira, and Pedro Le√£o
        </p>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8;">
            <strong>FAALPred</strong> is a comprehensive bioinformatics tool designed to 
            predict the fatty acid chain length specificity of substrates, ranging from C4 to C18.
        </p>
        <h5 style="color: #2c3e50; font-size: 20px; font-weight: bold; margin-top: 25px;">ABSTRACT</h5>
        <p style="color: #2c3e50; font-size: 18px; line-height: 1.8; text-align: justify;">
            Fatty Acyl-AMP Ligases (FAALs), identified by Zhang et al. (2011), activate 
            fatty acids of varying lengths for the biosynthesis of natural products. 
            These substrates enable the production of compounds such as nocuolin 
            (<em>Nodularia sp.</em>, Martins et al., 2022) and sulfolipid-1 
            (<em>Mycobacterium tuberculosis</em>, Yan et al., 2023), with applications in 
            cancer and tuberculosis treatment (Kurt et al., 2017; Gilmore et al., 2012). 
            Dr. Pedro Le√£o and his team identified several of these natural products in 
            cyanobacteria (<a href="https://leaolab.wixsite.com/leaolab" target="_blank" 
            style="color: #3498db; text-decoration: none;">visit here</a>), and FAALPred 
            classifies FAALs by their substrate specificity.
        </p>
        <div style="text-align: center; margin-top: 20px;">
            <img src="data:image/png;base64,{image_base64}" alt="FAAL Domain" 
                 style="width: auto; height: 120px; object-fit: contain;">
            <p style="text-align: center; color: #2c3e50; font-size: 14px; margin-top: 5px;">
                <em>FAAL Domain of Synechococcus sp. PCC7002, link: 
                <a href="https://www.rcsb.org/structure/7R7F" target="_blank" 
                   style="color: #3498db; text-decoration: none;">
                   https://www.rcsb.org/structure/7R7F
                </a>
                </em>
            </p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

st.sidebar.header("Input Parameters")

def save_uploaded_file(uploaded_file, save_path: str) -> str:
    """
    Saves a file uploaded by the user.
    
    Parameters:
    - uploaded_file: File uploaded by the user.
    - save_path (str): Path to save the file.
    
    Returns:
    - str: Path to the saved file.
    """
    with open(save_path, 'wb') as f:
        f.write(uploaded_file.getbuffer())
    return save_path

use_default_train = st.sidebar.checkbox("Use Default Training Data", value=True)
if not use_default_train:
    train_fasta_file = st.sidebar.file_uploader("Upload Training FASTA File", type=["fasta", "fa", "fna"])
    train_table_file = st.sidebar.file_uploader("Upload Training Table File (TSV)", type=["tsv"])
else:
    train_fasta_file = None
    train_table_file = None

predict_fasta_file = st.sidebar.file_uploader("Upload Prediction FASTA File", type=["fasta", "fa", "fna"])
kmer_size = st.sidebar.number_input("K-mer Size", min_value=1, max_value=10, value=3, step=1)
step_size = st.sidebar.number_input("Step Size", min_value=1, max_value=10, value=1, step=1)

aggregation_method = st.sidebar.selectbox(
    "Aggregation Method",
    options=['none', 'mean'],  
    index=0
)

st.sidebar.header("Customize Word2Vec Parameters")
custom_word2vec = st.sidebar.checkbox("Customize Word2Vec Parameters", value=False)
if custom_word2vec:
    window = st.sidebar.number_input("Window Size", min_value=5, max_value=20, value=10, step=5)
    workers = st.sidebar.number_input("Workers", min_value=1, max_value=112, value=8, step=8)
    epochs = st.sidebar.number_input("Epochs", min_value=1, max_value=2500, value=2500, step=100)
else:
    window = 10
    workers = 48
    epochs = 2500

model_dir = create_unique_model_directory("results", aggregation_method)
output_dir = model_dir

if st.sidebar.button("Run Analysis"):
    internal_train_fasta = "data/train.fasta"
    internal_train_table = "data/train_table.tsv"

    if use_default_train:
        train_fasta_path = internal_train_fasta
        train_table_path = internal_train_table
        st.markdown("<span style='color:white'>Using default training data.</span>", unsafe_allow_html=True)
    else:
        if train_fasta_file is not None and train_table_file is not None:
            train_fasta_path = os.path.join(output_dir, "uploaded_train.fasta")
            train_table_path = os.path.join(output_dir, "uploaded_train_table.tsv")
            save_uploaded_file(train_fasta_file, train_fasta_path)
            save_uploaded_file(train_table_file, train_table_path)
            st.markdown("<span style='color:white'>Uploaded training data will be used.</span>", unsafe_allow_html=True)
        else:
            st.error("Please upload both the training FASTA file and the training TSV table file.")
            st.stop()

    if predict_fasta_file is not None:
        predict_fasta_path = os.path.join(output_dir, "uploaded_predict.fasta")
        save_uploaded_file(predict_fasta_file, predict_fasta_path)
    else:
        st.error("Please upload a FASTA file for prediction.")
        st.stop()
        
    args = argparse.Namespace(
        train_fasta=train_fasta_path,
        train_table=train_table_path,
        predict_fasta=predict_fasta_path,
        kmer_size=kmer_size,
        step_size=step_size,
        aggregation_method=aggregation_method,
        results_file=os.path.join(output_dir, "predictions.tsv"),
        output_dir=output_dir,
        scatterplot_output=os.path.join(output_dir, "scatterplot_predictions.png"),
        excel_output=os.path.join(output_dir, "results.xlsx"),
        formatted_results_table=os.path.join(output_dir, "formatted_results.txt"),
        roc_curve_associated=os.path.join(output_dir, "roc_curve_associated.png"),
        learning_curve_associated=os.path.join(output_dir, "learning_curve_associated.png"),
        roc_values_associated=os.path.join(output_dir, "roc_values_associated.csv"),
        rf_model_associated="rf_model_associated.pkl",
        word2vec_model="word2vec_model.bin",
        scaler="scaler_associated.pkl",
        model_dir=model_dir,
    )

    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)

    st.markdown("<span style='color:white'>Processing data and running analysis...</span>", unsafe_allow_html=True)
    try:
        main(args)
    except Exception as e:
        st.error(f"An error occurred during processing: {e}")
        logging.error(f"An error occurred: {e}")

def load_and_resize_image_with_dpi(image_path: str, base_width: int, dpi: int = 300) -> Image.Image:
    try:
        image = Image.open(image_path)
        w_percent = (base_width / float(image.size[0]))
        h_size = int((float(image.size[1]) * float(w_percent)))
        resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)
        return resized_image
    except FileNotFoundError:
        logging.error(f"Image not found at {image_path}.")
        return None

def encode_image(image: Image.Image) -> str:
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

image_dir = "images"
image_paths = [
    os.path.join(image_dir, "lab_logo.png"),
    os.path.join(image_dir, "ciimar.png"),
    os.path.join(image_dir, "faal_pred_logo.png"), 
    os.path.join(image_dir, "bbf4.png"),
    os.path.join(image_dir, "google.png"),
    os.path.join(image_dir, "uniao.png"),
]

images = [load_and_resize_image_with_dpi(path, base_width=150, dpi=300) for path in image_paths]
encoded_images = [encode_image(img) for img in images if img is not None]

st.markdown(
    """
    <style>
    .footer-container {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 10px;
        margin-bottom: 10px;
        flex-wrap: wrap;
    }
    .footer-text {
        text-align: center;
        color: white;
        font-size: 12px;
        margin-top: 10px;
    }
    .support-text {
        text-align: center;
        color: white;
        font-size: 14px;
        font-weight: bold;
        margin-bottom: 5px;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

footer_html = """
<div class="support-text">Supported by:</div>
<div class="footer-container">
    {}
</div>
<div class="footer-text">
    CIIMAR - Pedro Le√£o @CNP - 2024 - All rights reserved.
</div>

"""

img_tags = "".join(
    f'<img src="data:image/png;base64,{img}" style="width: 100px;">' for img in encoded_images
)

st.markdown(footer_html.format(img_tags), unsafe_allow_html=True)
